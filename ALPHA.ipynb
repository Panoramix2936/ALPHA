{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE70w0d9bGJO"
      },
      "source": [
        "### Clear directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcWw8Fm8Mbf0",
        "outputId": "17df29aa-a831-481d-e511-302f5c20c64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.7)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYp2LQZQTdfY"
      },
      "outputs": [],
      "source": [
        "\"\"\"import shutil\n",
        "import logging\n",
        "import os\n",
        "# Utility Function: Delete all files and directories in a specified directory except for one file\n",
        "def delete_all_except(directory: str):\n",
        "    if not os.path.exists(directory):\n",
        "        logging.warning(f\"Directory '{directory}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.remove(file_path)\n",
        "                logging.info(f\"Deleted file: {file_path}\")\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "                logging.info(f\"Deleted directory and its contents: {file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "current_directory\n",
        "delete_all_except(current_directory)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33CQyR8lK2q7"
      },
      "source": [
        "# DATA COLLECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peDJ8RboK6Ta"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pD0yVvKPbSO"
      },
      "source": [
        "1. Downloading closing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3Jp2hKai20",
        "outputId": "bedba2fa-dba7-4338-c494-445328febe58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.61)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.4)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPXGaMrwcqTu"
      },
      "outputs": [],
      "source": [
        "tickers = [\n",
        "\"^GSPC\", # S&P 500\n",
        "\"^IXIC\", # NASDAQ Composite\n",
        "\"^DJI\", # Dow Jones Industrial Average\n",
        "\"^FCHI\", # CAC 40 (France)\n",
        "\"^FTSE\", # FTSE 100 (UK)\n",
        "\"^STOXX50E\",# EuroStoxx 50\n",
        "\"^HSI\", # Hang Seng Index (Hong Kong)\n",
        "\"000001.SS\",# Shanghai Composite (China)\n",
        "\"^BSESN\", # BSE Sensex (India)\n",
        "\"^NSEI\", # Nifty 50 (India)\n",
        "\"^KS11\", # KOSPI (South Korea)\n",
        "\"GC=F\", # Gold\n",
        "\"SI=F\", # Silver\n",
        "\"CL=F\", # WTI Crude Oil Futures\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SPinGzAcmDP",
        "outputId": "e3553913-411f-4988-bdba-8b95146bf914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  14 of 14 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing prices saved to stock_closing_prices.csv\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "start_date = \"2003-01-01\"  # Example start date\n",
        "end_date = \"2024-12-31\"   # Example end date\n",
        "\n",
        "# Function to fetch data with rate limit handling\n",
        "def fetch_data(tickers, start, end):\n",
        "    retry = True\n",
        "    delay = 60  # Delay for rate limit errors\n",
        "    data = None  # Initialize data to None\n",
        "    while retry:\n",
        "        try:\n",
        "            # Batch download, specify actions=False to avoid extra data\n",
        "            data = yf.download(tickers, start=start, end=end, actions=False)\n",
        "            # If multi-ticker, extract 'Close' from the multi-level columns\n",
        "            if len(tickers) > 1:\n",
        "                data = data['Close']\n",
        "            else:\n",
        "                data = data[['Close']]  # Single ticker case\n",
        "            retry = False\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(f\"Rate limit hit. Retrying after {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Error fetching data: {e}\")\n",
        "                retry = False\n",
        "                data = pd.DataFrame()  # Return empty DataFrame on failure\n",
        "    return data\n",
        "\n",
        "# Download closing prices\n",
        "data = fetch_data(tickers, start_date, end_date)\n",
        "\n",
        "# Check if data is not empty\n",
        "if not data.empty:\n",
        "    # Save to CSV\n",
        "    data.to_csv(\"stock_closing_prices.csv\")\n",
        "    print(\"Closing prices saved to stock_closing_prices.csv\")\n",
        "else:\n",
        "    print(\"No data to save.\")\n",
        "\n",
        "# Add delay to avoid rate limits for subsequent requests\n",
        "time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyTKpVpvdL9J",
        "outputId": "5d956176-512c-4371-b45c-15ca81302d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned data saved to clean_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_stock_data(input_csv, output_csv):\n",
        "    \"\"\"\n",
        "    Clean stock closing prices by applying backward fill and linear interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'stock_closing_prices.csv')\n",
        "    output_csv (str): Path to the output CSV file (e.g., 'clean_data.csv')\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the cleaned data to output_csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Apply backward fill to handle missing data at start/end\n",
        "        df = df.bfill()\n",
        "\n",
        "        # Apply linear interpolation to fill small gaps\n",
        "        df = df.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        # Save the cleaned data to a new CSV\n",
        "        df.to_csv(output_csv)\n",
        "        print(f\"Cleaned data saved to {output_csv}\")\n",
        "\n",
        "        # Report any remaining NaN values\n",
        "        if df.isna().any().any():\n",
        "            print(\"Warning: Some NaN values remain after cleaning.\")\n",
        "            print(df.isna().sum())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    clean_stock_data(\"stock_closing_prices.csv\", \"clean_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0XKR-QcZ3rr"
      },
      "source": [
        "2. Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLRd-ALJeaTk",
        "outputId": "3c754c00-7088-46d5-965a-0066ee16c536"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized data saved to normalized_data.csv\n",
            "Log evolution data saved to log_evolution_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def process_stock_data(input_csv, normalized_output_csv, log_output_csv):\n",
        "    \"\"\"\n",
        "    Process cleaned stock data to create normalized and log evolution CSVs.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'clean_data.csv')\n",
        "    normalized_output_csv (str): Path to the output CSV for normalized data\n",
        "    log_output_csv (str): Path to the output CSV for log evolution data\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the processed data to the specified output CSVs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"Error: Input CSV is empty.\")\n",
        "            return\n",
        "\n",
        "        # Initialize DataFrames for normalized and log evolution data\n",
        "        normalized_df = df.copy()\n",
        "        log_evolution_df = df.copy()\n",
        "\n",
        "        # Process each ticker (column)\n",
        "        for ticker in df.columns:\n",
        "            # Get the first non-null value for the ticker\n",
        "            first_valid = df[ticker].dropna().iloc[0] if df[ticker].dropna().size > 0 else None\n",
        "\n",
        "            if first_valid is not None and first_valid > 0:\n",
        "                # Normalize: divide by the first non-null value (set first point to 1)\n",
        "                normalized_df[ticker] = df[ticker] / first_valid\n",
        "\n",
        "                # Log evolution: ln(price / first_valid)\n",
        "                log_evolution_df[ticker] = np.log(df[ticker] / first_valid)\n",
        "            else:\n",
        "                # If no valid first value or first value is zero, set to NaN\n",
        "                print(f\"Warning: No valid first value for {ticker} or first value is zero.\")\n",
        "                normalized_df[ticker] = np.nan\n",
        "                log_evolution_df[ticker] = np.nan\n",
        "\n",
        "        # Save the normalized data\n",
        "        normalized_df.to_csv(normalized_output_csv)\n",
        "        print(f\"Normalized data saved to {normalized_output_csv}\")\n",
        "\n",
        "        # Save the log evolution data\n",
        "        log_evolution_df.to_csv(log_output_csv)\n",
        "        print(f\"Log evolution data saved to {log_output_csv}\")\n",
        "\n",
        "        # Report any columns with all NaN values\n",
        "        if normalized_df.isna().all().any():\n",
        "            print(\"Warning: Some tickers have all NaN in normalized data:\",\n",
        "                  normalized_df.columns[normalized_df.isna().all()].tolist())\n",
        "        if log_evolution_df.isna().all().any():\n",
        "            print(\"Warning: Some tickers have all NaN in log evolution data:\",\n",
        "                  log_evolution_df.columns[log_evolution_df.isna().all()].tolist())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    process_stock_data(\"clean_data.csv\", \"normalized_data.csv\", \"log_evolution_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM6LAJSAhDxg",
        "outputId": "bf5aaca3-4912-46cd-9201-0f0cba9fa23e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Portfolio evolution saved to portfolio_evolution.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def simulate_equal_weights_portfolio(input_csv, output_csv, initial_value=1):\n",
        "    \"\"\"\n",
        "    Simulate an equal weights portfolio and save its evolution and log evolution.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'clean_data.csv')\n",
        "    output_csv (str): Path to the output CSV file (e.g., 'portfolio_evolution.csv')\n",
        "    initial_value (float): Initial portfolio value in dollars (default: 1000)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the portfolio evolution and log evolution to output_csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"Error: Input CSV is empty.\")\n",
        "            return\n",
        "\n",
        "        # Drop columns with all NaN values (e.g., failed tickers)\n",
        "        df = df.dropna(axis=1, how='all')\n",
        "        if df.empty:\n",
        "            print(\"Error: No valid ticker data after dropping NaN columns.\")\n",
        "            return\n",
        "\n",
        "        # Number of tickers\n",
        "        n_tickers = len(df.columns)\n",
        "        if n_tickers == 0:\n",
        "            print(\"Error: No valid tickers found.\")\n",
        "            return\n",
        "\n",
        "        # Initial allocation: equal weight for each ticker\n",
        "        weight = 1.0 / n_tickers\n",
        "        initial_allocation = initial_value * weight\n",
        "\n",
        "        # Calculate number of shares for each ticker (based on first valid price)\n",
        "        first_valid_prices = df.apply(lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else np.nan)\n",
        "        if first_valid_prices.isna().any():\n",
        "            print(\"Warning: Some tickers have no valid prices:\",\n",
        "                  first_valid_prices.index[first_valid_prices.isna()].tolist())\n",
        "            df = df.loc[:, ~first_valid_prices.isna()]\n",
        "            first_valid_prices = first_valid_prices.dropna()\n",
        "            n_tickers = len(df.columns)\n",
        "            weight = 1.0 / n_tickers\n",
        "            initial_allocation = initial_value * weight\n",
        "\n",
        "        shares = initial_allocation / first_valid_prices\n",
        "\n",
        "        # Calculate portfolio value: sum of (shares * price) for each ticker\n",
        "        portfolio_values = (df * shares).sum(axis=1)\n",
        "\n",
        "        # Calculate log evolution: ln(portfolio_value / initial_value)\n",
        "        log_evolution = np.log(portfolio_values / initial_value)\n",
        "\n",
        "        # Create output DataFrame\n",
        "        output_df = pd.DataFrame({\n",
        "            'Portfolio_Value': portfolio_values,\n",
        "            'Log_Evolution': log_evolution\n",
        "        })\n",
        "\n",
        "        # Save to CSV\n",
        "        output_df.to_csv(output_csv)\n",
        "        print(f\"Portfolio evolution saved to {output_csv}\")\n",
        "\n",
        "        # Report any NaN values in the output\n",
        "        if output_df.isna().any().any():\n",
        "            print(\"Warning: Some NaN values in portfolio evolution.\")\n",
        "            print(output_df.isna().sum())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    simulate_equal_weights_portfolio(\"clean_data.csv\", \"portfolio_evolution.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mThEOljoPtUp"
      },
      "source": [
        "3. Separate by month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqGiDVl_ecTA",
        "outputId": "d73dad5f-514f-4799-e35f-8689c2e54744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved ./2003_01_closing.csv with 22 rows\n",
            "Saved ./2003_02_closing.csv with 20 rows\n",
            "Saved ./2003_03_closing.csv with 21 rows\n",
            "Saved ./2003_04_closing.csv with 22 rows\n",
            "Saved ./2003_05_closing.csv with 22 rows\n",
            "Saved ./2003_06_closing.csv with 21 rows\n",
            "Saved ./2003_07_closing.csv with 23 rows\n",
            "Saved ./2003_08_closing.csv with 21 rows\n",
            "Saved ./2003_09_closing.csv with 22 rows\n",
            "Saved ./2003_10_closing.csv with 23 rows\n",
            "Saved ./2003_11_closing.csv with 20 rows\n",
            "Saved ./2003_12_closing.csv with 23 rows\n",
            "Saved ./2004_01_closing.csv with 21 rows\n",
            "Saved ./2004_02_closing.csv with 20 rows\n",
            "Saved ./2004_03_closing.csv with 23 rows\n",
            "Saved ./2004_04_closing.csv with 22 rows\n",
            "Saved ./2004_05_closing.csv with 21 rows\n",
            "Saved ./2004_06_closing.csv with 22 rows\n",
            "Saved ./2004_07_closing.csv with 22 rows\n",
            "Saved ./2004_08_closing.csv with 22 rows\n",
            "Saved ./2004_09_closing.csv with 22 rows\n",
            "Saved ./2004_10_closing.csv with 21 rows\n",
            "Saved ./2004_11_closing.csv with 22 rows\n",
            "Saved ./2004_12_closing.csv with 23 rows\n",
            "Saved ./2005_01_closing.csv with 21 rows\n",
            "Saved ./2005_02_closing.csv with 20 rows\n",
            "Saved ./2005_03_closing.csv with 23 rows\n",
            "Saved ./2005_04_closing.csv with 21 rows\n",
            "Saved ./2005_05_closing.csv with 22 rows\n",
            "Saved ./2005_06_closing.csv with 22 rows\n",
            "Saved ./2005_07_closing.csv with 21 rows\n",
            "Saved ./2005_08_closing.csv with 23 rows\n",
            "Saved ./2005_09_closing.csv with 22 rows\n",
            "Saved ./2005_10_closing.csv with 21 rows\n",
            "Saved ./2005_11_closing.csv with 22 rows\n",
            "Saved ./2005_12_closing.csv with 22 rows\n",
            "Saved ./2006_01_closing.csv with 22 rows\n",
            "Saved ./2006_02_closing.csv with 20 rows\n",
            "Saved ./2006_03_closing.csv with 23 rows\n",
            "Saved ./2006_04_closing.csv with 20 rows\n",
            "Saved ./2006_05_closing.csv with 23 rows\n",
            "Saved ./2006_06_closing.csv with 22 rows\n",
            "Saved ./2006_07_closing.csv with 21 rows\n",
            "Saved ./2006_08_closing.csv with 23 rows\n",
            "Saved ./2006_09_closing.csv with 21 rows\n",
            "Saved ./2006_10_closing.csv with 22 rows\n",
            "Saved ./2006_11_closing.csv with 22 rows\n",
            "Saved ./2006_12_closing.csv with 21 rows\n",
            "Saved ./2007_01_closing.csv with 22 rows\n",
            "Saved ./2007_02_closing.csv with 20 rows\n",
            "Saved ./2007_03_closing.csv with 22 rows\n",
            "Saved ./2007_04_closing.csv with 21 rows\n",
            "Saved ./2007_05_closing.csv with 23 rows\n",
            "Saved ./2007_06_closing.csv with 21 rows\n",
            "Saved ./2007_07_closing.csv with 22 rows\n",
            "Saved ./2007_08_closing.csv with 23 rows\n",
            "Saved ./2007_09_closing.csv with 20 rows\n",
            "Saved ./2007_10_closing.csv with 23 rows\n",
            "Saved ./2007_11_closing.csv with 22 rows\n",
            "Saved ./2007_12_closing.csv with 21 rows\n",
            "Saved ./2008_01_closing.csv with 23 rows\n",
            "Saved ./2008_02_closing.csv with 21 rows\n",
            "Saved ./2008_03_closing.csv with 21 rows\n",
            "Saved ./2008_04_closing.csv with 22 rows\n",
            "Saved ./2008_05_closing.csv with 22 rows\n",
            "Saved ./2008_06_closing.csv with 21 rows\n",
            "Saved ./2008_07_closing.csv with 23 rows\n",
            "Saved ./2008_08_closing.csv with 21 rows\n",
            "Saved ./2008_09_closing.csv with 22 rows\n",
            "Saved ./2008_10_closing.csv with 23 rows\n",
            "Saved ./2008_11_closing.csv with 20 rows\n",
            "Saved ./2008_12_closing.csv with 23 rows\n",
            "Saved ./2009_01_closing.csv with 21 rows\n",
            "Saved ./2009_02_closing.csv with 20 rows\n",
            "Saved ./2009_03_closing.csv with 22 rows\n",
            "Saved ./2009_04_closing.csv with 22 rows\n",
            "Saved ./2009_05_closing.csv with 21 rows\n",
            "Saved ./2009_06_closing.csv with 22 rows\n",
            "Saved ./2009_07_closing.csv with 23 rows\n",
            "Saved ./2009_08_closing.csv with 21 rows\n",
            "Saved ./2009_09_closing.csv with 22 rows\n",
            "Saved ./2009_10_closing.csv with 22 rows\n",
            "Saved ./2009_11_closing.csv with 21 rows\n",
            "Saved ./2009_12_closing.csv with 23 rows\n",
            "Saved ./2010_01_closing.csv with 20 rows\n",
            "Saved ./2010_02_closing.csv with 20 rows\n",
            "Saved ./2010_03_closing.csv with 23 rows\n",
            "Saved ./2010_04_closing.csv with 22 rows\n",
            "Saved ./2010_05_closing.csv with 21 rows\n",
            "Saved ./2010_06_closing.csv with 22 rows\n",
            "Saved ./2010_07_closing.csv with 22 rows\n",
            "Saved ./2010_08_closing.csv with 22 rows\n",
            "Saved ./2010_09_closing.csv with 22 rows\n",
            "Saved ./2010_10_closing.csv with 21 rows\n",
            "Saved ./2010_11_closing.csv with 22 rows\n",
            "Saved ./2010_12_closing.csv with 23 rows\n",
            "Saved ./2011_01_closing.csv with 21 rows\n",
            "Saved ./2011_02_closing.csv with 20 rows\n",
            "Saved ./2011_03_closing.csv with 23 rows\n",
            "Saved ./2011_04_closing.csv with 21 rows\n",
            "Saved ./2011_05_closing.csv with 22 rows\n",
            "Saved ./2011_06_closing.csv with 22 rows\n",
            "Saved ./2011_07_closing.csv with 21 rows\n",
            "Saved ./2011_08_closing.csv with 23 rows\n",
            "Saved ./2011_09_closing.csv with 22 rows\n",
            "Saved ./2011_10_closing.csv with 21 rows\n",
            "Saved ./2011_11_closing.csv with 22 rows\n",
            "Saved ./2011_12_closing.csv with 22 rows\n",
            "Saved ./2012_01_closing.csv with 22 rows\n",
            "Saved ./2012_02_closing.csv with 21 rows\n",
            "Saved ./2012_03_closing.csv with 22 rows\n",
            "Saved ./2012_04_closing.csv with 21 rows\n",
            "Saved ./2012_05_closing.csv with 23 rows\n",
            "Saved ./2012_06_closing.csv with 21 rows\n",
            "Saved ./2012_07_closing.csv with 22 rows\n",
            "Saved ./2012_08_closing.csv with 23 rows\n",
            "Saved ./2012_09_closing.csv with 20 rows\n",
            "Saved ./2012_10_closing.csv with 23 rows\n",
            "Saved ./2012_11_closing.csv with 22 rows\n",
            "Saved ./2012_12_closing.csv with 21 rows\n",
            "Saved ./2013_01_closing.csv with 22 rows\n",
            "Saved ./2013_02_closing.csv with 20 rows\n",
            "Saved ./2013_03_closing.csv with 21 rows\n",
            "Saved ./2013_04_closing.csv with 22 rows\n",
            "Saved ./2013_05_closing.csv with 23 rows\n",
            "Saved ./2013_06_closing.csv with 20 rows\n",
            "Saved ./2013_07_closing.csv with 23 rows\n",
            "Saved ./2013_08_closing.csv with 22 rows\n",
            "Saved ./2013_09_closing.csv with 21 rows\n",
            "Saved ./2013_10_closing.csv with 23 rows\n",
            "Saved ./2013_11_closing.csv with 21 rows\n",
            "Saved ./2013_12_closing.csv with 22 rows\n",
            "Saved ./2014_01_closing.csv with 23 rows\n",
            "Saved ./2014_02_closing.csv with 20 rows\n",
            "Saved ./2014_03_closing.csv with 21 rows\n",
            "Saved ./2014_04_closing.csv with 22 rows\n",
            "Saved ./2014_05_closing.csv with 22 rows\n",
            "Saved ./2014_06_closing.csv with 21 rows\n",
            "Saved ./2014_07_closing.csv with 23 rows\n",
            "Saved ./2014_08_closing.csv with 21 rows\n",
            "Saved ./2014_09_closing.csv with 22 rows\n",
            "Saved ./2014_10_closing.csv with 23 rows\n",
            "Saved ./2014_11_closing.csv with 20 rows\n",
            "Saved ./2014_12_closing.csv with 22 rows\n",
            "Saved ./2015_01_closing.csv with 21 rows\n",
            "Saved ./2015_02_closing.csv with 20 rows\n",
            "Saved ./2015_03_closing.csv with 22 rows\n",
            "Saved ./2015_04_closing.csv with 22 rows\n",
            "Saved ./2015_05_closing.csv with 21 rows\n",
            "Saved ./2015_06_closing.csv with 22 rows\n",
            "Saved ./2015_07_closing.csv with 23 rows\n",
            "Saved ./2015_08_closing.csv with 21 rows\n",
            "Saved ./2015_09_closing.csv with 22 rows\n",
            "Saved ./2015_10_closing.csv with 22 rows\n",
            "Saved ./2015_11_closing.csv with 21 rows\n",
            "Saved ./2015_12_closing.csv with 23 rows\n",
            "Saved ./2016_01_closing.csv with 20 rows\n",
            "Saved ./2016_02_closing.csv with 21 rows\n",
            "Saved ./2016_03_closing.csv with 23 rows\n",
            "Saved ./2016_04_closing.csv with 21 rows\n",
            "Saved ./2016_05_closing.csv with 22 rows\n",
            "Saved ./2016_06_closing.csv with 22 rows\n",
            "Saved ./2016_07_closing.csv with 21 rows\n",
            "Saved ./2016_08_closing.csv with 23 rows\n",
            "Saved ./2016_09_closing.csv with 22 rows\n",
            "Saved ./2016_10_closing.csv with 21 rows\n",
            "Saved ./2016_11_closing.csv with 22 rows\n",
            "Saved ./2016_12_closing.csv with 22 rows\n",
            "Saved ./2017_01_closing.csv with 22 rows\n",
            "Saved ./2017_02_closing.csv with 20 rows\n",
            "Saved ./2017_03_closing.csv with 23 rows\n",
            "Saved ./2017_04_closing.csv with 20 rows\n",
            "Saved ./2017_05_closing.csv with 23 rows\n",
            "Saved ./2017_06_closing.csv with 22 rows\n",
            "Saved ./2017_07_closing.csv with 21 rows\n",
            "Saved ./2017_08_closing.csv with 23 rows\n",
            "Saved ./2017_09_closing.csv with 21 rows\n",
            "Saved ./2017_10_closing.csv with 22 rows\n",
            "Saved ./2017_11_closing.csv with 22 rows\n",
            "Saved ./2017_12_closing.csv with 21 rows\n",
            "Saved ./2018_01_closing.csv with 23 rows\n",
            "Saved ./2018_02_closing.csv with 20 rows\n",
            "Saved ./2018_03_closing.csv with 22 rows\n",
            "Saved ./2018_04_closing.csv with 21 rows\n",
            "Saved ./2018_05_closing.csv with 23 rows\n",
            "Saved ./2018_06_closing.csv with 21 rows\n",
            "Saved ./2018_07_closing.csv with 22 rows\n",
            "Saved ./2018_08_closing.csv with 23 rows\n",
            "Saved ./2018_09_closing.csv with 20 rows\n",
            "Saved ./2018_10_closing.csv with 23 rows\n",
            "Saved ./2018_11_closing.csv with 22 rows\n",
            "Saved ./2018_12_closing.csv with 21 rows\n",
            "Saved ./2019_01_closing.csv with 22 rows\n",
            "Saved ./2019_02_closing.csv with 20 rows\n",
            "Saved ./2019_03_closing.csv with 21 rows\n",
            "Saved ./2019_04_closing.csv with 22 rows\n",
            "Saved ./2019_05_closing.csv with 23 rows\n",
            "Saved ./2019_06_closing.csv with 20 rows\n",
            "Saved ./2019_07_closing.csv with 23 rows\n",
            "Saved ./2019_08_closing.csv with 22 rows\n",
            "Saved ./2019_09_closing.csv with 21 rows\n",
            "Saved ./2019_10_closing.csv with 23 rows\n",
            "Saved ./2019_11_closing.csv with 21 rows\n",
            "Saved ./2019_12_closing.csv with 22 rows\n",
            "Saved ./2020_01_closing.csv with 23 rows\n",
            "Saved ./2020_02_closing.csv with 20 rows\n",
            "Saved ./2020_03_closing.csv with 22 rows\n",
            "Saved ./2020_04_closing.csv with 22 rows\n",
            "Saved ./2020_05_closing.csv with 21 rows\n",
            "Saved ./2020_06_closing.csv with 22 rows\n",
            "Saved ./2020_07_closing.csv with 23 rows\n",
            "Saved ./2020_08_closing.csv with 21 rows\n",
            "Saved ./2020_09_closing.csv with 22 rows\n",
            "Saved ./2020_10_closing.csv with 22 rows\n",
            "Saved ./2020_11_closing.csv with 21 rows\n",
            "Saved ./2020_12_closing.csv with 23 rows\n",
            "Saved ./2021_01_closing.csv with 21 rows\n",
            "Saved ./2021_02_closing.csv with 20 rows\n",
            "Saved ./2021_03_closing.csv with 23 rows\n",
            "Saved ./2021_04_closing.csv with 22 rows\n",
            "Saved ./2021_05_closing.csv with 21 rows\n",
            "Saved ./2021_06_closing.csv with 22 rows\n",
            "Saved ./2021_07_closing.csv with 22 rows\n",
            "Saved ./2021_08_closing.csv with 22 rows\n",
            "Saved ./2021_09_closing.csv with 22 rows\n",
            "Saved ./2021_10_closing.csv with 21 rows\n",
            "Saved ./2021_11_closing.csv with 22 rows\n",
            "Saved ./2021_12_closing.csv with 23 rows\n",
            "Saved ./2022_01_closing.csv with 21 rows\n",
            "Saved ./2022_02_closing.csv with 20 rows\n",
            "Saved ./2022_03_closing.csv with 23 rows\n",
            "Saved ./2022_04_closing.csv with 21 rows\n",
            "Saved ./2022_05_closing.csv with 22 rows\n",
            "Saved ./2022_06_closing.csv with 22 rows\n",
            "Saved ./2022_07_closing.csv with 21 rows\n",
            "Saved ./2022_08_closing.csv with 23 rows\n",
            "Saved ./2022_09_closing.csv with 22 rows\n",
            "Saved ./2022_10_closing.csv with 21 rows\n",
            "Saved ./2022_11_closing.csv with 22 rows\n",
            "Saved ./2022_12_closing.csv with 22 rows\n",
            "Saved ./2023_01_closing.csv with 22 rows\n",
            "Saved ./2023_02_closing.csv with 20 rows\n",
            "Saved ./2023_03_closing.csv with 23 rows\n",
            "Saved ./2023_04_closing.csv with 20 rows\n",
            "Saved ./2023_05_closing.csv with 23 rows\n",
            "Saved ./2023_06_closing.csv with 22 rows\n",
            "Saved ./2023_07_closing.csv with 21 rows\n",
            "Saved ./2023_08_closing.csv with 23 rows\n",
            "Saved ./2023_09_closing.csv with 21 rows\n",
            "Saved ./2023_10_closing.csv with 22 rows\n",
            "Saved ./2023_11_closing.csv with 22 rows\n",
            "Saved ./2023_12_closing.csv with 21 rows\n",
            "Saved ./2024_01_closing.csv with 23 rows\n",
            "Saved ./2024_02_closing.csv with 21 rows\n",
            "Saved ./2024_03_closing.csv with 21 rows\n",
            "Saved ./2024_04_closing.csv with 22 rows\n",
            "Saved ./2024_05_closing.csv with 23 rows\n",
            "Saved ./2024_06_closing.csv with 20 rows\n",
            "Saved ./2024_07_closing.csv with 23 rows\n",
            "Saved ./2024_08_closing.csv with 22 rows\n",
            "Saved ./2024_09_closing.csv with 21 rows\n",
            "Saved ./2024_10_closing.csv with 23 rows\n",
            "Saved ./2024_11_closing.csv with 21 rows\n",
            "Saved ./2024_12_closing.csv with 21 rows\n",
            "All monthly CSVs generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def split_normalized_by_month(input_csv, output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Split normalized data CSV into monthly CSVs named {yyyy}_{mm}_closing.csv.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'normalized_data.csv')\n",
        "    output_dir (str): Directory to save output CSVs (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves monthly CSVs to the specified directory\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"Error: Input CSV is empty.\")\n",
        "            return\n",
        "\n",
        "        # Group data by year and month\n",
        "        grouped = df.groupby([df.index.year, df.index.month])\n",
        "\n",
        "        # Iterate through each year-month group\n",
        "        for (year, month), group_data in grouped:\n",
        "            # Format the output filename (e.g., 2003_01_closing.csv)\n",
        "            output_file = f\"{output_dir}/{year}_{month:02d}_closing.csv\"\n",
        "\n",
        "            # Save the group data to a CSV\n",
        "            group_data.to_csv(output_file)\n",
        "            print(f\"Saved {output_file} with {len(group_data)} rows\")\n",
        "\n",
        "            # Warn if the group has NaN values\n",
        "            if group_data.isna().any().any():\n",
        "                print(f\"Warning: {output_file} contains NaN values.\")\n",
        "                nan_columns = group_data.columns[group_data.isna().any()].tolist()\n",
        "                print(f\"Columns with NaN: {nan_columns}\")\n",
        "\n",
        "        print(\"All monthly CSVs generated successfully.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    split_normalized_by_month(\"normalized_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4CHBCo9Pwmx"
      },
      "source": [
        "4. Compute metrics for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boDhdB5Jh1tw",
        "outputId": "098f5e90-c313-4568-9fba-b28efd6c1cf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved ./2017_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_09_corr.csv with correlation matrix\n",
            "Saved ./2005_05_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_05_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_05_corr.csv with correlation matrix\n",
            "Warning: ./2005_05_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2011_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_07_corr.csv with correlation matrix\n",
            "Saved ./2011_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_03_corr.csv with correlation matrix\n",
            "Saved ./2004_08_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_08_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_08_corr.csv with correlation matrix\n",
            "Warning: ./2004_08_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2016_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_09_corr.csv with correlation matrix\n",
            "Saved ./2006_08_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_08_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_08_corr.csv with correlation matrix\n",
            "Warning: ./2006_08_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2016_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_02_corr.csv with correlation matrix\n",
            "Saved ./2022_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_03_corr.csv with correlation matrix\n",
            "Saved ./2010_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_02_corr.csv with correlation matrix\n",
            "Saved ./2018_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_10_corr.csv with correlation matrix\n",
            "Saved ./2004_09_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_09_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_09_corr.csv with correlation matrix\n",
            "Warning: ./2004_09_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2009_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_01_corr.csv with correlation matrix\n",
            "Saved ./2013_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_05_corr.csv with correlation matrix\n",
            "Saved ./2024_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_12_corr.csv with correlation matrix\n",
            "Saved ./2015_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_12_corr.csv with correlation matrix\n",
            "Saved ./2024_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_08_corr.csv with correlation matrix\n",
            "Saved ./2003_03_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_03_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_03_corr.csv with correlation matrix\n",
            "Warning: ./2003_03_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2021_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_05_corr.csv with correlation matrix\n",
            "Saved ./2022_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_06_corr.csv with correlation matrix\n",
            "Saved ./2008_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_10_corr.csv with correlation matrix\n",
            "Saved ./2011_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_12_corr.csv with correlation matrix\n",
            "Saved ./2022_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_07_corr.csv with correlation matrix\n",
            "Saved ./2021_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_10_corr.csv with correlation matrix\n",
            "Saved ./2023_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_06_corr.csv with correlation matrix\n",
            "Saved ./2012_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_02_corr.csv with correlation matrix\n",
            "Saved ./2021_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_03_corr.csv with correlation matrix\n",
            "Saved ./2018_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_01_corr.csv with correlation matrix\n",
            "Saved ./2011_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_01_corr.csv with correlation matrix\n",
            "Saved ./2014_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_07_corr.csv with correlation matrix\n",
            "Saved ./2016_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_05_corr.csv with correlation matrix\n",
            "Saved ./2022_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_04_corr.csv with correlation matrix\n",
            "Saved ./2007_09_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_09_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_09_corr.csv with correlation matrix\n",
            "Saved ./2006_09_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_09_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_09_corr.csv with correlation matrix\n",
            "Warning: ./2006_09_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2021_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_07_corr.csv with correlation matrix\n",
            "Saved ./2012_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_07_corr.csv with correlation matrix\n",
            "Saved ./2010_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_01_corr.csv with correlation matrix\n",
            "Saved ./2020_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_02_corr.csv with correlation matrix\n",
            "Saved ./2015_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_09_corr.csv with correlation matrix\n",
            "Saved ./2016_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_08_corr.csv with correlation matrix\n",
            "Saved ./2011_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_06_corr.csv with correlation matrix\n",
            "Saved ./2023_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_08_corr.csv with correlation matrix\n",
            "Saved ./2003_12_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_12_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_12_corr.csv with correlation matrix\n",
            "Warning: ./2003_12_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2023_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_09_corr.csv with correlation matrix\n",
            "Saved ./2020_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_09_corr.csv with correlation matrix\n",
            "Saved ./2007_04_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_04_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_04_corr.csv with correlation matrix\n",
            "Warning: ./2007_04_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2006_01_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_01_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_01_corr.csv with correlation matrix\n",
            "Warning: ./2006_01_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2014_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_04_corr.csv with correlation matrix\n",
            "Saved ./2005_06_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_06_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_06_corr.csv with correlation matrix\n",
            "Warning: ./2005_06_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2005_08_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_08_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_08_corr.csv with correlation matrix\n",
            "Warning: ./2005_08_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2020_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_03_corr.csv with correlation matrix\n",
            "Saved ./2004_12_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_12_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_12_corr.csv with correlation matrix\n",
            "Warning: ./2004_12_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2020_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_11_corr.csv with correlation matrix\n",
            "Saved ./2017_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_04_corr.csv with correlation matrix\n",
            "Saved ./2013_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_07_corr.csv with correlation matrix\n",
            "Saved ./2010_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_05_corr.csv with correlation matrix\n",
            "Saved ./2021_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_12_corr.csv with correlation matrix\n",
            "Saved ./2013_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_04_corr.csv with correlation matrix\n",
            "Saved ./2013_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_10_corr.csv with correlation matrix\n",
            "Saved ./2006_04_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_04_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_04_corr.csv with correlation matrix\n",
            "Warning: ./2006_04_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_05_corr.csv with correlation matrix\n",
            "Saved ./2023_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_01_corr.csv with correlation matrix\n",
            "Saved ./2006_06_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_06_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_06_corr.csv with correlation matrix\n",
            "Warning: ./2006_06_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2009_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_07_corr.csv with correlation matrix\n",
            "Saved ./2007_06_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_06_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_06_corr.csv with correlation matrix\n",
            "Warning: ./2007_06_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2018_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_04_corr.csv with correlation matrix\n",
            "Saved ./2006_03_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_03_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_03_corr.csv with correlation matrix\n",
            "Warning: ./2006_03_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2014_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_10_corr.csv with correlation matrix\n",
            "Saved ./2013_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_06_corr.csv with correlation matrix\n",
            "Saved ./2022_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_01_corr.csv with correlation matrix\n",
            "Saved ./2023_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_10_corr.csv with correlation matrix\n",
            "Saved ./2015_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_04_corr.csv with correlation matrix\n",
            "Saved ./2003_08_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_08_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_08_corr.csv with correlation matrix\n",
            "Warning: ./2003_08_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_09_corr.csv with correlation matrix\n",
            "Saved ./2022_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_11_corr.csv with correlation matrix\n",
            "Saved ./2008_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_02_corr.csv with correlation matrix\n",
            "Saved ./2016_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_06_corr.csv with correlation matrix\n",
            "Saved ./2007_01_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_01_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_01_corr.csv with correlation matrix\n",
            "Warning: ./2007_01_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2005_04_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_04_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_04_corr.csv with correlation matrix\n",
            "Warning: ./2005_04_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2005_07_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_07_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_07_corr.csv with correlation matrix\n",
            "Warning: ./2005_07_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2003_06_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_06_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_06_corr.csv with correlation matrix\n",
            "Warning: ./2003_06_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2007_05_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_05_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_05_corr.csv with correlation matrix\n",
            "Warning: ./2007_05_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2015_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_10_corr.csv with correlation matrix\n",
            "Saved ./2003_01_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_01_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_01_corr.csv with correlation matrix\n",
            "Warning: ./2003_01_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2019_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_10_corr.csv with correlation matrix\n",
            "Saved ./2011_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_10_corr.csv with correlation matrix\n",
            "Saved ./2018_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_08_corr.csv with correlation matrix\n",
            "Saved ./2018_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_11_corr.csv with correlation matrix\n",
            "Saved ./2011_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_09_corr.csv with correlation matrix\n",
            "Saved ./2006_11_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_11_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_11_corr.csv with correlation matrix\n",
            "Warning: ./2006_11_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_11_corr.csv with correlation matrix\n",
            "Saved ./2007_03_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_03_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_03_corr.csv with correlation matrix\n",
            "Warning: ./2007_03_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2020_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_06_corr.csv with correlation matrix\n",
            "Saved ./2014_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_08_corr.csv with correlation matrix\n",
            "Saved ./2021_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_04_corr.csv with correlation matrix\n",
            "Saved ./2009_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_02_corr.csv with correlation matrix\n",
            "Saved ./2005_01_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_01_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_01_corr.csv with correlation matrix\n",
            "Warning: ./2005_01_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2010_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_03_corr.csv with correlation matrix\n",
            "Saved ./2007_02_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_02_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_02_corr.csv with correlation matrix\n",
            "Warning: ./2007_02_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2019_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_12_corr.csv with correlation matrix\n",
            "Saved ./2016_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_01_corr.csv with correlation matrix\n",
            "Saved ./2014_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_03_corr.csv with correlation matrix\n",
            "Saved ./2012_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_12_corr.csv with correlation matrix\n",
            "Saved ./2004_10_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_10_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_10_corr.csv with correlation matrix\n",
            "Warning: ./2004_10_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2023_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_04_corr.csv with correlation matrix\n",
            "Saved ./2014_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_02_corr.csv with correlation matrix\n",
            "Saved ./2010_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_09_corr.csv with correlation matrix\n",
            "Saved ./2017_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_11_corr.csv with correlation matrix\n",
            "Saved ./2011_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_02_corr.csv with correlation matrix\n",
            "Saved ./2018_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_12_corr.csv with correlation matrix\n",
            "Saved ./2013_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_02_corr.csv with correlation matrix\n",
            "Saved ./2004_01_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_01_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_01_corr.csv with correlation matrix\n",
            "Warning: ./2004_01_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2021_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_06_corr.csv with correlation matrix\n",
            "Saved ./2022_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_10_corr.csv with correlation matrix\n",
            "Saved ./2007_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2007_11_corr.csv with correlation matrix\n",
            "Saved ./2019_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_04_corr.csv with correlation matrix\n",
            "Saved ./2013_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_11_corr.csv with correlation matrix\n",
            "Saved ./2011_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_11_corr.csv with correlation matrix\n",
            "Saved ./2005_02_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_02_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_02_corr.csv with correlation matrix\n",
            "Warning: ./2005_02_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2004_04_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_04_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_04_corr.csv with correlation matrix\n",
            "Warning: ./2004_04_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2017_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_01_corr.csv with correlation matrix\n",
            "Saved ./2019_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_01_corr.csv with correlation matrix\n",
            "Saved ./2017_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_10_corr.csv with correlation matrix\n",
            "Saved ./2016_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_10_corr.csv with correlation matrix\n",
            "Saved ./2004_02_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_02_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_02_corr.csv with correlation matrix\n",
            "Warning: ./2004_02_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_06_corr.csv with correlation matrix\n",
            "Saved ./2022_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_09_corr.csv with correlation matrix\n",
            "Saved ./2018_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_06_corr.csv with correlation matrix\n",
            "Saved ./2022_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_02_corr.csv with correlation matrix\n",
            "Saved ./2016_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_03_corr.csv with correlation matrix\n",
            "Saved ./2016_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_04_corr.csv with correlation matrix\n",
            "Saved ./2016_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_07_corr.csv with correlation matrix\n",
            "Saved ./2012_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_04_corr.csv with correlation matrix\n",
            "Saved ./2020_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_07_corr.csv with correlation matrix\n",
            "Saved ./2020_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_12_corr.csv with correlation matrix\n",
            "Saved ./2016_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_11_corr.csv with correlation matrix\n",
            "Saved ./2010_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_08_corr.csv with correlation matrix\n",
            "Saved ./2017_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_12_corr.csv with correlation matrix\n",
            "Saved ./2010_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_12_corr.csv with correlation matrix\n",
            "Saved ./2024_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_02_corr.csv with correlation matrix\n",
            "Saved ./2018_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_07_corr.csv with correlation matrix\n",
            "Saved ./2009_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_06_corr.csv with correlation matrix\n",
            "Saved ./2017_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_02_corr.csv with correlation matrix\n",
            "Saved ./2008_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_09_corr.csv with correlation matrix\n",
            "Saved ./2014_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_05_corr.csv with correlation matrix\n",
            "Saved ./2006_05_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_05_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_05_corr.csv with correlation matrix\n",
            "Warning: ./2006_05_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2017_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_06_corr.csv with correlation matrix\n",
            "Saved ./2024_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_01_corr.csv with correlation matrix\n",
            "Saved ./2015_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_03_corr.csv with correlation matrix\n",
            "Saved ./2010_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_10_corr.csv with correlation matrix\n",
            "Saved ./2007_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2007_12_corr.csv with correlation matrix\n",
            "Saved ./2009_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_11_corr.csv with correlation matrix\n",
            "Saved ./2009_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_03_corr.csv with correlation matrix\n",
            "Saved ./2003_05_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_05_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_05_corr.csv with correlation matrix\n",
            "Warning: ./2003_05_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2015_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_05_corr.csv with correlation matrix\n",
            "Saved ./2017_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_03_corr.csv with correlation matrix\n",
            "Saved ./2018_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_05_corr.csv with correlation matrix\n",
            "Saved ./2005_10_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_10_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_10_corr.csv with correlation matrix\n",
            "Warning: ./2005_10_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2005_11_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_11_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_11_corr.csv with correlation matrix\n",
            "Warning: ./2005_11_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_08_corr.csv with correlation matrix\n",
            "Saved ./2024_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_04_corr.csv with correlation matrix\n",
            "Saved ./2011_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_04_corr.csv with correlation matrix\n",
            "Saved ./2021_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_11_corr.csv with correlation matrix\n",
            "Saved ./2020_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_08_corr.csv with correlation matrix\n",
            "Saved ./2022_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_05_corr.csv with correlation matrix\n",
            "Saved ./2015_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_06_corr.csv with correlation matrix\n",
            "Saved ./2009_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_08_corr.csv with correlation matrix\n",
            "Saved ./2003_02_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_02_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_02_corr.csv with correlation matrix\n",
            "Warning: ./2003_02_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2006_12_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_12_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_12_corr.csv with correlation matrix\n",
            "Warning: ./2006_12_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_03_corr.csv with correlation matrix\n",
            "Saved ./2009_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_05_corr.csv with correlation matrix\n",
            "Saved ./2010_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_06_corr.csv with correlation matrix\n",
            "Saved ./2008_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_11_corr.csv with correlation matrix\n",
            "Saved ./2015_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_01_corr.csv with correlation matrix\n",
            "Saved ./2023_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_02_corr.csv with correlation matrix\n",
            "Saved ./2018_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_09_corr.csv with correlation matrix\n",
            "Saved ./2015_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_08_corr.csv with correlation matrix\n",
            "Saved ./2004_06_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_06_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_06_corr.csv with correlation matrix\n",
            "Warning: ./2004_06_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2021_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_09_corr.csv with correlation matrix\n",
            "Saved ./2012_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_01_corr.csv with correlation matrix\n",
            "Saved ./2024_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_07_corr.csv with correlation matrix\n",
            "Saved ./2010_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_07_corr.csv with correlation matrix\n",
            "Saved ./2010_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_11_corr.csv with correlation matrix\n",
            "Saved ./2017_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_08_corr.csv with correlation matrix\n",
            "Saved ./2013_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_03_corr.csv with correlation matrix\n",
            "Saved ./2015_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_07_corr.csv with correlation matrix\n",
            "Saved ./2009_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_09_corr.csv with correlation matrix\n",
            "Saved ./2023_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_11_corr.csv with correlation matrix\n",
            "Saved ./2024_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_10_corr.csv with correlation matrix\n",
            "Saved ./2004_05_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_05_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_05_corr.csv with correlation matrix\n",
            "Warning: ./2004_05_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2023_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_12_corr.csv with correlation matrix\n",
            "Saved ./2003_04_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_04_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_04_corr.csv with correlation matrix\n",
            "Warning: ./2003_04_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2022_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_08_corr.csv with correlation matrix\n",
            "Saved ./2018_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_03_corr.csv with correlation matrix\n",
            "Saved ./2003_07_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_07_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_07_corr.csv with correlation matrix\n",
            "Warning: ./2003_07_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2007_07_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_07_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_07_corr.csv with correlation matrix\n",
            "Warning: ./2007_07_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2020_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_01_corr.csv with correlation matrix\n",
            "Saved ./2008_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_04_corr.csv with correlation matrix\n",
            "Saved ./2019_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_07_corr.csv with correlation matrix\n",
            "Saved ./2007_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2007_10_corr.csv with correlation matrix\n",
            "Saved ./2003_10_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_10_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_10_corr.csv with correlation matrix\n",
            "Warning: ./2003_10_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2017_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_05_corr.csv with correlation matrix\n",
            "Saved ./2019_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_03_corr.csv with correlation matrix\n",
            "Saved ./2014_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_11_corr.csv with correlation matrix\n",
            "Saved ./2008_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_03_corr.csv with correlation matrix\n",
            "Saved ./2014_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_01_corr.csv with correlation matrix\n",
            "Saved ./2015_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_02_corr.csv with correlation matrix\n",
            "Saved ./2011_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_08_corr.csv with correlation matrix\n",
            "Saved ./2003_09_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_09_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_09_corr.csv with correlation matrix\n",
            "Warning: ./2003_09_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2018_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2018_02_corr.csv with correlation matrix\n",
            "Saved ./2008_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_07_corr.csv with correlation matrix\n",
            "Saved ./2015_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2015_11_corr.csv with correlation matrix\n",
            "Saved ./2014_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_09_corr.csv with correlation matrix\n",
            "Saved ./2019_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_11_corr.csv with correlation matrix\n",
            "Saved ./2022_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2022_12_corr.csv with correlation matrix\n",
            "Saved ./2006_10_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_10_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_10_corr.csv with correlation matrix\n",
            "Warning: ./2006_10_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2019_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_05_corr.csv with correlation matrix\n",
            "Saved ./2014_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_12_corr.csv with correlation matrix\n",
            "Saved ./2024_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_05_corr.csv with correlation matrix\n",
            "Saved ./2007_08_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2007_08_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2007_08_corr.csv with correlation matrix\n",
            "Warning: ./2007_08_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2013_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_08_corr.csv with correlation matrix\n",
            "Saved ./2008_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_01_corr.csv with correlation matrix\n",
            "Saved ./2009_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_04_corr.csv with correlation matrix\n",
            "Saved ./2023_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_07_corr.csv with correlation matrix\n",
            "Saved ./2011_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2011_05_corr.csv with correlation matrix\n",
            "Saved ./2009_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_10_corr.csv with correlation matrix\n",
            "Saved ./2024_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_06_corr.csv with correlation matrix\n",
            "Saved ./2005_03_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_03_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_03_corr.csv with correlation matrix\n",
            "Warning: ./2005_03_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2003_11_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2003_11_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2003_11_corr.csv with correlation matrix\n",
            "Warning: ./2003_11_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2021_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_01_corr.csv with correlation matrix\n",
            "Saved ./2008_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_12_corr.csv with correlation matrix\n",
            "Saved ./2009_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2009_12_corr.csv with correlation matrix\n",
            "Saved ./2013_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_09_corr.csv with correlation matrix\n",
            "Saved ./2020_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_10_corr.csv with correlation matrix\n",
            "Saved ./2004_11_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_11_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_11_corr.csv with correlation matrix\n",
            "Warning: ./2004_11_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2008_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_05_corr.csv with correlation matrix\n",
            "Saved ./2019_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_02_corr.csv with correlation matrix\n",
            "Saved ./2008_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_06_corr.csv with correlation matrix\n",
            "Saved ./2004_03_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_03_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_03_corr.csv with correlation matrix\n",
            "Warning: ./2004_03_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2013_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_12_corr.csv with correlation matrix\n",
            "Saved ./2013_01_obs.csv with metrics for 14 tickers\n",
            "Saved ./2013_01_corr.csv with correlation matrix\n",
            "Saved ./2019_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_06_corr.csv with correlation matrix\n",
            "Saved ./2021_02_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_02_corr.csv with correlation matrix\n",
            "Saved ./2024_11_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_11_corr.csv with correlation matrix\n",
            "Saved ./2004_07_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2004_07_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2004_07_corr.csv with correlation matrix\n",
            "Warning: ./2004_07_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2006_02_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_02_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_02_corr.csv with correlation matrix\n",
            "Warning: ./2006_02_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2021_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2021_08_corr.csv with correlation matrix\n",
            "Saved ./2008_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2008_08_corr.csv with correlation matrix\n",
            "Saved ./2024_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_03_corr.csv with correlation matrix\n",
            "Saved ./2020_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_04_corr.csv with correlation matrix\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log\n",
            "  result = func(self.values, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved ./2023_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_05_corr.csv with correlation matrix\n",
            "Saved ./2005_09_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_09_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_09_corr.csv with correlation matrix\n",
            "Warning: ./2005_09_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2005_12_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2005_12_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2005_12_corr.csv with correlation matrix\n",
            "Warning: ./2005_12_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2012_10_obs.csv with metrics for 14 tickers\n",
            "Saved ./2012_10_corr.csv with correlation matrix\n",
            "Saved ./2020_05_obs.csv with metrics for 14 tickers\n",
            "Saved ./2020_05_corr.csv with correlation matrix\n",
            "Saved ./2019_08_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_08_corr.csv with correlation matrix\n",
            "Saved ./2024_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2024_09_corr.csv with correlation matrix\n",
            "Saved ./2010_04_obs.csv with metrics for 14 tickers\n",
            "Saved ./2010_04_corr.csv with correlation matrix\n",
            "Saved ./2017_07_obs.csv with metrics for 14 tickers\n",
            "Saved ./2017_07_corr.csv with correlation matrix\n",
            "Saved ./2016_12_obs.csv with metrics for 14 tickers\n",
            "Saved ./2016_12_corr.csv with correlation matrix\n",
            "Saved ./2014_06_obs.csv with metrics for 14 tickers\n",
            "Saved ./2014_06_corr.csv with correlation matrix\n",
            "Saved ./2006_07_obs.csv with metrics for 14 tickers\n",
            "Warning: ./2006_07_obs.csv contains NaN values.\n",
            "Metrics with NaN: ['Sharpe_Ratio', 'Sortino_Ratio', 'Calmar_Ratio']\n",
            "Saved ./2006_07_corr.csv with correlation matrix\n",
            "Warning: ./2006_07_corr.csv contains NaN values.\n",
            "Columns with NaN: ['000001.SS', 'CL=F', 'GC=F', 'SI=F', '^BSESN', '^DJI', '^FCHI', '^FTSE', '^GSPC', '^HSI', '^IXIC', '^KS11', '^NSEI', '^STOXX50E']\n",
            "Saved ./2023_03_obs.csv with metrics for 14 tickers\n",
            "Saved ./2023_03_corr.csv with correlation matrix\n",
            "Saved ./2019_09_obs.csv with metrics for 14 tickers\n",
            "Saved ./2019_09_corr.csv with correlation matrix\n",
            "All observation and correlation CSVs generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def calculate_monthly_metrics(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Generate monthly observation and correlation CSVs from normalized closing data.\n",
        "\n",
        "    For each {yyyy}_{mm}_closing.csv, creates:\n",
        "    - {yyyy}_{mm}_obs.csv: First close, last close, volatility, Sharpe, Sortino, Calmar, MDD\n",
        "    - {yyyy}_{mm}_corr.csv: Correlation matrix of daily log returns\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing input CSVs (default: current directory)\n",
        "    output_dir (str): Directory to save output CSVs (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves observation and correlation CSVs to output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find all monthly closing CSVs\n",
        "        closing_files = glob(f\"{input_dir}/*_closing.csv\")\n",
        "        if not closing_files:\n",
        "            print(f\"Error: No *_closing.csv files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        for file in closing_files:\n",
        "            # Extract year and month from filename (e.g., 2003_01_closing.csv)\n",
        "            filename = os.path.basename(file)\n",
        "            year_month = filename.replace(\"_closing.csv\", \"\")\n",
        "            year, month = map(int, year_month.split(\"_\"))\n",
        "\n",
        "            # Read the monthly CSV\n",
        "            df = pd.read_csv(file, index_col='Date', parse_dates=True)\n",
        "\n",
        "            if df.empty:\n",
        "                print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Drop columns with all NaN values\n",
        "            df = df.dropna(axis=1, how='all')\n",
        "            if df.empty:\n",
        "                print(f\"Warning: {filename} has no valid data after dropping NaN columns. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate daily log returns: ln(price_t / price_{t-1})\n",
        "            log_returns = np.log(df / df.shift(1)).dropna()\n",
        "\n",
        "            # Initialize metrics DataFrame\n",
        "            metrics = pd.DataFrame(index=df.columns)\n",
        "\n",
        "            # Month First Close: First non-null normalized price\n",
        "            metrics['Month_First_Close'] = df.apply(lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else np.nan)\n",
        "\n",
        "            # Month Last Close: Last non-null normalized price\n",
        "            metrics['Month_Last_Close'] = df.apply(lambda x: x.dropna().iloc[-1] if x.dropna().size > 0 else np.nan)\n",
        "\n",
        "            # Volatility: Annualized standard deviation of daily log returns\n",
        "            metrics['Volatility'] = log_returns.std() * np.sqrt(252)\n",
        "\n",
        "            # Sharpe Ratio: Annualized mean log return / volatility (risk-free rate = 0)\n",
        "            metrics['Sharpe_Ratio'] = (log_returns.mean() * 252) / metrics['Volatility']\n",
        "\n",
        "            # Sortino Ratio: Annualized mean log return / downside volatility\n",
        "            downside_returns = log_returns.where(log_returns < 0, 0)\n",
        "            downside_vol = downside_returns.std() * np.sqrt(252)\n",
        "            metrics['Sortino_Ratio'] = (log_returns.mean() * 252) / downside_vol\n",
        "\n",
        "            # Maximum Drawdown (MDD): Max peak-to-trough decline\n",
        "            def calculate_mdd(series):\n",
        "                cumulative = series / series.iloc[0]  # Normalize to start at 1\n",
        "                peak = cumulative.cummax()\n",
        "                drawdown = (peak - cumulative) / peak\n",
        "                return drawdown.max() if drawdown.size > 0 else np.nan\n",
        "\n",
        "            metrics['MDD'] = df.apply(calculate_mdd)\n",
        "\n",
        "            # Calmar Ratio: Annualized return / MDD\n",
        "            monthly_return = (metrics['Month_Last_Close'] / metrics['Month_First_Close']) - 1\n",
        "            annualized_return = (1 + monthly_return) ** (12 / 1) - 1  # Annualize monthly return\n",
        "            metrics['Calmar_Ratio'] = annualized_return / metrics['MDD']\n",
        "\n",
        "            # Replace infinities with NaN (e.g., zero volatility or MDD)\n",
        "            metrics = metrics.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "            # Save observation CSV\n",
        "            obs_file = f\"{output_dir}/{year_month}_obs.csv\"\n",
        "            metrics.to_csv(obs_file)\n",
        "            print(f\"Saved {obs_file} with metrics for {len(metrics)} tickers\")\n",
        "\n",
        "            # Warn about NaN values in metrics\n",
        "            if metrics.isna().any().any():\n",
        "                print(f\"Warning: {obs_file} contains NaN values.\")\n",
        "                nan_columns = metrics.columns[metrics.isna().any()].tolist()\n",
        "                print(f\"Metrics with NaN: {nan_columns}\")\n",
        "\n",
        "            # Correlation matrix of daily log returns\n",
        "            if not log_returns.empty:\n",
        "                corr_matrix = log_returns.corr()\n",
        "                corr_file = f\"{output_dir}/{year_month}_corr.csv\"\n",
        "                corr_matrix.to_csv(corr_file)\n",
        "                print(f\"Saved {corr_file} with correlation matrix\")\n",
        "\n",
        "                # Warn about NaN values in correlation matrix\n",
        "                if corr_matrix.isna().any().any():\n",
        "                    print(f\"Warning: {corr_file} contains NaN values.\")\n",
        "                    nan_columns = corr_matrix.columns[corr_matrix.isna().any()].tolist()\n",
        "                    print(f\"Columns with NaN: {nan_columns}\")\n",
        "            else:\n",
        "                print(f\"Warning: No valid log returns for {year_month}. Skipping correlation matrix.\")\n",
        "\n",
        "        print(\"All observation and correlation CSVs generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    calculate_monthly_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl5xByxTjQx-",
        "outputId": "a20d3f3c-71d6-4bb3-ff6d-58180de20d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed normalized_data.csv: Replaced NaN with 0s\n",
            "Processed log_evolution_data.csv: Replaced NaN with 0s\n",
            "Processed portfolio_evolution.csv: Replaced NaN with 0s\n",
            "Processed 2017_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2018_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2015_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2022_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2011_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2003_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2009_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2013_01_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_11_closing.csv: Replaced NaN with 0s\n",
            "Processed 2004_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_02_closing.csv: Replaced NaN with 0s\n",
            "Processed 2021_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2008_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2005_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2012_10_closing.csv: Replaced NaN with 0s\n",
            "Processed 2020_05_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_08_closing.csv: Replaced NaN with 0s\n",
            "Processed 2024_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2010_04_closing.csv: Replaced NaN with 0s\n",
            "Processed 2017_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2016_12_closing.csv: Replaced NaN with 0s\n",
            "Processed 2014_06_closing.csv: Replaced NaN with 0s\n",
            "Processed 2006_07_closing.csv: Replaced NaN with 0s\n",
            "Processed 2023_03_closing.csv: Replaced NaN with 0s\n",
            "Processed 2019_09_closing.csv: Replaced NaN with 0s\n",
            "Processed 2007_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2023_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2007_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2019_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2003_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2008_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2004_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_02_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2011_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2012_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_01_obs.csv: Replaced NaN with 0s\n",
            "Processed 2015_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2024_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_11_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_05_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2016_04_obs.csv: Replaced NaN with 0s\n",
            "Processed 2018_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2013_07_obs.csv: Replaced NaN with 0s\n",
            "Processed 2020_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2005_10_obs.csv: Replaced NaN with 0s\n",
            "Processed 2022_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2017_08_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2009_06_obs.csv: Replaced NaN with 0s\n",
            "Processed 2014_03_obs.csv: Replaced NaN with 0s\n",
            "Processed 2021_09_obs.csv: Replaced NaN with 0s\n",
            "Processed 2006_12_obs.csv: Replaced NaN with 0s\n",
            "Processed 2010_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2003_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2020_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2006_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2019_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2022_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2007_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_12_corr.csv: Replaced NaN with 0s\n",
            "Processed 2015_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2010_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2009_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_05_corr.csv: Replaced NaN with 0s\n",
            "Processed 2004_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2011_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2018_03_corr.csv: Replaced NaN with 0s\n",
            "Processed 2005_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_01_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2014_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2017_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2012_06_corr.csv: Replaced NaN with 0s\n",
            "Processed 2008_02_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_08_corr.csv: Replaced NaN with 0s\n",
            "Processed 2023_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2024_10_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_11_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_07_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2021_04_corr.csv: Replaced NaN with 0s\n",
            "Processed 2016_09_corr.csv: Replaced NaN with 0s\n",
            "Processed 2013_01_corr.csv: Replaced NaN with 0s\n",
            "All specified CSVs processed successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def replace_nan_with_zeros(input_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Replace NaN values with 0s in specified CSV files.\n",
        "\n",
        "    Targets:\n",
        "    - normalized_data.csv\n",
        "    - log_evolution_data.csv\n",
        "    - portfolio_evolution.csv\n",
        "    - {yyyy}_{mm}_closing.csv\n",
        "    - {yyyy}_{mm}_obs.csv\n",
        "    - {yyyy}_{mm}_corr.csv\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the CSV files (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Overwrites original CSVs with NaN replaced by 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # List of specific files to process\n",
        "        specific_files = [\n",
        "            \"normalized_data.csv\",\n",
        "            \"log_evolution_data.csv\",\n",
        "            \"portfolio_evolution.csv\"\n",
        "        ]\n",
        "\n",
        "        # Add monthly files using glob\n",
        "        monthly_patterns = [\n",
        "            \"*_closing.csv\",\n",
        "            \"*_obs.csv\",\n",
        "            \"*_corr.csv\"\n",
        "        ]\n",
        "\n",
        "        # Collect all files to process\n",
        "        all_files = []\n",
        "        for file in specific_files:\n",
        "            file_path = os.path.join(input_dir, file)\n",
        "            if os.path.exists(file_path):\n",
        "                all_files.append(file_path)\n",
        "\n",
        "        for pattern in monthly_patterns:\n",
        "            all_files.extend(glob(os.path.join(input_dir, pattern)))\n",
        "\n",
        "        if not all_files:\n",
        "            print(f\"Error: No matching CSV files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each file\n",
        "        for file in all_files:\n",
        "            try:\n",
        "                # Determine index column based on file type\n",
        "                filename = os.path.basename(file)\n",
        "                if filename.endswith((\"_obs.csv\", \"_corr.csv\")):\n",
        "                    # _obs.csv and _corr.csv use first column as index (tickers)\n",
        "                    df = pd.read_csv(file, index_col=0)\n",
        "                else:\n",
        "                    # Other CSVs use Date as index\n",
        "                    df = pd.read_csv(file, index_col='Date', parse_dates=True)\n",
        "\n",
        "                # Replace NaN with 0\n",
        "                df = df.fillna(0)\n",
        "\n",
        "                # Save back to the original file\n",
        "                df.to_csv(file)\n",
        "                print(f\"Processed {filename}: Replaced NaN with 0s\")\n",
        "\n",
        "                # Verify no NaNs remain\n",
        "                if df.isna().any().any():\n",
        "                    print(f\"Warning: {filename} still contains NaN values after processing.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All specified CSVs processed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    replace_nan_with_zeros()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-5Em0E-P2JH"
      },
      "source": [
        "Create obs vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvncqkIWjo87",
        "outputId": "0c15daec-7df0-4f7d-f339-c526fea87ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved ./2007_12_combined.csv with 189 rows\n",
            "Saved ./2017_05_combined.csv with 189 rows\n",
            "Saved ./2003_10_combined.csv with 189 rows\n",
            "Note: ./2003_10_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2010_10_combined.csv with 189 rows\n",
            "Saved ./2012_02_combined.csv with 189 rows\n",
            "Saved ./2017_06_combined.csv with 189 rows\n",
            "Saved ./2003_12_combined.csv with 189 rows\n",
            "Note: ./2003_12_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2009_08_combined.csv with 189 rows\n",
            "Saved ./2021_12_combined.csv with 189 rows\n",
            "Saved ./2019_01_combined.csv with 189 rows\n",
            "Saved ./2017_03_combined.csv with 189 rows\n",
            "Saved ./2016_11_combined.csv with 189 rows\n",
            "Saved ./2018_10_combined.csv with 189 rows\n",
            "Saved ./2009_09_combined.csv with 189 rows\n",
            "Saved ./2021_11_combined.csv with 189 rows\n",
            "Saved ./2018_09_combined.csv with 189 rows\n",
            "Saved ./2024_03_combined.csv with 189 rows\n",
            "Saved ./2011_01_combined.csv with 189 rows\n",
            "Saved ./2011_12_combined.csv with 189 rows\n",
            "Saved ./2007_05_combined.csv with 189 rows\n",
            "Note: ./2007_05_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2008_10_combined.csv with 189 rows\n",
            "Saved ./2020_11_combined.csv with 189 rows\n",
            "Saved ./2010_03_combined.csv with 189 rows\n",
            "Saved ./2016_02_combined.csv with 189 rows\n",
            "Saved ./2023_03_combined.csv with 189 rows\n",
            "Saved ./2015_10_combined.csv with 189 rows\n",
            "Saved ./2004_09_combined.csv with 189 rows\n",
            "Note: ./2004_09_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2019_02_combined.csv with 189 rows\n",
            "Saved ./2004_04_combined.csv with 189 rows\n",
            "Note: ./2004_04_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2017_10_combined.csv with 189 rows\n",
            "Saved ./2007_02_combined.csv with 189 rows\n",
            "Note: ./2007_02_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2023_05_combined.csv with 189 rows\n",
            "Saved ./2005_03_combined.csv with 189 rows\n",
            "Note: ./2005_03_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2015_12_combined.csv with 189 rows\n",
            "Saved ./2023_01_combined.csv with 189 rows\n",
            "Saved ./2018_12_combined.csv with 189 rows\n",
            "Saved ./2012_11_combined.csv with 189 rows\n",
            "Saved ./2007_09_combined.csv with 189 rows\n",
            "Note: ./2007_09_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2023_06_combined.csv with 189 rows\n",
            "Saved ./2024_11_combined.csv with 189 rows\n",
            "Saved ./2009_02_combined.csv with 189 rows\n",
            "Saved ./2020_04_combined.csv with 189 rows\n",
            "Saved ./2024_10_combined.csv with 189 rows\n",
            "Saved ./2016_07_combined.csv with 189 rows\n",
            "Saved ./2007_11_combined.csv with 189 rows\n",
            "Saved ./2004_01_combined.csv with 189 rows\n",
            "Note: ./2004_01_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2016_08_combined.csv with 189 rows\n",
            "Saved ./2003_08_combined.csv with 189 rows\n",
            "Note: ./2003_08_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2019_07_combined.csv with 189 rows\n",
            "Saved ./2021_07_combined.csv with 189 rows\n",
            "Saved ./2003_07_combined.csv with 189 rows\n",
            "Note: ./2003_07_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2024_08_combined.csv with 189 rows\n",
            "Saved ./2014_06_combined.csv with 189 rows\n",
            "Saved ./2004_11_combined.csv with 189 rows\n",
            "Note: ./2004_11_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2010_05_combined.csv with 189 rows\n",
            "Saved ./2021_04_combined.csv with 189 rows\n",
            "Saved ./2018_11_combined.csv with 189 rows\n",
            "Saved ./2013_12_combined.csv with 189 rows\n",
            "Saved ./2009_10_combined.csv with 189 rows\n",
            "Saved ./2012_10_combined.csv with 189 rows\n",
            "Saved ./2005_12_combined.csv with 189 rows\n",
            "Note: ./2005_12_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2013_03_combined.csv with 189 rows\n",
            "Saved ./2017_09_combined.csv with 189 rows\n",
            "Saved ./2016_03_combined.csv with 189 rows\n",
            "Saved ./2005_09_combined.csv with 189 rows\n",
            "Note: ./2005_09_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2023_11_combined.csv with 189 rows\n",
            "Saved ./2015_05_combined.csv with 189 rows\n",
            "Saved ./2024_02_combined.csv with 189 rows\n",
            "Saved ./2013_09_combined.csv with 189 rows\n",
            "Saved ./2017_11_combined.csv with 189 rows\n",
            "Saved ./2008_06_combined.csv with 189 rows\n",
            "Saved ./2014_04_combined.csv with 189 rows\n",
            "Note: ./2014_04_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2024_12_combined.csv with 189 rows\n",
            "Saved ./2006_06_combined.csv with 189 rows\n",
            "Note: ./2006_06_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2005_05_combined.csv with 189 rows\n",
            "Note: ./2005_05_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2007_06_combined.csv with 189 rows\n",
            "Note: ./2007_06_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2023_12_combined.csv with 189 rows\n",
            "Saved ./2007_03_combined.csv with 189 rows\n",
            "Note: ./2007_03_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2013_02_combined.csv with 189 rows\n",
            "Saved ./2007_07_combined.csv with 189 rows\n",
            "Note: ./2007_07_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2011_02_combined.csv with 189 rows\n",
            "Saved ./2018_05_combined.csv with 189 rows\n",
            "Saved ./2015_11_combined.csv with 189 rows\n",
            "Saved ./2006_04_combined.csv with 189 rows\n",
            "Note: ./2006_04_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2012_04_combined.csv with 189 rows\n",
            "Saved ./2012_08_combined.csv with 189 rows\n",
            "Saved ./2006_08_combined.csv with 189 rows\n",
            "Note: ./2006_08_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2015_01_combined.csv with 189 rows\n",
            "Saved ./2009_12_combined.csv with 189 rows\n",
            "Saved ./2008_04_combined.csv with 189 rows\n",
            "Saved ./2013_01_combined.csv with 189 rows\n",
            "Saved ./2023_07_combined.csv with 189 rows\n",
            "Saved ./2006_10_combined.csv with 189 rows\n",
            "Note: ./2006_10_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2019_05_combined.csv with 189 rows\n",
            "Saved ./2013_05_combined.csv with 189 rows\n",
            "Saved ./2017_01_combined.csv with 189 rows\n",
            "Saved ./2004_02_combined.csv with 189 rows\n",
            "Note: ./2004_02_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2011_05_combined.csv with 189 rows\n",
            "Saved ./2004_07_combined.csv with 189 rows\n",
            "Note: ./2004_07_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2015_03_combined.csv with 189 rows\n",
            "Saved ./2014_02_combined.csv with 189 rows\n",
            "Saved ./2019_10_combined.csv with 189 rows\n",
            "Saved ./2020_08_combined.csv with 189 rows\n",
            "Saved ./2005_01_combined.csv with 189 rows\n",
            "Note: ./2005_01_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2022_07_combined.csv with 189 rows\n",
            "Saved ./2015_02_combined.csv with 189 rows\n",
            "Saved ./2022_11_combined.csv with 189 rows\n",
            "Saved ./2023_08_combined.csv with 189 rows\n",
            "Saved ./2023_02_combined.csv with 189 rows\n",
            "Saved ./2004_03_combined.csv with 189 rows\n",
            "Note: ./2004_03_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2019_09_combined.csv with 189 rows\n",
            "Saved ./2018_07_combined.csv with 189 rows\n",
            "Saved ./2009_07_combined.csv with 189 rows\n",
            "Saved ./2011_09_combined.csv with 189 rows\n",
            "Saved ./2008_08_combined.csv with 189 rows\n",
            "Saved ./2012_12_combined.csv with 189 rows\n",
            "Saved ./2016_06_combined.csv with 189 rows\n",
            "Saved ./2021_06_combined.csv with 189 rows\n",
            "Saved ./2017_02_combined.csv with 189 rows\n",
            "Saved ./2005_06_combined.csv with 189 rows\n",
            "Note: ./2005_06_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2006_02_combined.csv with 189 rows\n",
            "Note: ./2006_02_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2009_04_combined.csv with 189 rows\n",
            "Saved ./2021_08_combined.csv with 189 rows\n",
            "Saved ./2008_03_combined.csv with 189 rows\n",
            "Saved ./2019_12_combined.csv with 189 rows\n",
            "Saved ./2010_07_combined.csv with 189 rows\n",
            "Saved ./2022_01_combined.csv with 189 rows\n",
            "Saved ./2014_09_combined.csv with 189 rows\n",
            "Saved ./2004_06_combined.csv with 189 rows\n",
            "Note: ./2004_06_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2010_08_combined.csv with 189 rows\n",
            "Saved ./2024_06_combined.csv with 189 rows\n",
            "Saved ./2007_04_combined.csv with 189 rows\n",
            "Note: ./2007_04_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2024_01_combined.csv with 189 rows\n",
            "Saved ./2012_05_combined.csv with 189 rows\n",
            "Saved ./2011_10_combined.csv with 189 rows\n",
            "Saved ./2018_04_combined.csv with 189 rows\n",
            "Saved ./2015_06_combined.csv with 189 rows\n",
            "Saved ./2019_04_combined.csv with 189 rows\n",
            "Saved ./2010_04_combined.csv with 189 rows\n",
            "Saved ./2003_04_combined.csv with 189 rows\n",
            "Note: ./2003_04_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2023_10_combined.csv with 189 rows\n",
            "Saved ./2022_12_combined.csv with 189 rows\n",
            "Saved ./2008_02_combined.csv with 189 rows\n",
            "Saved ./2006_07_combined.csv with 189 rows\n",
            "Note: ./2006_07_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2013_11_combined.csv with 189 rows\n",
            "Saved ./2021_03_combined.csv with 189 rows\n",
            "Saved ./2004_12_combined.csv with 189 rows\n",
            "Note: ./2004_12_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2017_12_combined.csv with 189 rows\n",
            "Saved ./2009_01_combined.csv with 189 rows\n",
            "Saved ./2014_01_combined.csv with 189 rows\n",
            "Saved ./2011_07_combined.csv with 189 rows\n",
            "Saved ./2012_09_combined.csv with 189 rows\n",
            "Saved ./2019_08_combined.csv with 189 rows\n",
            "Saved ./2023_09_combined.csv with 189 rows\n",
            "Saved ./2022_03_combined.csv with 189 rows\n",
            "Saved ./2004_10_combined.csv with 189 rows\n",
            "Note: ./2004_10_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2003_09_combined.csv with 189 rows\n",
            "Note: ./2003_09_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2020_10_combined.csv with 189 rows\n",
            "Saved ./2022_02_combined.csv with 189 rows\n",
            "Saved ./2022_04_combined.csv with 189 rows\n",
            "Saved ./2015_07_combined.csv with 189 rows\n",
            "Saved ./2019_03_combined.csv with 189 rows\n",
            "Saved ./2016_09_combined.csv with 189 rows\n",
            "Saved ./2003_05_combined.csv with 189 rows\n",
            "Note: ./2003_05_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2020_01_combined.csv with 189 rows\n",
            "Saved ./2007_10_combined.csv with 189 rows\n",
            "Saved ./2012_06_combined.csv with 189 rows\n",
            "Saved ./2015_09_combined.csv with 189 rows\n",
            "Saved ./2006_05_combined.csv with 189 rows\n",
            "Note: ./2006_05_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2021_01_combined.csv with 189 rows\n",
            "Saved ./2024_04_combined.csv with 189 rows\n",
            "Saved ./2017_04_combined.csv with 189 rows\n",
            "Saved ./2018_08_combined.csv with 189 rows\n",
            "Saved ./2020_06_combined.csv with 189 rows\n",
            "Saved ./2024_07_combined.csv with 189 rows\n",
            "Saved ./2012_07_combined.csv with 189 rows\n",
            "Saved ./2009_11_combined.csv with 189 rows\n",
            "Saved ./2022_06_combined.csv with 189 rows\n",
            "Saved ./2014_05_combined.csv with 189 rows\n",
            "Saved ./2024_09_combined.csv with 189 rows\n",
            "Saved ./2006_03_combined.csv with 189 rows\n",
            "Note: ./2006_03_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2007_08_combined.csv with 189 rows\n",
            "Note: ./2007_08_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2016_12_combined.csv with 189 rows\n",
            "Saved ./2003_03_combined.csv with 189 rows\n",
            "Note: ./2003_03_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2020_05_combined.csv with 189 rows\n",
            "Saved ./2013_06_combined.csv with 189 rows\n",
            "Saved ./2010_01_combined.csv with 189 rows\n",
            "Saved ./2021_10_combined.csv with 189 rows\n",
            "Saved ./2017_07_combined.csv with 189 rows\n",
            "Saved ./2010_12_combined.csv with 189 rows\n",
            "Saved ./2013_08_combined.csv with 189 rows\n",
            "Saved ./2009_05_combined.csv with 189 rows\n",
            "Saved ./2018_01_combined.csv with 189 rows\n",
            "Saved ./2005_04_combined.csv with 189 rows\n",
            "Note: ./2005_04_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2008_12_combined.csv with 189 rows\n",
            "Saved ./2023_04_combined.csv with 189 rows\n",
            "Saved ./2003_02_combined.csv with 189 rows\n",
            "Note: ./2003_02_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2008_05_combined.csv with 189 rows\n",
            "Saved ./2010_06_combined.csv with 189 rows\n",
            "Saved ./2014_08_combined.csv with 189 rows\n",
            "Saved ./2020_02_combined.csv with 189 rows\n",
            "Saved ./2016_05_combined.csv with 189 rows\n",
            "Saved ./2006_11_combined.csv with 189 rows\n",
            "Note: ./2006_11_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2018_03_combined.csv with 189 rows\n",
            "Saved ./2013_10_combined.csv with 189 rows\n",
            "Saved ./2008_11_combined.csv with 189 rows\n",
            "Saved ./2011_03_combined.csv with 189 rows\n",
            "Saved ./2021_02_combined.csv with 189 rows\n",
            "Saved ./2013_04_combined.csv with 189 rows\n",
            "Saved ./2007_01_combined.csv with 189 rows\n",
            "Note: ./2007_01_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2003_11_combined.csv with 189 rows\n",
            "Note: ./2003_11_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2008_01_combined.csv with 189 rows\n",
            "Saved ./2019_11_combined.csv with 189 rows\n",
            "Saved ./2003_06_combined.csv with 189 rows\n",
            "Note: ./2003_06_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2004_08_combined.csv with 189 rows\n",
            "Note: ./2004_08_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2015_04_combined.csv with 189 rows\n",
            "Saved ./2020_07_combined.csv with 189 rows\n",
            "Saved ./2008_07_combined.csv with 189 rows\n",
            "Saved ./2011_04_combined.csv with 189 rows\n",
            "Saved ./2014_07_combined.csv with 189 rows\n",
            "Saved ./2012_01_combined.csv with 189 rows\n",
            "Saved ./2005_08_combined.csv with 189 rows\n",
            "Note: ./2005_08_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2019_06_combined.csv with 189 rows\n",
            "Saved ./2003_01_combined.csv with 189 rows\n",
            "Note: ./2003_01_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2006_09_combined.csv with 189 rows\n",
            "Note: ./2006_09_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2008_09_combined.csv with 189 rows\n",
            "Saved ./2018_02_combined.csv with 189 rows\n",
            "Saved ./2020_03_combined.csv with 189 rows\n",
            "Saved ./2022_08_combined.csv with 189 rows\n",
            "Saved ./2005_02_combined.csv with 189 rows\n",
            "Note: ./2005_02_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2004_05_combined.csv with 189 rows\n",
            "Note: ./2004_05_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2022_10_combined.csv with 189 rows\n",
            "Saved ./2010_02_combined.csv with 189 rows\n",
            "Saved ./2006_01_combined.csv with 189 rows\n",
            "Note: ./2006_01_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2011_11_combined.csv with 189 rows\n",
            "Saved ./2011_08_combined.csv with 189 rows\n",
            "Saved ./2014_11_combined.csv with 189 rows\n",
            "Saved ./2020_09_combined.csv with 189 rows\n",
            "Saved ./2011_06_combined.csv with 189 rows\n",
            "Saved ./2012_03_combined.csv with 189 rows\n",
            "Saved ./2005_07_combined.csv with 189 rows\n",
            "Note: ./2005_07_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2014_10_combined.csv with 189 rows\n",
            "Saved ./2016_10_combined.csv with 189 rows\n",
            "Saved ./2010_11_combined.csv with 189 rows\n",
            "Saved ./2016_01_combined.csv with 189 rows\n",
            "Saved ./2015_08_combined.csv with 189 rows\n",
            "Saved ./2021_05_combined.csv with 189 rows\n",
            "Saved ./2014_12_combined.csv with 189 rows\n",
            "Saved ./2024_05_combined.csv with 189 rows\n",
            "Saved ./2005_11_combined.csv with 189 rows\n",
            "Note: ./2005_11_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2022_05_combined.csv with 189 rows\n",
            "Saved ./2009_03_combined.csv with 189 rows\n",
            "Saved ./2016_04_combined.csv with 189 rows\n",
            "Saved ./2018_06_combined.csv with 189 rows\n",
            "Saved ./2013_07_combined.csv with 189 rows\n",
            "Saved ./2020_12_combined.csv with 189 rows\n",
            "Saved ./2005_10_combined.csv with 189 rows\n",
            "Note: ./2005_10_combined.csv contains zero values (from prior NaN replacement).\n",
            "Saved ./2022_09_combined.csv with 189 rows\n",
            "Saved ./2017_08_combined.csv with 189 rows\n",
            "Saved ./2010_09_combined.csv with 189 rows\n",
            "Saved ./2009_06_combined.csv with 189 rows\n",
            "Saved ./2014_03_combined.csv with 189 rows\n",
            "Saved ./2021_09_combined.csv with 189 rows\n",
            "Saved ./2006_12_combined.csv with 189 rows\n",
            "Note: ./2006_12_combined.csv contains zero values (from prior NaN replacement).\n",
            "All combined CSVs generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def combine_monthly_metrics(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Combine monthly observation and correlation data into a single-column CSV.\n",
        "\n",
        "    For each {yyyy}_{mm}_obs.csv and {yyyy}_{mm}_corr.csv, creates:\n",
        "    - {yyyy}_{mm}_combined.csv: Single column with metrics and flattened correlations\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing input CSVs (default: current directory)\n",
        "    output_dir (str): Directory to save output CSVs (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves combined CSVs to output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find all observation CSVs\n",
        "        obs_files = glob(f\"{input_dir}/*_obs.csv\")\n",
        "        if not obs_files:\n",
        "            print(f\"Error: No *_obs.csv files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        for obs_file in obs_files:\n",
        "            # Extract year and month from filename (e.g., 2003_01_obs.csv)\n",
        "            filename = os.path.basename(obs_file)\n",
        "            year_month = filename.replace(\"_obs.csv\", \"\")\n",
        "\n",
        "            # Corresponding correlation file\n",
        "            corr_file = f\"{input_dir}/{year_month}_corr.csv\"\n",
        "\n",
        "            # Check if correlation file exists\n",
        "            if not os.path.exists(corr_file):\n",
        "                print(f\"Warning: {corr_file} not found. Skipping {year_month}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read observation CSV (tickers as index, metrics as columns)\n",
        "                obs_df = pd.read_csv(obs_file, index_col=0)\n",
        "\n",
        "                # Read correlation CSV (tickers as both index and columns)\n",
        "                corr_df = pd.read_csv(corr_file, index_col=0)\n",
        "\n",
        "                # Validate DataFrames\n",
        "                if obs_df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping {year_month}.\")\n",
        "                    continue\n",
        "                if corr_df.empty:\n",
        "                    print(f\"Warning: {corr_file} is empty. Skipping {year_month}.\")\n",
        "                    continue\n",
        "\n",
        "                # Initialize list for combined data\n",
        "                combined_data = []\n",
        "\n",
        "                # Process observation metrics\n",
        "                for ticker in obs_df.index:\n",
        "                    for metric in obs_df.columns:\n",
        "                        value = obs_df.at[ticker, metric]\n",
        "                        row_label = f\"{ticker}_{metric}\"\n",
        "                        combined_data.append((row_label, value))\n",
        "\n",
        "                # Flatten correlation matrix (upper triangle, exclude diagonal)\n",
        "                tickers = corr_df.index\n",
        "                for i in range(len(tickers)):\n",
        "                    for j in range(i + 1, len(tickers)):  # Upper triangle\n",
        "                        ticker1, ticker2 = tickers[i], tickers[j]\n",
        "                        value = corr_df.at[ticker1, ticker2]\n",
        "                        row_label = f\"{ticker1}_{ticker2}_Correlation\"\n",
        "                        combined_data.append((row_label, value))\n",
        "\n",
        "                # Create single-column DataFrame\n",
        "                combined_df = pd.DataFrame(\n",
        "                    [x[1] for x in combined_data],\n",
        "                    index=[x[0] for x in combined_data],\n",
        "                    columns=['Value']\n",
        "                )\n",
        "\n",
        "                # Save to combined CSV\n",
        "                output_file = f\"{output_dir}/{year_month}_combined.csv\"\n",
        "                combined_df.to_csv(output_file)\n",
        "                print(f\"Saved {output_file} with {len(combined_df)} rows\")\n",
        "\n",
        "                # Warn about any unexpected values (e.g., NaNs, though replaced with 0s)\n",
        "                if combined_df['Value'].eq(0).any():\n",
        "                    print(f\"Note: {output_file} contains zero values (from prior NaN replacement).\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {year_month}: {e}\")\n",
        "\n",
        "        print(\"All combined CSVs generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    combine_monthly_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beF8G9atkajK",
        "outputId": "6dfbcf95-e483-475e-a3d2-2eaafe43f968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created/Verified folder: ./combined\n",
            "Created/Verified folder: ./price\n",
            "Created/Verified folder: ./usage\n",
            "Moved 2003_07_combined.csv to ./combined\n",
            "Moved 2006_08_combined.csv to ./combined\n",
            "Moved 2007_07_combined.csv to ./combined\n",
            "Moved 2003_03_combined.csv to ./combined\n",
            "Moved 2024_08_combined.csv to ./combined\n",
            "Moved 2014_07_combined.csv to ./combined\n",
            "Moved 2022_12_combined.csv to ./combined\n",
            "Moved 2008_09_combined.csv to ./combined\n",
            "Moved 2017_03_combined.csv to ./combined\n",
            "Moved 2015_10_combined.csv to ./combined\n",
            "Moved 2016_08_combined.csv to ./combined\n",
            "Moved 2024_05_combined.csv to ./combined\n",
            "Moved 2003_09_combined.csv to ./combined\n",
            "Moved 2011_03_combined.csv to ./combined\n",
            "Moved 2007_01_combined.csv to ./combined\n",
            "Moved 2022_04_combined.csv to ./combined\n",
            "Moved 2023_07_combined.csv to ./combined\n",
            "Moved 2003_08_combined.csv to ./combined\n",
            "Moved 2020_07_combined.csv to ./combined\n",
            "Moved 2015_09_combined.csv to ./combined\n",
            "Moved 2007_09_combined.csv to ./combined\n",
            "Moved 2007_08_combined.csv to ./combined\n",
            "Moved 2012_10_combined.csv to ./combined\n",
            "Moved 2015_07_combined.csv to ./combined\n",
            "Moved 2006_03_combined.csv to ./combined\n",
            "Moved 2014_10_combined.csv to ./combined\n",
            "Moved 2015_03_combined.csv to ./combined\n",
            "Moved 2023_05_combined.csv to ./combined\n",
            "Moved 2017_12_combined.csv to ./combined\n",
            "Moved 2020_09_combined.csv to ./combined\n",
            "Moved 2019_04_combined.csv to ./combined\n",
            "Moved 2023_09_combined.csv to ./combined\n",
            "Moved 2019_05_combined.csv to ./combined\n",
            "Moved 2021_02_combined.csv to ./combined\n",
            "Moved 2019_12_combined.csv to ./combined\n",
            "Moved 2017_11_combined.csv to ./combined\n",
            "Moved 2013_11_combined.csv to ./combined\n",
            "Moved 2005_06_combined.csv to ./combined\n",
            "Moved 2010_11_combined.csv to ./combined\n",
            "Moved 2024_06_combined.csv to ./combined\n",
            "Moved 2007_03_combined.csv to ./combined\n",
            "Moved 2023_10_combined.csv to ./combined\n",
            "Moved 2004_07_combined.csv to ./combined\n",
            "Moved 2012_09_combined.csv to ./combined\n",
            "Moved 2011_12_combined.csv to ./combined\n",
            "Moved 2014_05_combined.csv to ./combined\n",
            "Moved 2016_10_combined.csv to ./combined\n",
            "Moved 2024_11_combined.csv to ./combined\n",
            "Moved 2012_07_combined.csv to ./combined\n",
            "Moved 2011_11_combined.csv to ./combined\n",
            "Moved 2012_05_combined.csv to ./combined\n",
            "Moved 2006_01_combined.csv to ./combined\n",
            "Moved 2014_09_combined.csv to ./combined\n",
            "Moved 2010_03_combined.csv to ./combined\n",
            "Moved 2004_03_combined.csv to ./combined\n",
            "Moved 2005_03_combined.csv to ./combined\n",
            "Moved 2008_08_combined.csv to ./combined\n",
            "Moved 2008_10_combined.csv to ./combined\n",
            "Moved 2017_10_combined.csv to ./combined\n",
            "Moved 2005_05_combined.csv to ./combined\n",
            "Moved 2021_06_combined.csv to ./combined\n",
            "Moved 2021_04_combined.csv to ./combined\n",
            "Moved 2012_02_combined.csv to ./combined\n",
            "Moved 2010_05_combined.csv to ./combined\n",
            "Moved 2015_12_combined.csv to ./combined\n",
            "Moved 2004_02_combined.csv to ./combined\n",
            "Moved 2004_04_combined.csv to ./combined\n",
            "Moved 2013_02_combined.csv to ./combined\n",
            "Moved 2018_03_combined.csv to ./combined\n",
            "Moved 2009_11_combined.csv to ./combined\n",
            "Moved 2008_03_combined.csv to ./combined\n",
            "Moved 2017_04_combined.csv to ./combined\n",
            "Moved 2016_02_combined.csv to ./combined\n",
            "Moved 2021_09_combined.csv to ./combined\n",
            "Moved 2023_03_combined.csv to ./combined\n",
            "Moved 2020_03_combined.csv to ./combined\n",
            "Moved 2011_02_combined.csv to ./combined\n",
            "Moved 2005_08_combined.csv to ./combined\n",
            "Moved 2004_06_combined.csv to ./combined\n",
            "Moved 2015_08_combined.csv to ./combined\n",
            "Moved 2019_09_combined.csv to ./combined\n",
            "Moved 2010_02_combined.csv to ./combined\n",
            "Moved 2024_04_combined.csv to ./combined\n",
            "Moved 2018_07_combined.csv to ./combined\n",
            "Moved 2021_07_combined.csv to ./combined\n",
            "Moved 2010_01_combined.csv to ./combined\n",
            "Moved 2017_09_combined.csv to ./combined\n",
            "Moved 2010_06_combined.csv to ./combined\n",
            "Moved 2023_04_combined.csv to ./combined\n",
            "Moved 2009_07_combined.csv to ./combined\n",
            "Moved 2003_05_combined.csv to ./combined\n",
            "Moved 2017_08_combined.csv to ./combined\n",
            "Moved 2020_02_combined.csv to ./combined\n",
            "Moved 2014_12_combined.csv to ./combined\n",
            "Moved 2005_12_combined.csv to ./combined\n",
            "Moved 2024_01_combined.csv to ./combined\n",
            "Moved 2014_08_combined.csv to ./combined\n",
            "Moved 2013_05_combined.csv to ./combined\n",
            "Moved 2019_08_combined.csv to ./combined\n",
            "Moved 2005_07_combined.csv to ./combined\n",
            "Moved 2020_04_combined.csv to ./combined\n",
            "Moved 2017_06_combined.csv to ./combined\n",
            "Moved 2018_02_combined.csv to ./combined\n",
            "Moved 2005_09_combined.csv to ./combined\n",
            "Moved 2008_05_combined.csv to ./combined\n",
            "Moved 2006_06_combined.csv to ./combined\n",
            "Moved 2004_08_combined.csv to ./combined\n",
            "Moved 2008_04_combined.csv to ./combined\n",
            "Moved 2003_10_combined.csv to ./combined\n",
            "Moved 2014_04_combined.csv to ./combined\n",
            "Moved 2013_12_combined.csv to ./combined\n",
            "Moved 2020_08_combined.csv to ./combined\n",
            "Moved 2006_05_combined.csv to ./combined\n",
            "Moved 2004_11_combined.csv to ./combined\n",
            "Moved 2005_01_combined.csv to ./combined\n",
            "Moved 2009_08_combined.csv to ./combined\n",
            "Moved 2010_10_combined.csv to ./combined\n",
            "Moved 2005_11_combined.csv to ./combined\n",
            "Moved 2017_01_combined.csv to ./combined\n",
            "Moved 2009_05_combined.csv to ./combined\n",
            "Moved 2023_02_combined.csv to ./combined\n",
            "Moved 2022_06_combined.csv to ./combined\n",
            "Moved 2015_01_combined.csv to ./combined\n",
            "Moved 2016_09_combined.csv to ./combined\n",
            "Moved 2020_11_combined.csv to ./combined\n",
            "Moved 2014_02_combined.csv to ./combined\n",
            "Moved 2020_12_combined.csv to ./combined\n",
            "Moved 2009_03_combined.csv to ./combined\n",
            "Moved 2014_06_combined.csv to ./combined\n",
            "Moved 2011_04_combined.csv to ./combined\n",
            "Moved 2022_05_combined.csv to ./combined\n",
            "Moved 2007_02_combined.csv to ./combined\n",
            "Moved 2021_12_combined.csv to ./combined\n",
            "Moved 2020_01_combined.csv to ./combined\n",
            "Moved 2013_06_combined.csv to ./combined\n",
            "Moved 2014_01_combined.csv to ./combined\n",
            "Moved 2004_10_combined.csv to ./combined\n",
            "Moved 2023_12_combined.csv to ./combined\n",
            "Moved 2009_12_combined.csv to ./combined\n",
            "Moved 2014_03_combined.csv to ./combined\n",
            "Moved 2011_09_combined.csv to ./combined\n",
            "Moved 2017_02_combined.csv to ./combined\n",
            "Moved 2010_04_combined.csv to ./combined\n",
            "Moved 2020_10_combined.csv to ./combined\n",
            "Moved 2021_05_combined.csv to ./combined\n",
            "Moved 2015_05_combined.csv to ./combined\n",
            "Moved 2004_01_combined.csv to ./combined\n",
            "Moved 2007_06_combined.csv to ./combined\n",
            "Moved 2022_11_combined.csv to ./combined\n",
            "Moved 2008_11_combined.csv to ./combined\n",
            "Moved 2019_10_combined.csv to ./combined\n",
            "Moved 2016_04_combined.csv to ./combined\n",
            "Moved 2012_04_combined.csv to ./combined\n",
            "Moved 2024_07_combined.csv to ./combined\n",
            "Moved 2011_06_combined.csv to ./combined\n",
            "Moved 2022_09_combined.csv to ./combined\n",
            "Moved 2003_01_combined.csv to ./combined\n",
            "Moved 2006_11_combined.csv to ./combined\n",
            "Moved 2007_05_combined.csv to ./combined\n",
            "Moved 2024_12_combined.csv to ./combined\n",
            "Moved 2009_06_combined.csv to ./combined\n",
            "Moved 2004_05_combined.csv to ./combined\n",
            "Moved 2012_08_combined.csv to ./combined\n",
            "Moved 2018_10_combined.csv to ./combined\n",
            "Moved 2020_05_combined.csv to ./combined\n",
            "Moved 2024_03_combined.csv to ./combined\n",
            "Moved 2017_05_combined.csv to ./combined\n",
            "Moved 2011_07_combined.csv to ./combined\n",
            "Moved 2019_02_combined.csv to ./combined\n",
            "Moved 2022_03_combined.csv to ./combined\n",
            "Moved 2009_02_combined.csv to ./combined\n",
            "Moved 2016_06_combined.csv to ./combined\n",
            "Moved 2007_11_combined.csv to ./combined\n",
            "Moved 2017_07_combined.csv to ./combined\n",
            "Moved 2004_09_combined.csv to ./combined\n",
            "Moved 2020_06_combined.csv to ./combined\n",
            "Moved 2016_01_combined.csv to ./combined\n",
            "Moved 2003_04_combined.csv to ./combined\n",
            "Moved 2008_06_combined.csv to ./combined\n",
            "Moved 2009_10_combined.csv to ./combined\n",
            "Moved 2016_11_combined.csv to ./combined\n",
            "Moved 2013_07_combined.csv to ./combined\n",
            "Moved 2015_11_combined.csv to ./combined\n",
            "Moved 2003_11_combined.csv to ./combined\n",
            "Moved 2013_03_combined.csv to ./combined\n",
            "Moved 2013_01_combined.csv to ./combined\n",
            "Moved 2022_08_combined.csv to ./combined\n",
            "Moved 2015_02_combined.csv to ./combined\n",
            "Moved 2021_10_combined.csv to ./combined\n",
            "Moved 2022_02_combined.csv to ./combined\n",
            "Moved 2018_08_combined.csv to ./combined\n",
            "Moved 2008_07_combined.csv to ./combined\n",
            "Moved 2015_04_combined.csv to ./combined\n",
            "Moved 2004_12_combined.csv to ./combined\n",
            "Moved 2022_01_combined.csv to ./combined\n",
            "Moved 2013_04_combined.csv to ./combined\n",
            "Moved 2012_01_combined.csv to ./combined\n",
            "Moved 2024_10_combined.csv to ./combined\n",
            "Moved 2021_08_combined.csv to ./combined\n",
            "Moved 2018_06_combined.csv to ./combined\n",
            "Moved 2010_12_combined.csv to ./combined\n",
            "Moved 2023_06_combined.csv to ./combined\n",
            "Moved 2011_01_combined.csv to ./combined\n",
            "Moved 2010_07_combined.csv to ./combined\n",
            "Moved 2023_01_combined.csv to ./combined\n",
            "Moved 2013_09_combined.csv to ./combined\n",
            "Moved 2003_12_combined.csv to ./combined\n",
            "Moved 2013_10_combined.csv to ./combined\n",
            "Moved 2018_04_combined.csv to ./combined\n",
            "Moved 2023_11_combined.csv to ./combined\n",
            "Moved 2003_02_combined.csv to ./combined\n",
            "Moved 2018_12_combined.csv to ./combined\n",
            "Moved 2016_03_combined.csv to ./combined\n",
            "Moved 2024_09_combined.csv to ./combined\n",
            "Moved 2011_05_combined.csv to ./combined\n",
            "Moved 2019_06_combined.csv to ./combined\n",
            "Moved 2021_01_combined.csv to ./combined\n",
            "Moved 2003_06_combined.csv to ./combined\n",
            "Moved 2009_04_combined.csv to ./combined\n",
            "Moved 2006_02_combined.csv to ./combined\n",
            "Moved 2016_07_combined.csv to ./combined\n",
            "Moved 2005_10_combined.csv to ./combined\n",
            "Moved 2016_12_combined.csv to ./combined\n",
            "Moved 2018_01_combined.csv to ./combined\n",
            "Moved 2007_04_combined.csv to ./combined\n",
            "Moved 2022_10_combined.csv to ./combined\n",
            "Moved 2008_12_combined.csv to ./combined\n",
            "Moved 2012_11_combined.csv to ./combined\n",
            "Moved 2016_05_combined.csv to ./combined\n",
            "Moved 2008_01_combined.csv to ./combined\n",
            "Moved 2005_02_combined.csv to ./combined\n",
            "Moved 2023_08_combined.csv to ./combined\n",
            "Moved 2010_09_combined.csv to ./combined\n",
            "Moved 2024_02_combined.csv to ./combined\n",
            "Moved 2018_09_combined.csv to ./combined\n",
            "Moved 2006_04_combined.csv to ./combined\n",
            "Moved 2019_01_combined.csv to ./combined\n",
            "Moved 2015_06_combined.csv to ./combined\n",
            "Moved 2012_06_combined.csv to ./combined\n",
            "Moved 2005_04_combined.csv to ./combined\n",
            "Moved 2021_11_combined.csv to ./combined\n",
            "Moved 2010_08_combined.csv to ./combined\n",
            "Moved 2018_05_combined.csv to ./combined\n",
            "Moved 2014_11_combined.csv to ./combined\n",
            "Moved 2019_07_combined.csv to ./combined\n",
            "Moved 2018_11_combined.csv to ./combined\n",
            "Moved 2019_03_combined.csv to ./combined\n",
            "Moved 2021_03_combined.csv to ./combined\n",
            "Moved 2011_10_combined.csv to ./combined\n",
            "Moved 2013_08_combined.csv to ./combined\n",
            "Moved 2006_10_combined.csv to ./combined\n",
            "Moved 2009_01_combined.csv to ./combined\n",
            "Moved 2011_08_combined.csv to ./combined\n",
            "Moved 2006_09_combined.csv to ./combined\n",
            "Moved 2006_12_combined.csv to ./combined\n",
            "Moved 2006_07_combined.csv to ./combined\n",
            "Moved 2007_10_combined.csv to ./combined\n",
            "Moved 2022_07_combined.csv to ./combined\n",
            "Moved 2019_11_combined.csv to ./combined\n",
            "Moved 2012_12_combined.csv to ./combined\n",
            "Moved 2007_12_combined.csv to ./combined\n",
            "Moved 2012_03_combined.csv to ./combined\n",
            "Moved 2009_09_combined.csv to ./combined\n",
            "Moved 2008_02_combined.csv to ./combined\n",
            "Moved clean_data.csv to ./price\n",
            "Moved normalized_data.csv to ./price\n",
            "Moved log_evolution_data.csv to ./price\n",
            "Moved portfolio_evolution.csv to ./price\n",
            "Moved stock_closing_prices.csv to ./price\n",
            "Moved 2017_09_closing.csv to ./usage\n",
            "Moved 2005_05_closing.csv to ./usage\n",
            "Moved 2011_07_closing.csv to ./usage\n",
            "Moved 2011_03_closing.csv to ./usage\n",
            "Moved 2004_08_closing.csv to ./usage\n",
            "Moved 2016_09_closing.csv to ./usage\n",
            "Moved 2006_08_closing.csv to ./usage\n",
            "Moved 2016_02_closing.csv to ./usage\n",
            "Moved 2022_03_closing.csv to ./usage\n",
            "Moved 2010_02_closing.csv to ./usage\n",
            "Moved 2018_10_closing.csv to ./usage\n",
            "Moved 2004_09_closing.csv to ./usage\n",
            "Moved 2009_01_closing.csv to ./usage\n",
            "Moved 2013_05_closing.csv to ./usage\n",
            "Moved 2024_12_closing.csv to ./usage\n",
            "Moved 2015_12_closing.csv to ./usage\n",
            "Moved 2024_08_closing.csv to ./usage\n",
            "Moved 2003_03_closing.csv to ./usage\n",
            "Moved 2021_05_closing.csv to ./usage\n",
            "Moved 2022_06_closing.csv to ./usage\n",
            "Moved 2008_10_closing.csv to ./usage\n",
            "Moved 2011_12_closing.csv to ./usage\n",
            "Moved 2022_07_closing.csv to ./usage\n",
            "Moved 2021_10_closing.csv to ./usage\n",
            "Moved 2023_06_closing.csv to ./usage\n",
            "Moved 2012_02_closing.csv to ./usage\n",
            "Moved 2021_03_closing.csv to ./usage\n",
            "Moved 2018_01_closing.csv to ./usage\n",
            "Moved 2011_01_closing.csv to ./usage\n",
            "Moved 2014_07_closing.csv to ./usage\n",
            "Moved 2016_05_closing.csv to ./usage\n",
            "Moved 2022_04_closing.csv to ./usage\n",
            "Moved 2007_09_closing.csv to ./usage\n",
            "Moved 2006_09_closing.csv to ./usage\n",
            "Moved 2021_07_closing.csv to ./usage\n",
            "Moved 2012_07_closing.csv to ./usage\n",
            "Moved 2010_01_closing.csv to ./usage\n",
            "Moved 2020_02_closing.csv to ./usage\n",
            "Moved 2015_09_closing.csv to ./usage\n",
            "Moved 2016_08_closing.csv to ./usage\n",
            "Moved 2011_06_closing.csv to ./usage\n",
            "Moved 2023_08_closing.csv to ./usage\n",
            "Moved 2003_12_closing.csv to ./usage\n",
            "Moved 2023_09_closing.csv to ./usage\n",
            "Moved 2020_09_closing.csv to ./usage\n",
            "Moved 2007_04_closing.csv to ./usage\n",
            "Moved 2006_01_closing.csv to ./usage\n",
            "Moved 2014_04_closing.csv to ./usage\n",
            "Moved 2005_06_closing.csv to ./usage\n",
            "Moved 2005_08_closing.csv to ./usage\n",
            "Moved 2020_03_closing.csv to ./usage\n",
            "Moved 2004_12_closing.csv to ./usage\n",
            "Moved 2020_11_closing.csv to ./usage\n",
            "Moved 2017_04_closing.csv to ./usage\n",
            "Moved 2013_07_closing.csv to ./usage\n",
            "Moved 2010_05_closing.csv to ./usage\n",
            "Moved 2021_12_closing.csv to ./usage\n",
            "Moved 2013_04_closing.csv to ./usage\n",
            "Moved 2013_10_closing.csv to ./usage\n",
            "Moved 2006_04_closing.csv to ./usage\n",
            "Moved 2012_05_closing.csv to ./usage\n",
            "Moved 2023_01_closing.csv to ./usage\n",
            "Moved 2006_06_closing.csv to ./usage\n",
            "Moved 2009_07_closing.csv to ./usage\n",
            "Moved 2007_06_closing.csv to ./usage\n",
            "Moved 2018_04_closing.csv to ./usage\n",
            "Moved 2006_03_closing.csv to ./usage\n",
            "Moved 2014_10_closing.csv to ./usage\n",
            "Moved 2013_06_closing.csv to ./usage\n",
            "Moved 2022_01_closing.csv to ./usage\n",
            "Moved 2023_10_closing.csv to ./usage\n",
            "Moved 2015_04_closing.csv to ./usage\n",
            "Moved 2003_08_closing.csv to ./usage\n",
            "Moved 2012_09_closing.csv to ./usage\n",
            "Moved 2022_11_closing.csv to ./usage\n",
            "Moved 2008_02_closing.csv to ./usage\n",
            "Moved 2016_06_closing.csv to ./usage\n",
            "Moved 2007_01_closing.csv to ./usage\n",
            "Moved 2005_04_closing.csv to ./usage\n",
            "Moved 2005_07_closing.csv to ./usage\n",
            "Moved 2003_06_closing.csv to ./usage\n",
            "Moved 2007_05_closing.csv to ./usage\n",
            "Moved 2015_10_closing.csv to ./usage\n",
            "Moved 2003_01_closing.csv to ./usage\n",
            "Moved 2019_10_closing.csv to ./usage\n",
            "Moved 2011_10_closing.csv to ./usage\n",
            "Moved 2018_08_closing.csv to ./usage\n",
            "Moved 2018_11_closing.csv to ./usage\n",
            "Moved 2011_09_closing.csv to ./usage\n",
            "Moved 2006_11_closing.csv to ./usage\n",
            "Moved 2012_11_closing.csv to ./usage\n",
            "Moved 2007_03_closing.csv to ./usage\n",
            "Moved 2020_06_closing.csv to ./usage\n",
            "Moved 2014_08_closing.csv to ./usage\n",
            "Moved 2021_04_closing.csv to ./usage\n",
            "Moved 2009_02_closing.csv to ./usage\n",
            "Moved 2005_01_closing.csv to ./usage\n",
            "Moved 2010_03_closing.csv to ./usage\n",
            "Moved 2007_02_closing.csv to ./usage\n",
            "Moved 2019_12_closing.csv to ./usage\n",
            "Moved 2016_01_closing.csv to ./usage\n",
            "Moved 2014_03_closing.csv to ./usage\n",
            "Moved 2012_12_closing.csv to ./usage\n",
            "Moved 2004_10_closing.csv to ./usage\n",
            "Moved 2023_04_closing.csv to ./usage\n",
            "Moved 2014_02_closing.csv to ./usage\n",
            "Moved 2010_09_closing.csv to ./usage\n",
            "Moved 2017_11_closing.csv to ./usage\n",
            "Moved 2011_02_closing.csv to ./usage\n",
            "Moved 2018_12_closing.csv to ./usage\n",
            "Moved 2013_02_closing.csv to ./usage\n",
            "Moved 2004_01_closing.csv to ./usage\n",
            "Moved 2021_06_closing.csv to ./usage\n",
            "Moved 2022_10_closing.csv to ./usage\n",
            "Moved 2007_11_closing.csv to ./usage\n",
            "Moved 2019_04_closing.csv to ./usage\n",
            "Moved 2013_11_closing.csv to ./usage\n",
            "Moved 2011_11_closing.csv to ./usage\n",
            "Moved 2005_02_closing.csv to ./usage\n",
            "Moved 2004_04_closing.csv to ./usage\n",
            "Moved 2017_01_closing.csv to ./usage\n",
            "Moved 2019_01_closing.csv to ./usage\n",
            "Moved 2017_10_closing.csv to ./usage\n",
            "Moved 2016_10_closing.csv to ./usage\n",
            "Moved 2004_02_closing.csv to ./usage\n",
            "Moved 2012_06_closing.csv to ./usage\n",
            "Moved 2022_09_closing.csv to ./usage\n",
            "Moved 2018_06_closing.csv to ./usage\n",
            "Moved 2022_02_closing.csv to ./usage\n",
            "Moved 2016_03_closing.csv to ./usage\n",
            "Moved 2016_04_closing.csv to ./usage\n",
            "Moved 2016_07_closing.csv to ./usage\n",
            "Moved 2012_04_closing.csv to ./usage\n",
            "Moved 2020_07_closing.csv to ./usage\n",
            "Moved 2020_12_closing.csv to ./usage\n",
            "Moved 2016_11_closing.csv to ./usage\n",
            "Moved 2010_08_closing.csv to ./usage\n",
            "Moved 2017_12_closing.csv to ./usage\n",
            "Moved 2010_12_closing.csv to ./usage\n",
            "Moved 2024_02_closing.csv to ./usage\n",
            "Moved 2018_07_closing.csv to ./usage\n",
            "Moved 2009_06_closing.csv to ./usage\n",
            "Moved 2017_02_closing.csv to ./usage\n",
            "Moved 2008_09_closing.csv to ./usage\n",
            "Moved 2014_05_closing.csv to ./usage\n",
            "Moved 2006_05_closing.csv to ./usage\n",
            "Moved 2017_06_closing.csv to ./usage\n",
            "Moved 2024_01_closing.csv to ./usage\n",
            "Moved 2015_03_closing.csv to ./usage\n",
            "Moved 2010_10_closing.csv to ./usage\n",
            "Moved 2007_12_closing.csv to ./usage\n",
            "Moved 2009_11_closing.csv to ./usage\n",
            "Moved 2009_03_closing.csv to ./usage\n",
            "Moved 2003_05_closing.csv to ./usage\n",
            "Moved 2015_05_closing.csv to ./usage\n",
            "Moved 2017_03_closing.csv to ./usage\n",
            "Moved 2018_05_closing.csv to ./usage\n",
            "Moved 2005_10_closing.csv to ./usage\n",
            "Moved 2005_11_closing.csv to ./usage\n",
            "Moved 2012_08_closing.csv to ./usage\n",
            "Moved 2024_04_closing.csv to ./usage\n",
            "Moved 2011_04_closing.csv to ./usage\n",
            "Moved 2021_11_closing.csv to ./usage\n",
            "Moved 2020_08_closing.csv to ./usage\n",
            "Moved 2022_05_closing.csv to ./usage\n",
            "Moved 2015_06_closing.csv to ./usage\n",
            "Moved 2009_08_closing.csv to ./usage\n",
            "Moved 2003_02_closing.csv to ./usage\n",
            "Moved 2006_12_closing.csv to ./usage\n",
            "Moved 2012_03_closing.csv to ./usage\n",
            "Moved 2009_05_closing.csv to ./usage\n",
            "Moved 2010_06_closing.csv to ./usage\n",
            "Moved 2008_11_closing.csv to ./usage\n",
            "Moved 2015_01_closing.csv to ./usage\n",
            "Moved 2023_02_closing.csv to ./usage\n",
            "Moved 2018_09_closing.csv to ./usage\n",
            "Moved 2015_08_closing.csv to ./usage\n",
            "Moved 2004_06_closing.csv to ./usage\n",
            "Moved 2021_09_closing.csv to ./usage\n",
            "Moved 2012_01_closing.csv to ./usage\n",
            "Moved 2024_07_closing.csv to ./usage\n",
            "Moved 2010_07_closing.csv to ./usage\n",
            "Moved 2010_11_closing.csv to ./usage\n",
            "Moved 2017_08_closing.csv to ./usage\n",
            "Moved 2013_03_closing.csv to ./usage\n",
            "Moved 2015_07_closing.csv to ./usage\n",
            "Moved 2009_09_closing.csv to ./usage\n",
            "Moved 2023_11_closing.csv to ./usage\n",
            "Moved 2024_10_closing.csv to ./usage\n",
            "Moved 2004_05_closing.csv to ./usage\n",
            "Moved 2023_12_closing.csv to ./usage\n",
            "Moved 2003_04_closing.csv to ./usage\n",
            "Moved 2022_08_closing.csv to ./usage\n",
            "Moved 2018_03_closing.csv to ./usage\n",
            "Moved 2003_07_closing.csv to ./usage\n",
            "Moved 2007_07_closing.csv to ./usage\n",
            "Moved 2020_01_closing.csv to ./usage\n",
            "Moved 2008_04_closing.csv to ./usage\n",
            "Moved 2019_07_closing.csv to ./usage\n",
            "Moved 2007_10_closing.csv to ./usage\n",
            "Moved 2003_10_closing.csv to ./usage\n",
            "Moved 2017_05_closing.csv to ./usage\n",
            "Moved 2019_03_closing.csv to ./usage\n",
            "Moved 2014_11_closing.csv to ./usage\n",
            "Moved 2008_03_closing.csv to ./usage\n",
            "Moved 2014_01_closing.csv to ./usage\n",
            "Moved 2015_02_closing.csv to ./usage\n",
            "Moved 2011_08_closing.csv to ./usage\n",
            "Moved 2003_09_closing.csv to ./usage\n",
            "Moved 2018_02_closing.csv to ./usage\n",
            "Moved 2008_07_closing.csv to ./usage\n",
            "Moved 2015_11_closing.csv to ./usage\n",
            "Moved 2014_09_closing.csv to ./usage\n",
            "Moved 2019_11_closing.csv to ./usage\n",
            "Moved 2022_12_closing.csv to ./usage\n",
            "Moved 2006_10_closing.csv to ./usage\n",
            "Moved 2019_05_closing.csv to ./usage\n",
            "Moved 2014_12_closing.csv to ./usage\n",
            "Moved 2024_05_closing.csv to ./usage\n",
            "Moved 2007_08_closing.csv to ./usage\n",
            "Moved 2013_08_closing.csv to ./usage\n",
            "Moved 2008_01_closing.csv to ./usage\n",
            "Moved 2009_04_closing.csv to ./usage\n",
            "Moved 2023_07_closing.csv to ./usage\n",
            "Moved 2011_05_closing.csv to ./usage\n",
            "Moved 2009_10_closing.csv to ./usage\n",
            "Moved 2024_06_closing.csv to ./usage\n",
            "Moved 2005_03_closing.csv to ./usage\n",
            "Moved 2003_11_closing.csv to ./usage\n",
            "Moved 2021_01_closing.csv to ./usage\n",
            "Moved 2008_12_closing.csv to ./usage\n",
            "Moved 2009_12_closing.csv to ./usage\n",
            "Moved 2013_09_closing.csv to ./usage\n",
            "Moved 2020_10_closing.csv to ./usage\n",
            "Moved 2004_11_closing.csv to ./usage\n",
            "Moved 2008_05_closing.csv to ./usage\n",
            "Moved 2019_02_closing.csv to ./usage\n",
            "Moved 2008_06_closing.csv to ./usage\n",
            "Moved 2004_03_closing.csv to ./usage\n",
            "Moved 2013_12_closing.csv to ./usage\n",
            "Moved 2013_01_closing.csv to ./usage\n",
            "Moved 2019_06_closing.csv to ./usage\n",
            "Moved 2021_02_closing.csv to ./usage\n",
            "Moved 2024_11_closing.csv to ./usage\n",
            "Moved 2004_07_closing.csv to ./usage\n",
            "Moved 2006_02_closing.csv to ./usage\n",
            "Moved 2021_08_closing.csv to ./usage\n",
            "Moved 2008_08_closing.csv to ./usage\n",
            "Moved 2024_03_closing.csv to ./usage\n",
            "Moved 2020_04_closing.csv to ./usage\n",
            "Moved 2023_05_closing.csv to ./usage\n",
            "Moved 2005_09_closing.csv to ./usage\n",
            "Moved 2005_12_closing.csv to ./usage\n",
            "Moved 2012_10_closing.csv to ./usage\n",
            "Moved 2020_05_closing.csv to ./usage\n",
            "Moved 2019_08_closing.csv to ./usage\n",
            "Moved 2024_09_closing.csv to ./usage\n",
            "Moved 2010_04_closing.csv to ./usage\n",
            "Moved 2017_07_closing.csv to ./usage\n",
            "Moved 2016_12_closing.csv to ./usage\n",
            "Moved 2014_06_closing.csv to ./usage\n",
            "Moved 2006_07_closing.csv to ./usage\n",
            "Moved 2023_03_closing.csv to ./usage\n",
            "Moved 2019_09_closing.csv to ./usage\n",
            "Moved 2007_12_obs.csv to ./usage\n",
            "Moved 2017_05_obs.csv to ./usage\n",
            "Moved 2003_10_obs.csv to ./usage\n",
            "Moved 2010_10_obs.csv to ./usage\n",
            "Moved 2012_02_obs.csv to ./usage\n",
            "Moved 2017_06_obs.csv to ./usage\n",
            "Moved 2003_12_obs.csv to ./usage\n",
            "Moved 2009_08_obs.csv to ./usage\n",
            "Moved 2021_12_obs.csv to ./usage\n",
            "Moved 2019_01_obs.csv to ./usage\n",
            "Moved 2017_03_obs.csv to ./usage\n",
            "Moved 2016_11_obs.csv to ./usage\n",
            "Moved 2018_10_obs.csv to ./usage\n",
            "Moved 2009_09_obs.csv to ./usage\n",
            "Moved 2021_11_obs.csv to ./usage\n",
            "Moved 2018_09_obs.csv to ./usage\n",
            "Moved 2024_03_obs.csv to ./usage\n",
            "Moved 2011_01_obs.csv to ./usage\n",
            "Moved 2011_12_obs.csv to ./usage\n",
            "Moved 2007_05_obs.csv to ./usage\n",
            "Moved 2008_10_obs.csv to ./usage\n",
            "Moved 2020_11_obs.csv to ./usage\n",
            "Moved 2010_03_obs.csv to ./usage\n",
            "Moved 2016_02_obs.csv to ./usage\n",
            "Moved 2023_03_obs.csv to ./usage\n",
            "Moved 2015_10_obs.csv to ./usage\n",
            "Moved 2004_09_obs.csv to ./usage\n",
            "Moved 2019_02_obs.csv to ./usage\n",
            "Moved 2004_04_obs.csv to ./usage\n",
            "Moved 2017_10_obs.csv to ./usage\n",
            "Moved 2007_02_obs.csv to ./usage\n",
            "Moved 2023_05_obs.csv to ./usage\n",
            "Moved 2005_03_obs.csv to ./usage\n",
            "Moved 2015_12_obs.csv to ./usage\n",
            "Moved 2023_01_obs.csv to ./usage\n",
            "Moved 2018_12_obs.csv to ./usage\n",
            "Moved 2012_11_obs.csv to ./usage\n",
            "Moved 2007_09_obs.csv to ./usage\n",
            "Moved 2023_06_obs.csv to ./usage\n",
            "Moved 2024_11_obs.csv to ./usage\n",
            "Moved 2009_02_obs.csv to ./usage\n",
            "Moved 2020_04_obs.csv to ./usage\n",
            "Moved 2024_10_obs.csv to ./usage\n",
            "Moved 2016_07_obs.csv to ./usage\n",
            "Moved 2007_11_obs.csv to ./usage\n",
            "Moved 2004_01_obs.csv to ./usage\n",
            "Moved 2016_08_obs.csv to ./usage\n",
            "Moved 2003_08_obs.csv to ./usage\n",
            "Moved 2019_07_obs.csv to ./usage\n",
            "Moved 2021_07_obs.csv to ./usage\n",
            "Moved 2003_07_obs.csv to ./usage\n",
            "Moved 2024_08_obs.csv to ./usage\n",
            "Moved 2014_06_obs.csv to ./usage\n",
            "Moved 2004_11_obs.csv to ./usage\n",
            "Moved 2010_05_obs.csv to ./usage\n",
            "Moved 2021_04_obs.csv to ./usage\n",
            "Moved 2018_11_obs.csv to ./usage\n",
            "Moved 2013_12_obs.csv to ./usage\n",
            "Moved 2009_10_obs.csv to ./usage\n",
            "Moved 2012_10_obs.csv to ./usage\n",
            "Moved 2005_12_obs.csv to ./usage\n",
            "Moved 2013_03_obs.csv to ./usage\n",
            "Moved 2017_09_obs.csv to ./usage\n",
            "Moved 2016_03_obs.csv to ./usage\n",
            "Moved 2005_09_obs.csv to ./usage\n",
            "Moved 2023_11_obs.csv to ./usage\n",
            "Moved 2015_05_obs.csv to ./usage\n",
            "Moved 2024_02_obs.csv to ./usage\n",
            "Moved 2013_09_obs.csv to ./usage\n",
            "Moved 2017_11_obs.csv to ./usage\n",
            "Moved 2008_06_obs.csv to ./usage\n",
            "Moved 2014_04_obs.csv to ./usage\n",
            "Moved 2024_12_obs.csv to ./usage\n",
            "Moved 2006_06_obs.csv to ./usage\n",
            "Moved 2005_05_obs.csv to ./usage\n",
            "Moved 2007_06_obs.csv to ./usage\n",
            "Moved 2023_12_obs.csv to ./usage\n",
            "Moved 2007_03_obs.csv to ./usage\n",
            "Moved 2013_02_obs.csv to ./usage\n",
            "Moved 2007_07_obs.csv to ./usage\n",
            "Moved 2011_02_obs.csv to ./usage\n",
            "Moved 2018_05_obs.csv to ./usage\n",
            "Moved 2015_11_obs.csv to ./usage\n",
            "Moved 2006_04_obs.csv to ./usage\n",
            "Moved 2012_04_obs.csv to ./usage\n",
            "Moved 2012_08_obs.csv to ./usage\n",
            "Moved 2006_08_obs.csv to ./usage\n",
            "Moved 2015_01_obs.csv to ./usage\n",
            "Moved 2009_12_obs.csv to ./usage\n",
            "Moved 2008_04_obs.csv to ./usage\n",
            "Moved 2013_01_obs.csv to ./usage\n",
            "Moved 2023_07_obs.csv to ./usage\n",
            "Moved 2006_10_obs.csv to ./usage\n",
            "Moved 2019_05_obs.csv to ./usage\n",
            "Moved 2013_05_obs.csv to ./usage\n",
            "Moved 2017_01_obs.csv to ./usage\n",
            "Moved 2004_02_obs.csv to ./usage\n",
            "Moved 2011_05_obs.csv to ./usage\n",
            "Moved 2004_07_obs.csv to ./usage\n",
            "Moved 2015_03_obs.csv to ./usage\n",
            "Moved 2014_02_obs.csv to ./usage\n",
            "Moved 2019_10_obs.csv to ./usage\n",
            "Moved 2020_08_obs.csv to ./usage\n",
            "Moved 2005_01_obs.csv to ./usage\n",
            "Moved 2022_07_obs.csv to ./usage\n",
            "Moved 2015_02_obs.csv to ./usage\n",
            "Moved 2022_11_obs.csv to ./usage\n",
            "Moved 2023_08_obs.csv to ./usage\n",
            "Moved 2023_02_obs.csv to ./usage\n",
            "Moved 2004_03_obs.csv to ./usage\n",
            "Moved 2019_09_obs.csv to ./usage\n",
            "Moved 2018_07_obs.csv to ./usage\n",
            "Moved 2009_07_obs.csv to ./usage\n",
            "Moved 2011_09_obs.csv to ./usage\n",
            "Moved 2008_08_obs.csv to ./usage\n",
            "Moved 2012_12_obs.csv to ./usage\n",
            "Moved 2016_06_obs.csv to ./usage\n",
            "Moved 2021_06_obs.csv to ./usage\n",
            "Moved 2017_02_obs.csv to ./usage\n",
            "Moved 2005_06_obs.csv to ./usage\n",
            "Moved 2006_02_obs.csv to ./usage\n",
            "Moved 2009_04_obs.csv to ./usage\n",
            "Moved 2021_08_obs.csv to ./usage\n",
            "Moved 2008_03_obs.csv to ./usage\n",
            "Moved 2019_12_obs.csv to ./usage\n",
            "Moved 2010_07_obs.csv to ./usage\n",
            "Moved 2022_01_obs.csv to ./usage\n",
            "Moved 2014_09_obs.csv to ./usage\n",
            "Moved 2004_06_obs.csv to ./usage\n",
            "Moved 2010_08_obs.csv to ./usage\n",
            "Moved 2024_06_obs.csv to ./usage\n",
            "Moved 2007_04_obs.csv to ./usage\n",
            "Moved 2024_01_obs.csv to ./usage\n",
            "Moved 2012_05_obs.csv to ./usage\n",
            "Moved 2011_10_obs.csv to ./usage\n",
            "Moved 2018_04_obs.csv to ./usage\n",
            "Moved 2015_06_obs.csv to ./usage\n",
            "Moved 2019_04_obs.csv to ./usage\n",
            "Moved 2010_04_obs.csv to ./usage\n",
            "Moved 2003_04_obs.csv to ./usage\n",
            "Moved 2023_10_obs.csv to ./usage\n",
            "Moved 2022_12_obs.csv to ./usage\n",
            "Moved 2008_02_obs.csv to ./usage\n",
            "Moved 2006_07_obs.csv to ./usage\n",
            "Moved 2013_11_obs.csv to ./usage\n",
            "Moved 2021_03_obs.csv to ./usage\n",
            "Moved 2004_12_obs.csv to ./usage\n",
            "Moved 2017_12_obs.csv to ./usage\n",
            "Moved 2009_01_obs.csv to ./usage\n",
            "Moved 2014_01_obs.csv to ./usage\n",
            "Moved 2011_07_obs.csv to ./usage\n",
            "Moved 2012_09_obs.csv to ./usage\n",
            "Moved 2019_08_obs.csv to ./usage\n",
            "Moved 2023_09_obs.csv to ./usage\n",
            "Moved 2022_03_obs.csv to ./usage\n",
            "Moved 2004_10_obs.csv to ./usage\n",
            "Moved 2003_09_obs.csv to ./usage\n",
            "Moved 2020_10_obs.csv to ./usage\n",
            "Moved 2022_02_obs.csv to ./usage\n",
            "Moved 2022_04_obs.csv to ./usage\n",
            "Moved 2015_07_obs.csv to ./usage\n",
            "Moved 2019_03_obs.csv to ./usage\n",
            "Moved 2016_09_obs.csv to ./usage\n",
            "Moved 2003_05_obs.csv to ./usage\n",
            "Moved 2020_01_obs.csv to ./usage\n",
            "Moved 2007_10_obs.csv to ./usage\n",
            "Moved 2012_06_obs.csv to ./usage\n",
            "Moved 2015_09_obs.csv to ./usage\n",
            "Moved 2006_05_obs.csv to ./usage\n",
            "Moved 2021_01_obs.csv to ./usage\n",
            "Moved 2024_04_obs.csv to ./usage\n",
            "Moved 2017_04_obs.csv to ./usage\n",
            "Moved 2018_08_obs.csv to ./usage\n",
            "Moved 2020_06_obs.csv to ./usage\n",
            "Moved 2024_07_obs.csv to ./usage\n",
            "Moved 2012_07_obs.csv to ./usage\n",
            "Moved 2009_11_obs.csv to ./usage\n",
            "Moved 2022_06_obs.csv to ./usage\n",
            "Moved 2014_05_obs.csv to ./usage\n",
            "Moved 2024_09_obs.csv to ./usage\n",
            "Moved 2006_03_obs.csv to ./usage\n",
            "Moved 2007_08_obs.csv to ./usage\n",
            "Moved 2016_12_obs.csv to ./usage\n",
            "Moved 2003_03_obs.csv to ./usage\n",
            "Moved 2020_05_obs.csv to ./usage\n",
            "Moved 2013_06_obs.csv to ./usage\n",
            "Moved 2010_01_obs.csv to ./usage\n",
            "Moved 2021_10_obs.csv to ./usage\n",
            "Moved 2017_07_obs.csv to ./usage\n",
            "Moved 2010_12_obs.csv to ./usage\n",
            "Moved 2013_08_obs.csv to ./usage\n",
            "Moved 2009_05_obs.csv to ./usage\n",
            "Moved 2018_01_obs.csv to ./usage\n",
            "Moved 2005_04_obs.csv to ./usage\n",
            "Moved 2008_12_obs.csv to ./usage\n",
            "Moved 2023_04_obs.csv to ./usage\n",
            "Moved 2003_02_obs.csv to ./usage\n",
            "Moved 2008_05_obs.csv to ./usage\n",
            "Moved 2010_06_obs.csv to ./usage\n",
            "Moved 2014_08_obs.csv to ./usage\n",
            "Moved 2020_02_obs.csv to ./usage\n",
            "Moved 2016_05_obs.csv to ./usage\n",
            "Moved 2006_11_obs.csv to ./usage\n",
            "Moved 2018_03_obs.csv to ./usage\n",
            "Moved 2013_10_obs.csv to ./usage\n",
            "Moved 2008_11_obs.csv to ./usage\n",
            "Moved 2011_03_obs.csv to ./usage\n",
            "Moved 2021_02_obs.csv to ./usage\n",
            "Moved 2013_04_obs.csv to ./usage\n",
            "Moved 2007_01_obs.csv to ./usage\n",
            "Moved 2003_11_obs.csv to ./usage\n",
            "Moved 2008_01_obs.csv to ./usage\n",
            "Moved 2019_11_obs.csv to ./usage\n",
            "Moved 2003_06_obs.csv to ./usage\n",
            "Moved 2004_08_obs.csv to ./usage\n",
            "Moved 2015_04_obs.csv to ./usage\n",
            "Moved 2020_07_obs.csv to ./usage\n",
            "Moved 2008_07_obs.csv to ./usage\n",
            "Moved 2011_04_obs.csv to ./usage\n",
            "Moved 2014_07_obs.csv to ./usage\n",
            "Moved 2012_01_obs.csv to ./usage\n",
            "Moved 2005_08_obs.csv to ./usage\n",
            "Moved 2019_06_obs.csv to ./usage\n",
            "Moved 2003_01_obs.csv to ./usage\n",
            "Moved 2006_09_obs.csv to ./usage\n",
            "Moved 2008_09_obs.csv to ./usage\n",
            "Moved 2018_02_obs.csv to ./usage\n",
            "Moved 2020_03_obs.csv to ./usage\n",
            "Moved 2022_08_obs.csv to ./usage\n",
            "Moved 2005_02_obs.csv to ./usage\n",
            "Moved 2004_05_obs.csv to ./usage\n",
            "Moved 2022_10_obs.csv to ./usage\n",
            "Moved 2010_02_obs.csv to ./usage\n",
            "Moved 2006_01_obs.csv to ./usage\n",
            "Moved 2011_11_obs.csv to ./usage\n",
            "Moved 2011_08_obs.csv to ./usage\n",
            "Moved 2014_11_obs.csv to ./usage\n",
            "Moved 2020_09_obs.csv to ./usage\n",
            "Moved 2011_06_obs.csv to ./usage\n",
            "Moved 2012_03_obs.csv to ./usage\n",
            "Moved 2005_07_obs.csv to ./usage\n",
            "Moved 2014_10_obs.csv to ./usage\n",
            "Moved 2016_10_obs.csv to ./usage\n",
            "Moved 2010_11_obs.csv to ./usage\n",
            "Moved 2016_01_obs.csv to ./usage\n",
            "Moved 2015_08_obs.csv to ./usage\n",
            "Moved 2021_05_obs.csv to ./usage\n",
            "Moved 2014_12_obs.csv to ./usage\n",
            "Moved 2024_05_obs.csv to ./usage\n",
            "Moved 2005_11_obs.csv to ./usage\n",
            "Moved 2022_05_obs.csv to ./usage\n",
            "Moved 2009_03_obs.csv to ./usage\n",
            "Moved 2016_04_obs.csv to ./usage\n",
            "Moved 2018_06_obs.csv to ./usage\n",
            "Moved 2013_07_obs.csv to ./usage\n",
            "Moved 2020_12_obs.csv to ./usage\n",
            "Moved 2005_10_obs.csv to ./usage\n",
            "Moved 2022_09_obs.csv to ./usage\n",
            "Moved 2017_08_obs.csv to ./usage\n",
            "Moved 2010_09_obs.csv to ./usage\n",
            "Moved 2009_06_obs.csv to ./usage\n",
            "Moved 2014_03_obs.csv to ./usage\n",
            "Moved 2021_09_obs.csv to ./usage\n",
            "Moved 2006_12_obs.csv to ./usage\n",
            "Moved 2010_06_corr.csv to ./usage\n",
            "Moved 2013_11_corr.csv to ./usage\n",
            "Moved 2005_05_corr.csv to ./usage\n",
            "Moved 2009_02_corr.csv to ./usage\n",
            "Moved 2018_11_corr.csv to ./usage\n",
            "Moved 2014_11_corr.csv to ./usage\n",
            "Moved 2008_09_corr.csv to ./usage\n",
            "Moved 2005_07_corr.csv to ./usage\n",
            "Moved 2005_11_corr.csv to ./usage\n",
            "Moved 2024_11_corr.csv to ./usage\n",
            "Moved 2013_07_corr.csv to ./usage\n",
            "Moved 2009_01_corr.csv to ./usage\n",
            "Moved 2024_03_corr.csv to ./usage\n",
            "Moved 2009_03_corr.csv to ./usage\n",
            "Moved 2021_07_corr.csv to ./usage\n",
            "Moved 2018_04_corr.csv to ./usage\n",
            "Moved 2018_06_corr.csv to ./usage\n",
            "Moved 2011_11_corr.csv to ./usage\n",
            "Moved 2022_07_corr.csv to ./usage\n",
            "Moved 2006_10_corr.csv to ./usage\n",
            "Moved 2008_01_corr.csv to ./usage\n",
            "Moved 2022_11_corr.csv to ./usage\n",
            "Moved 2022_04_corr.csv to ./usage\n",
            "Moved 2018_09_corr.csv to ./usage\n",
            "Moved 2016_12_corr.csv to ./usage\n",
            "Moved 2023_08_corr.csv to ./usage\n",
            "Moved 2011_03_corr.csv to ./usage\n",
            "Moved 2012_05_corr.csv to ./usage\n",
            "Moved 2013_08_corr.csv to ./usage\n",
            "Moved 2017_12_corr.csv to ./usage\n",
            "Moved 2021_06_corr.csv to ./usage\n",
            "Moved 2007_04_corr.csv to ./usage\n",
            "Moved 2023_01_corr.csv to ./usage\n",
            "Moved 2003_12_corr.csv to ./usage\n",
            "Moved 2015_06_corr.csv to ./usage\n",
            "Moved 2019_09_corr.csv to ./usage\n",
            "Moved 2003_07_corr.csv to ./usage\n",
            "Moved 2003_11_corr.csv to ./usage\n",
            "Moved 2013_06_corr.csv to ./usage\n",
            "Moved 2018_01_corr.csv to ./usage\n",
            "Moved 2022_09_corr.csv to ./usage\n",
            "Moved 2012_09_corr.csv to ./usage\n",
            "Moved 2006_08_corr.csv to ./usage\n",
            "Moved 2014_01_corr.csv to ./usage\n",
            "Moved 2008_10_corr.csv to ./usage\n",
            "Moved 2014_02_corr.csv to ./usage\n",
            "Moved 2013_10_corr.csv to ./usage\n",
            "Moved 2022_12_corr.csv to ./usage\n",
            "Moved 2017_05_corr.csv to ./usage\n",
            "Moved 2024_06_corr.csv to ./usage\n",
            "Moved 2003_06_corr.csv to ./usage\n",
            "Moved 2022_02_corr.csv to ./usage\n",
            "Moved 2016_02_corr.csv to ./usage\n",
            "Moved 2020_01_corr.csv to ./usage\n",
            "Moved 2011_10_corr.csv to ./usage\n",
            "Moved 2012_12_corr.csv to ./usage\n",
            "Moved 2006_01_corr.csv to ./usage\n",
            "Moved 2011_04_corr.csv to ./usage\n",
            "Moved 2003_02_corr.csv to ./usage\n",
            "Moved 2016_01_corr.csv to ./usage\n",
            "Moved 2009_11_corr.csv to ./usage\n",
            "Moved 2023_06_corr.csv to ./usage\n",
            "Moved 2013_05_corr.csv to ./usage\n",
            "Moved 2023_05_corr.csv to ./usage\n",
            "Moved 2021_01_corr.csv to ./usage\n",
            "Moved 2019_10_corr.csv to ./usage\n",
            "Moved 2003_10_corr.csv to ./usage\n",
            "Moved 2004_04_corr.csv to ./usage\n",
            "Moved 2008_11_corr.csv to ./usage\n",
            "Moved 2010_07_corr.csv to ./usage\n",
            "Moved 2022_03_corr.csv to ./usage\n",
            "Moved 2020_07_corr.csv to ./usage\n",
            "Moved 2003_09_corr.csv to ./usage\n",
            "Moved 2009_05_corr.csv to ./usage\n",
            "Moved 2021_05_corr.csv to ./usage\n",
            "Moved 2016_05_corr.csv to ./usage\n",
            "Moved 2020_06_corr.csv to ./usage\n",
            "Moved 2004_07_corr.csv to ./usage\n",
            "Moved 2010_10_corr.csv to ./usage\n",
            "Moved 2016_08_corr.csv to ./usage\n",
            "Moved 2024_07_corr.csv to ./usage\n",
            "Moved 2020_08_corr.csv to ./usage\n",
            "Moved 2005_03_corr.csv to ./usage\n",
            "Moved 2015_02_corr.csv to ./usage\n",
            "Moved 2011_06_corr.csv to ./usage\n",
            "Moved 2012_01_corr.csv to ./usage\n",
            "Moved 2012_07_corr.csv to ./usage\n",
            "Moved 2011_09_corr.csv to ./usage\n",
            "Moved 2007_06_corr.csv to ./usage\n",
            "Moved 2011_02_corr.csv to ./usage\n",
            "Moved 2020_10_corr.csv to ./usage\n",
            "Moved 2009_06_corr.csv to ./usage\n",
            "Moved 2019_04_corr.csv to ./usage\n",
            "Moved 2008_03_corr.csv to ./usage\n",
            "Moved 2023_11_corr.csv to ./usage\n",
            "Moved 2018_07_corr.csv to ./usage\n",
            "Moved 2012_11_corr.csv to ./usage\n",
            "Moved 2021_09_corr.csv to ./usage\n",
            "Moved 2006_04_corr.csv to ./usage\n",
            "Moved 2023_10_corr.csv to ./usage\n",
            "Moved 2020_03_corr.csv to ./usage\n",
            "Moved 2007_11_corr.csv to ./usage\n",
            "Moved 2008_07_corr.csv to ./usage\n",
            "Moved 2007_01_corr.csv to ./usage\n",
            "Moved 2003_03_corr.csv to ./usage\n",
            "Moved 2020_04_corr.csv to ./usage\n",
            "Moved 2019_05_corr.csv to ./usage\n",
            "Moved 2010_02_corr.csv to ./usage\n",
            "Moved 2011_07_corr.csv to ./usage\n",
            "Moved 2023_02_corr.csv to ./usage\n",
            "Moved 2015_09_corr.csv to ./usage\n",
            "Moved 2003_04_corr.csv to ./usage\n",
            "Moved 2004_10_corr.csv to ./usage\n",
            "Moved 2024_05_corr.csv to ./usage\n",
            "Moved 2014_12_corr.csv to ./usage\n",
            "Moved 2019_08_corr.csv to ./usage\n",
            "Moved 2004_11_corr.csv to ./usage\n",
            "Moved 2004_09_corr.csv to ./usage\n",
            "Moved 2024_08_corr.csv to ./usage\n",
            "Moved 2003_01_corr.csv to ./usage\n",
            "Moved 2006_09_corr.csv to ./usage\n",
            "Moved 2007_09_corr.csv to ./usage\n",
            "Moved 2009_12_corr.csv to ./usage\n",
            "Moved 2003_08_corr.csv to ./usage\n",
            "Moved 2010_08_corr.csv to ./usage\n",
            "Moved 2007_08_corr.csv to ./usage\n",
            "Moved 2014_06_corr.csv to ./usage\n",
            "Moved 2020_05_corr.csv to ./usage\n",
            "Moved 2023_09_corr.csv to ./usage\n",
            "Moved 2023_04_corr.csv to ./usage\n",
            "Moved 2024_01_corr.csv to ./usage\n",
            "Moved 2020_11_corr.csv to ./usage\n",
            "Moved 2018_02_corr.csv to ./usage\n",
            "Moved 2017_09_corr.csv to ./usage\n",
            "Moved 2004_12_corr.csv to ./usage\n",
            "Moved 2005_08_corr.csv to ./usage\n",
            "Moved 2016_06_corr.csv to ./usage\n",
            "Moved 2015_12_corr.csv to ./usage\n",
            "Moved 2004_08_corr.csv to ./usage\n",
            "Moved 2013_03_corr.csv to ./usage\n",
            "Moved 2003_05_corr.csv to ./usage\n",
            "Moved 2010_04_corr.csv to ./usage\n",
            "Moved 2023_03_corr.csv to ./usage\n",
            "Moved 2015_05_corr.csv to ./usage\n",
            "Moved 2007_12_corr.csv to ./usage\n",
            "Moved 2014_08_corr.csv to ./usage\n",
            "Moved 2010_11_corr.csv to ./usage\n",
            "Moved 2005_12_corr.csv to ./usage\n",
            "Moved 2015_04_corr.csv to ./usage\n",
            "Moved 2020_09_corr.csv to ./usage\n",
            "Moved 2016_11_corr.csv to ./usage\n",
            "Moved 2009_10_corr.csv to ./usage\n",
            "Moved 2013_09_corr.csv to ./usage\n",
            "Moved 2020_02_corr.csv to ./usage\n",
            "Moved 2006_03_corr.csv to ./usage\n",
            "Moved 2024_02_corr.csv to ./usage\n",
            "Moved 2013_04_corr.csv to ./usage\n",
            "Moved 2006_02_corr.csv to ./usage\n",
            "Moved 2022_08_corr.csv to ./usage\n",
            "Moved 2015_08_corr.csv to ./usage\n",
            "Moved 2010_05_corr.csv to ./usage\n",
            "Moved 2015_03_corr.csv to ./usage\n",
            "Moved 2014_03_corr.csv to ./usage\n",
            "Moved 2006_07_corr.csv to ./usage\n",
            "Moved 2014_05_corr.csv to ./usage\n",
            "Moved 2020_12_corr.csv to ./usage\n",
            "Moved 2017_11_corr.csv to ./usage\n",
            "Moved 2014_04_corr.csv to ./usage\n",
            "Moved 2005_02_corr.csv to ./usage\n",
            "Moved 2018_10_corr.csv to ./usage\n",
            "Moved 2022_05_corr.csv to ./usage\n",
            "Moved 2007_02_corr.csv to ./usage\n",
            "Moved 2012_03_corr.csv to ./usage\n",
            "Moved 2018_08_corr.csv to ./usage\n",
            "Moved 2005_06_corr.csv to ./usage\n",
            "Moved 2004_01_corr.csv to ./usage\n",
            "Moved 2019_02_corr.csv to ./usage\n",
            "Moved 2019_11_corr.csv to ./usage\n",
            "Moved 2008_12_corr.csv to ./usage\n",
            "Moved 2011_05_corr.csv to ./usage\n",
            "Moved 2012_02_corr.csv to ./usage\n",
            "Moved 2015_11_corr.csv to ./usage\n",
            "Moved 2014_09_corr.csv to ./usage\n",
            "Moved 2015_07_corr.csv to ./usage\n",
            "Moved 2006_05_corr.csv to ./usage\n",
            "Moved 2009_04_corr.csv to ./usage\n",
            "Moved 2007_05_corr.csv to ./usage\n",
            "Moved 2017_07_corr.csv to ./usage\n",
            "Moved 2024_09_corr.csv to ./usage\n",
            "Moved 2019_06_corr.csv to ./usage\n",
            "Moved 2009_08_corr.csv to ./usage\n",
            "Moved 2007_07_corr.csv to ./usage\n",
            "Moved 2011_08_corr.csv to ./usage\n",
            "Moved 2013_02_corr.csv to ./usage\n",
            "Moved 2006_11_corr.csv to ./usage\n",
            "Moved 2005_09_corr.csv to ./usage\n",
            "Moved 2017_02_corr.csv to ./usage\n",
            "Moved 2017_04_corr.csv to ./usage\n",
            "Moved 2004_02_corr.csv to ./usage\n",
            "Moved 2021_02_corr.csv to ./usage\n",
            "Moved 2018_05_corr.csv to ./usage\n",
            "Moved 2019_03_corr.csv to ./usage\n",
            "Moved 2017_03_corr.csv to ./usage\n",
            "Moved 2022_01_corr.csv to ./usage\n",
            "Moved 2017_08_corr.csv to ./usage\n",
            "Moved 2021_03_corr.csv to ./usage\n",
            "Moved 2019_12_corr.csv to ./usage\n",
            "Moved 2018_12_corr.csv to ./usage\n",
            "Moved 2019_07_corr.csv to ./usage\n",
            "Moved 2012_08_corr.csv to ./usage\n",
            "Moved 2022_06_corr.csv to ./usage\n",
            "Moved 2014_07_corr.csv to ./usage\n",
            "Moved 2010_03_corr.csv to ./usage\n",
            "Moved 2024_12_corr.csv to ./usage\n",
            "Moved 2012_10_corr.csv to ./usage\n",
            "Moved 2013_12_corr.csv to ./usage\n",
            "Moved 2009_07_corr.csv to ./usage\n",
            "Moved 2005_04_corr.csv to ./usage\n",
            "Moved 2012_04_corr.csv to ./usage\n",
            "Moved 2007_03_corr.csv to ./usage\n",
            "Moved 2006_06_corr.csv to ./usage\n",
            "Moved 2006_12_corr.csv to ./usage\n",
            "Moved 2019_01_corr.csv to ./usage\n",
            "Moved 2004_03_corr.csv to ./usage\n",
            "Moved 2011_12_corr.csv to ./usage\n",
            "Moved 2015_01_corr.csv to ./usage\n",
            "Moved 2017_06_corr.csv to ./usage\n",
            "Moved 2008_04_corr.csv to ./usage\n",
            "Moved 2024_04_corr.csv to ./usage\n",
            "Moved 2010_12_corr.csv to ./usage\n",
            "Moved 2008_06_corr.csv to ./usage\n",
            "Moved 2021_10_corr.csv to ./usage\n",
            "Moved 2010_09_corr.csv to ./usage\n",
            "Moved 2021_12_corr.csv to ./usage\n",
            "Moved 2022_10_corr.csv to ./usage\n",
            "Moved 2007_10_corr.csv to ./usage\n",
            "Moved 2004_05_corr.csv to ./usage\n",
            "Moved 2023_12_corr.csv to ./usage\n",
            "Moved 2015_10_corr.csv to ./usage\n",
            "Moved 2010_01_corr.csv to ./usage\n",
            "Moved 2009_09_corr.csv to ./usage\n",
            "Moved 2005_01_corr.csv to ./usage\n",
            "Moved 2008_05_corr.csv to ./usage\n",
            "Moved 2004_06_corr.csv to ./usage\n",
            "Moved 2016_03_corr.csv to ./usage\n",
            "Moved 2011_01_corr.csv to ./usage\n",
            "Moved 2018_03_corr.csv to ./usage\n",
            "Moved 2005_10_corr.csv to ./usage\n",
            "Moved 2017_01_corr.csv to ./usage\n",
            "Moved 2008_08_corr.csv to ./usage\n",
            "Moved 2014_10_corr.csv to ./usage\n",
            "Moved 2017_10_corr.csv to ./usage\n",
            "Moved 2012_06_corr.csv to ./usage\n",
            "Moved 2008_02_corr.csv to ./usage\n",
            "Moved 2021_08_corr.csv to ./usage\n",
            "Moved 2023_07_corr.csv to ./usage\n",
            "Moved 2016_10_corr.csv to ./usage\n",
            "Moved 2024_10_corr.csv to ./usage\n",
            "Moved 2021_11_corr.csv to ./usage\n",
            "Moved 2016_07_corr.csv to ./usage\n",
            "Moved 2016_04_corr.csv to ./usage\n",
            "Moved 2021_04_corr.csv to ./usage\n",
            "Moved 2016_09_corr.csv to ./usage\n",
            "Moved 2013_01_corr.csv to ./usage\n",
            "All CSV files organized successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "def organize_csv_files(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Organize CSV files into three folders: combined, price, and usage.\n",
        "\n",
        "    Folders:\n",
        "    - combined: {yyyy}_{mm}_combined.csv\n",
        "    - price: clean_data.csv, normalized_data.csv, log_evolution_data.csv, portfolio_evolution.csv\n",
        "    - usage: {yyyy}_{mm}_closing.csv, {yyyy}_{mm}_obs.csv, {yyyy}_{mm}_corr.csv\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the CSV files (default: current directory)\n",
        "    output_dir (str): Directory to create folders and move files (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Moves CSV files to appropriate folders\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define folder names and their corresponding files\n",
        "        folders = {\n",
        "            \"combined\": glob(os.path.join(input_dir, \"*_combined.csv\")),\n",
        "            \"price\": [\n",
        "                os.path.join(input_dir, f) for f in [\n",
        "                    \"clean_data.csv\",\n",
        "                    \"normalized_data.csv\",\n",
        "                    \"log_evolution_data.csv\",\n",
        "                    \"portfolio_evolution.csv\",\n",
        "                    \"stock_closing_prices.csv\"\n",
        "                ] if os.path.exists(os.path.join(input_dir, f))\n",
        "            ],\n",
        "            \"usage\": (\n",
        "                glob(os.path.join(input_dir, \"*_closing.csv\")) +\n",
        "                glob(os.path.join(input_dir, \"*_obs.csv\")) +\n",
        "                glob(os.path.join(input_dir, \"*_corr.csv\"))\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Check if any files were found\n",
        "        total_files = sum(len(files) for files in folders.values())\n",
        "        if total_files == 0:\n",
        "            print(f\"Error: No matching CSV files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Create folders if they don't exist\n",
        "        for folder in folders:\n",
        "            folder_path = os.path.join(output_dir, folder)\n",
        "            os.makedirs(folder_path, exist_ok=True)\n",
        "            print(f\"Created/Verified folder: {folder_path}\")\n",
        "\n",
        "        # Move files to their respective folders\n",
        "        for folder, files in folders.items():\n",
        "            target_dir = os.path.join(output_dir, folder)\n",
        "            for file in files:\n",
        "                filename = os.path.basename(file)\n",
        "                target_path = os.path.join(target_dir, filename)\n",
        "\n",
        "                # Skip if file already exists in target to avoid overwriting\n",
        "                if os.path.exists(target_path):\n",
        "                    print(f\"Skipped {filename}: Already exists in {target_dir}\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    shutil.move(file, target_path)\n",
        "                    print(f\"Moved {filename} to {target_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error moving {filename}: {e}\")\n",
        "\n",
        "        print(\"All CSV files organized successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error organizing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    organize_csv_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjcIIx5QK8oC"
      },
      "source": [
        "## NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5297XEAQ2s0"
      },
      "source": [
        "### Main pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEClekVWQ4_G"
      },
      "source": [
        "### Simulated output for functionality check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOF-jErcRBwP"
      },
      "source": [
        "We simulate results to check the functionality of the code as the NLP main pipeline is quite time expensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il7MWpjULA5h"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdMqrdwZQbNb"
      },
      "source": [
        "###  Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG_UBO3VP-_e"
      },
      "source": [
        "1. Plot price evolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dWuLBo5QCDQ"
      },
      "source": [
        "2. Plot log prices evolution for normalized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uksir3kUQNWt"
      },
      "source": [
        "3. Plot log prices evolution for normalized data VS EQWP (Equal weight portfolio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyBcc0BnQLbB"
      },
      "source": [
        "4. Plot correaltion matrix heatmap for each month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPh2lx9_lHcv",
        "outputId": "bc71cdd2-cc1f-47cb-a4b2-aa95f07ab819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created/Verified folders: ./organized/plots, ./organized/plots/corr\n",
            "Saved plots/normalized_evolution.png\n",
            "Saved plots/log_evolution.png\n",
            "Saved plots/log_evolution_portfolio.png\n",
            "Saved plots/normalized_portfolio.png\n",
            "Saved ./organized/plots/corr/2010_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2003_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2020_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2006_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2019_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2022_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2007_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_12_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2015_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2010_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2009_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_05_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2004_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2011_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2018_03_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2005_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_01_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2014_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2017_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2012_06_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2008_02_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_08_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2023_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2024_10_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_11_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_07_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2021_04_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2016_09_corr_heatmap.png\n",
            "Saved ./organized/plots/corr/2013_01_corr_heatmap.png\n",
            "All available plots and heatmaps generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def generate_plots(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Generate plots and correlation heatmaps from CSV files.\n",
        "\n",
        "    Creates:\n",
        "    - plots/normalized_evolution.png: Normalized prices for all tickers\n",
        "    - plots/log_evolution.png: Log evolution for all tickers\n",
        "    - plots/log_evolution_portfolio.png: Log evolution + portfolio log evolution\n",
        "    - plots/normalized_portfolio.png: Normalized prices + normalized portfolio\n",
        "    - plots/corr/{yyyy}_{mm}_corr_heatmap.png: Heatmaps for monthly correlations\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing price and usage folders (default: current)\n",
        "    output_dir (str): Directory to save plots folder (default: current)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves plots and heatmaps to plots and plots/corr folders\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create plots and corr folders\n",
        "        plots_dir = os.path.join(output_dir, \"plots\")\n",
        "        corr_dir = os.path.join(plots_dir, \"corr\")\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        os.makedirs(corr_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folders: {plots_dir}, {corr_dir}\")\n",
        "\n",
        "        # Paths to input files\n",
        "        price_dir = os.path.join(input_dir, \"price\")\n",
        "        usage_dir = os.path.join(input_dir, \"usage\")\n",
        "\n",
        "        # Load data for plots, handling missing files\n",
        "        normalized_df = None\n",
        "        log_evolution_df = None\n",
        "        portfolio_df = None\n",
        "\n",
        "        normalized_file = os.path.join(price_dir, \"normalized_data.csv\")\n",
        "        if os.path.exists(normalized_file):\n",
        "            try:\n",
        "                normalized_df = pd.read_csv(normalized_file, index_col=\"Date\", parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {normalized_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {normalized_file} not found. Skipping normalized plots.\")\n",
        "\n",
        "        log_evolution_file = os.path.join(price_dir, \"log_evolution_data.csv\")\n",
        "        if os.path.exists(log_evolution_file):\n",
        "            try:\n",
        "                log_evolution_df = pd.read_csv(log_evolution_file, index_col=\"Date\", parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {log_evolution_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {log_evolution_file} not found. Skipping log evolution plots.\")\n",
        "\n",
        "        portfolio_file = os.path.join(price_dir, \"portfolio_evolution.csv\")\n",
        "        if os.path.exists(portfolio_file):\n",
        "            try:\n",
        "                portfolio_df = pd.read_csv(portfolio_file, index_col=\"Date\", parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {portfolio_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {portfolio_file} not found. Skipping portfolio-related plots.\")\n",
        "\n",
        "        # Plot 1: Normalized Evolution\n",
        "        if normalized_df is not None and not normalized_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in normalized_df.columns:\n",
        "                if normalized_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in normalized_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(normalized_df.index, normalized_df[column], label=column)\n",
        "            plt.title(\"Normalized Evolution of Tickers\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Normalized Price (First Point = 1)\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"normalized_evolution.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/normalized_evolution.png\")\n",
        "        else:\n",
        "            print(\"Skipping normalized_evolution.png: No valid normalized data.\")\n",
        "\n",
        "        # Plot 2: Log Evolution\n",
        "        if log_evolution_df is not None and not log_evolution_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in log_evolution_df.columns:\n",
        "                if log_evolution_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in log_evolution_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(log_evolution_df.index, log_evolution_df[column], label=column)\n",
        "            plt.title(\"Log Evolution of Tickers\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Log Evolution (ln(price / price_0))\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"log_evolution.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/log_evolution.png\")\n",
        "        else:\n",
        "            print(\"Skipping log_evolution.png: No valid log evolution data.\")\n",
        "\n",
        "        # Plot 3: Log Evolution + Portfolio\n",
        "        if log_evolution_df is not None and portfolio_df is not None and not log_evolution_df.empty and not portfolio_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in log_evolution_df.columns:\n",
        "                if log_evolution_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in log_evolution_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(log_evolution_df.index, log_evolution_df[column], label=column)\n",
        "            plt.plot(portfolio_df.index, portfolio_df[\"Log_Evolution\"],\n",
        "                    label=\"Equal Weights Portfolio\", linewidth=2, linestyle=\"--\")\n",
        "            plt.title(\"Log Evolution of Tickers and Equal Weights Portfolio\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Log Evolution\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"log_evolution_portfolio.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/log_evolution_portfolio.png\")\n",
        "        else:\n",
        "            print(\"Skipping log_evolution_portfolio.png: Missing log evolution or portfolio data.\")\n",
        "\n",
        "        # Plot 4: Normalized + Portfolio\n",
        "        if normalized_df is not None and portfolio_df is not None and not normalized_df.empty and not portfolio_df.empty:\n",
        "            # Normalize portfolio value to start at 1\n",
        "            portfolio_normalized = portfolio_df[\"Portfolio_Value\"] / portfolio_df[\"Portfolio_Value\"].iloc[0]\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in normalized_df.columns:\n",
        "                if normalized_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in normalized_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(normalized_df.index, normalized_df[column], label=column)\n",
        "            plt.plot(portfolio_normalized.index, portfolio_normalized,\n",
        "                    label=\"Equal Weights Portfolio\", linewidth=2, linestyle=\"--\")\n",
        "            plt.title(\"Normalized Evolution of Tickers and Equal Weights Portfolio\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Normalized Value (First Point = 1)\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"normalized_portfolio.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/normalized_portfolio.png\")\n",
        "        else:\n",
        "            print(\"Skipping normalized_portfolio.png: Missing normalized or portfolio data.\")\n",
        "\n",
        "        # Generate correlation matrix heatmaps\n",
        "        corr_files = glob(os.path.join(usage_dir, \"*_corr.csv\"))\n",
        "        if not corr_files:\n",
        "            print(f\"Warning: No *_corr.csv files found in {usage_dir}.\")\n",
        "        else:\n",
        "            for corr_file in corr_files:\n",
        "                try:\n",
        "                    # Extract year and month from filename\n",
        "                    filename = os.path.basename(corr_file)\n",
        "                    year_month = filename.replace(\"_corr.csv\", \"\")\n",
        "\n",
        "                    # Read correlation matrix\n",
        "                    corr_df = pd.read_csv(corr_file, index_col=0)\n",
        "\n",
        "                    # Skip empty or invalid matrices\n",
        "                    if corr_df.empty or corr_df.eq(0).all().all():\n",
        "                        print(f\"Warning: {filename} is empty or all zeros. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Plot heatmap\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    sns.heatmap(corr_df, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1,\n",
        "                               center=0, fmt=\".2f\")\n",
        "                    plt.title(f\"Correlation Matrix - {year_month}\")\n",
        "                    output_file = os.path.join(corr_dir, f\"{year_month}_corr_heatmap.png\")\n",
        "                    plt.savefig(output_file, bbox_inches=\"tight\")\n",
        "                    plt.close()\n",
        "                    print(f\"Saved {output_file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All available plots and heatmaps generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating plots: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming organized structure from previous step\n",
        "    generate_plots(\".\", \"./organized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EBTh4vrQY0X"
      },
      "source": [
        "### NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1OOYB4AQggJ"
      },
      "source": [
        "1. Sentiment score evolution per ticker per month (EMA 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUVFwQZ5Qk-3"
      },
      "source": [
        "2. Volatility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w2OY7FTLEKg"
      },
      "source": [
        "## Data organization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7RP2j-yQrTG"
      },
      "source": [
        "Create files and subfiles for everything, make sure the data fetching section is in a single folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2F1u21Vmbnv",
        "outputId": "368782c3-6f2d-4950-8017-d4dbc5a9e261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created/Verified folder: ./observation/metrics\n",
            "Saved ./observation/metrics/2003_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2017_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2020_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2004_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2003_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2016_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2023_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2024_02_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2015_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_06_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2005_04_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2010_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_05_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2014_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2018_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2021_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2013_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_01_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2011_08_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2006_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_10_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2022_07_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2019_11_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2007_12_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2012_03_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2009_09_combined.csv with 189 rows\n",
            "Saved ./observation/metrics/2008_02_combined.csv with 189 rows\n",
            "All combined metrics CSVs processed successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def extract_combined_metrics(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Extract the second column (Value) from combined CSVs and save to observation/metrics.\n",
        "\n",
        "    Processes:\n",
        "    - All {yyyy}_{mm}_combined.csv from input_dir/organized/combined/\n",
        "    - Saves single-column CSVs to output_dir/observation/metrics/\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing organized/combined folder (default: current)\n",
        "    output_dir (str): Directory to create observation/metrics folder (default: current)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves extracted CSVs to observation/metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define input and output paths\n",
        "        combined_dir = os.path.join(input_dir, \"combined\")\n",
        "        metrics_dir = os.path.join(output_dir, \"observation\", \"metrics\")\n",
        "\n",
        "        # Create observation/metrics folder\n",
        "        os.makedirs(metrics_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {metrics_dir}\")\n",
        "\n",
        "        # Find all combined CSV files\n",
        "        combined_files = glob(os.path.join(combined_dir, \"*_combined.csv\"))\n",
        "        if not combined_files:\n",
        "            print(f\"Error: No *_combined.csv files found in {combined_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each combined CSV\n",
        "        for file in combined_files:\n",
        "            try:\n",
        "                # Extract filename\n",
        "                filename = os.path.basename(file)\n",
        "\n",
        "                # Read CSV (index is first column, Value is second)\n",
        "                df = pd.read_csv(file, index_col=0)\n",
        "\n",
        "                if df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Verify the second column (Value)\n",
        "                if len(df.columns) != 1 or df.columns[0] != \"Value\":\n",
        "                    print(f\"Warning: {filename} does not have exactly one data column (Value). Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract the Value column with the index\n",
        "                output_df = df[[\"Value\"]]\n",
        "\n",
        "                # Save to observation/metrics with the same filename\n",
        "                output_file = os.path.join(metrics_dir, filename)\n",
        "                output_df.to_csv(output_file)\n",
        "                print(f\"Saved {output_file} with {len(output_df)} rows\")\n",
        "\n",
        "                # Note if all values are zero\n",
        "                if output_df[\"Value\"].eq(0).all():\n",
        "                    print(f\"Note: {filename} contains all zero values.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All combined metrics CSVs processed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming organized structure from previous steps\n",
        "    extract_combined_metrics(\".\", \".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24gcJq79nXYX",
        "outputId": "7b69a5df-166a-4490-d66c-9c90728a689a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 2003_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2017_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2020_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2004_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2003_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2016_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2023_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2024_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2015_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_06_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2005_04_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2010_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_05_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2014_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2018_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2021_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2013_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_01_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2011_08_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2006_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_10_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2022_07_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2019_11_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2007_12_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2012_03_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2009_09_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "Processed 2008_02_combined.csv: Removed index column, kept Value column (189 rows)\n",
            "All CSVs in observation/metrics processed successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def remove_index_column(metrics_dir):\n",
        "    \"\"\"\n",
        "    Remove the first column (index) from CSVs in observation/metrics, keeping only the Value column.\n",
        "\n",
        "    Processes:\n",
        "    - All CSVs in metrics_dir (e.g., 2003_01_combined.csv)\n",
        "    - Overwrites each CSV with a single-column version (Value only, no index or header)\n",
        "\n",
        "    Parameters:\n",
        "    metrics_dir (str): Path to observation/metrics directory\n",
        "\n",
        "    Returns:\n",
        "    None: Overwrites CSVs in metrics_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find all CSV files in metrics_dir\n",
        "        csv_files = glob(os.path.join(metrics_dir, \"*.csv\"))\n",
        "        if not csv_files:\n",
        "            print(f\"Error: No CSV files found in {metrics_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each CSV\n",
        "        for file in csv_files:\n",
        "            try:\n",
        "                # Extract filename\n",
        "                filename = os.path.basename(file)\n",
        "\n",
        "                # Read CSV (index is first column, Value is second)\n",
        "                df = pd.read_csv(file, index_col=0)\n",
        "\n",
        "                if df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Verify the second column (Value)\n",
        "                if len(df.columns) != 1 or df.columns[0] != \"Value\":\n",
        "                    print(f\"Warning: {filename} does not have exactly one data column (Value). Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract the Value column without index\n",
        "                values = df[\"Value\"]\n",
        "\n",
        "                # Save as single-column CSV without index or header\n",
        "                values.to_csv(file, index=False, header=False)\n",
        "                print(f\"Processed {filename}: Removed index column, kept Value column ({len(values)} rows)\")\n",
        "\n",
        "                # Note if all values are zero\n",
        "                if values.eq(0).all():\n",
        "                    print(f\"Note: {filename} contains all zero values.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All CSVs in observation/metrics processed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming organized structure from previous steps\n",
        "    metrics_dir = \"observation/metrics\"\n",
        "    remove_index_column(metrics_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sdy4k0iwCqt"
      },
      "source": [
        "## Simulating NLP Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DhxzNcJvnwx",
        "outputId": "07b003c0-b734-4b50-8989-f7a3f0f34d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created/Verified folder: ./sim_nlp\n",
            "Saved ./sim_nlp/200307.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200608.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200707.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200303.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202408.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201407.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202212.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200809.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201703.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201510.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201608.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202405.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200309.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201103.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200701.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202204.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202307.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200308.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202007.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201509.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200709.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200708.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201210.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201507.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200603.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201410.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201503.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202305.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201712.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202009.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201904.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202309.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201905.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202102.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201912.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201711.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201311.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200506.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201011.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202406.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200703.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202310.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200407.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201209.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201112.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201405.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201610.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202411.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201207.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201111.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201205.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200601.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201409.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201003.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200403.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200503.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200808.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200810.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201710.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200505.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202106.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202104.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201202.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201005.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201512.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200402.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200404.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201302.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201803.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200911.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200803.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201704.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201602.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202109.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202303.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202003.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201102.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200508.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200406.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201508.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201909.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201002.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202404.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201807.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202107.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201001.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201709.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201006.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202304.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200907.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200305.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201708.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202002.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201412.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200512.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202401.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201408.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201305.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201908.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200507.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202004.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201706.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201802.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200509.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200805.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200606.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200408.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200804.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200310.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201404.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201312.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202008.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200605.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200411.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200501.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200908.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201010.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200511.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201701.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200905.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202302.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202206.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201501.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201609.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202011.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201402.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202012.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200903.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201406.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201104.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202205.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200702.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202112.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202001.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201306.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201401.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200410.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202312.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200912.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201403.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201109.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201702.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201004.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202010.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202105.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201505.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200401.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200706.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202211.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200811.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201910.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201604.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201204.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202407.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201106.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202209.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200301.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200611.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200705.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202412.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200906.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200405.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201208.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201810.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202005.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202403.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201705.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201107.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201902.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202203.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200902.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201606.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200711.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201707.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200409.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202006.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201601.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200304.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200806.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200910.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201611.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201307.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201511.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200311.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201303.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201301.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202208.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201502.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202110.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202202.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201808.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200807.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201504.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200412.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202201.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201304.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201201.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202410.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202108.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201806.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201012.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202306.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201101.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201007.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202301.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201309.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200312.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201310.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201804.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202311.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200302.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201812.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201603.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202409.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201105.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201906.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202101.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200306.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200904.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200602.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201607.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200510.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201612.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201801.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200704.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202210.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200812.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201211.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201605.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200801.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200502.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202308.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201009.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202402.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201809.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200604.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201901.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201506.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201206.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200504.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202111.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201008.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201805.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201411.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201907.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201811.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201903.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202103.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201110.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201308.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200610.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200901.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201108.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200609.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200612.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200607.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200710.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/202207.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201911.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201212.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200712.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/201203.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200909.csv with 28-dimensional vector\n",
            "Saved ./sim_nlp/200802.csv with 28-dimensional vector\n",
            "All sim_nlp CSVs generated successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def generate_sim_nlp_vectors(input_dir=\"organized\", output_dir=\"organized\"):\n",
        "    \"\"\"\n",
        "    Create monthly CSVs with 28-dimensional vectors: 14 volatilities + 14 simulated sentiments.\n",
        "\n",
        "    For each month:\n",
        "    - Reads organized/observation/metrics/{yyyy}_{mm}_combined.csv\n",
        "    - Extracts volatilities for 14 assets\n",
        "    - Simulates sentiment scores (-1 to 1) for 14 assets\n",
        "    - Saves as {yyyy}{mm}.csv in sim_nlp/\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing organized/observation/metrics (default: 'organized')\n",
        "    output_dir (str): Directory to create sim_nlp folder (default: 'organized')\n",
        "\n",
        "    Returns:\n",
        "    None: Saves CSVs to sim_nlp\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define tickers (in order)\n",
        "        tickers = [\n",
        "            'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "            '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "        ]\n",
        "\n",
        "        # Define paths\n",
        "        metrics_dir = os.path.join(input_dir, \"observation\", \"metrics\")\n",
        "        sim_nlp_dir = os.path.join(output_dir, \"sim_nlp\")\n",
        "\n",
        "        # Create sim_nlp folder\n",
        "        os.makedirs(sim_nlp_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {sim_nlp_dir}\")\n",
        "\n",
        "        # Find all combined CSV files\n",
        "        combined_files = glob(os.path.join(metrics_dir, \"*_combined.csv\"))\n",
        "        if not combined_files:\n",
        "            print(f\"Error: No *_combined.csv files found in {metrics_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each combined CSV\n",
        "        for file in combined_files:\n",
        "            try:\n",
        "                # Extract year and month from filename (e.g., 2003_01_combined.csv)\n",
        "                filename = os.path.basename(file)\n",
        "                year_month = filename.replace(\"_combined.csv\", \"\")\n",
        "                year, month = year_month.split(\"_\")\n",
        "                output_filename = f\"{year}{month}.csv\"\n",
        "\n",
        "                # Read CSV (single-column, no header)\n",
        "                df = pd.read_csv(file, header=None)\n",
        "\n",
        "                if df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Ensure correct number of rows\n",
        "                expected_rows = 14 * 7 + (14 * 13 // 2)  # 98 metrics + 91 correlations\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {filename} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract volatilities (rows 3rd to 16th, assuming 7 metrics per ticker)\n",
        "                volatility_indices = [2 + i * 7 for i in range(14)]  # Volatility is 3rd metric\n",
        "                volatilities = df.iloc[volatility_indices, 0].values\n",
        "\n",
        "                # Simulate sentiment scores\n",
        "                sentiments = np.random.uniform(-1, 1, 14)\n",
        "\n",
        "                # Combine into 28-dimensional vector\n",
        "                vector = np.concatenate([volatilities, sentiments])\n",
        "\n",
        "                # Create column names\n",
        "                columns = [f\"Vol_{ticker}\" for ticker in tickers] + [f\"Sent_{ticker}\" for ticker in tickers]\n",
        "\n",
        "                # Create single-row DataFrame\n",
        "                output_df = pd.DataFrame([vector], columns=columns)\n",
        "\n",
        "                # Save to sim_nlp\n",
        "                output_file = os.path.join(sim_nlp_dir, output_filename)\n",
        "                output_df.to_csv(output_file, index=False)\n",
        "                print(f\"Saved {output_file} with 28-dimensional vector\")\n",
        "\n",
        "                # Note if volatilities are all zero\n",
        "                if np.all(volatilities == 0):\n",
        "                    print(f\"Note: {filename} has all zero volatilities.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All sim_nlp CSVs generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    generate_sim_nlp_vectors(\".\", \".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5UZORtJnqXS",
        "outputId": "758ce678-c00b-4d7b-dc5d-85bc961ebe61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created/Verified folder: ./metrics_used\n",
            "Moved combined to ./metrics_used\n",
            "Moved sim_nlp to ./metrics_used\n",
            "Moved usage to ./metrics_used\n",
            "Moved organized to ./metrics_used\n",
            "Moved price to ./metrics_used\n",
            "Error moving metrics_used: Cannot move a directory './metrics_used' into itself './metrics_used/metrics_used'.\n",
            "Confirmed: observation folder remains in .\n",
            "All specified folders moved successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_folders_to_metrics_used(input_dir=\"organized\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Create a metrics_used folder and move all folders from input_dir except observation.\n",
        "\n",
        "    Moves:\n",
        "    - combined, price, usage, plots to metrics_used\n",
        "    - Leaves observation in input_dir\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing organized folders (default: 'organized')\n",
        "    output_dir (str): Directory to create metrics_used folder (default: current)\n",
        "\n",
        "    Returns:\n",
        "    None: Moves folders to metrics_used\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define paths\n",
        "        metrics_used_dir = os.path.join(output_dir, \"metrics_used\")\n",
        "\n",
        "        # Create metrics_used folder\n",
        "        os.makedirs(metrics_used_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {metrics_used_dir}\")\n",
        "\n",
        "        # Check if input_dir exists\n",
        "        if not os.path.exists(input_dir):\n",
        "            print(f\"Error: Input directory {input_dir} does not exist.\")\n",
        "            return\n",
        "\n",
        "        # Get list of folders in input_dir\n",
        "        folders = [f for f in os.listdir(input_dir)\n",
        "                  if os.path.isdir(os.path.join(input_dir, f))]\n",
        "\n",
        "        if not folders:\n",
        "            print(f\"Error: No folders found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Exclude observation folder\n",
        "        folders_to_move = [f for f in folders if f != \"observation\"]\n",
        "\n",
        "        if not folders_to_move:\n",
        "            print(f\"Warning: No folders to move (only observation found in {input_dir}).\")\n",
        "            return\n",
        "\n",
        "        # Move each folder to metrics_used\n",
        "        for folder in folders_to_move:\n",
        "            source_path = os.path.join(input_dir, folder)\n",
        "            target_path = os.path.join(metrics_used_dir, folder)\n",
        "\n",
        "            # Skip if folder already exists in target\n",
        "            if os.path.exists(target_path):\n",
        "                print(f\"Skipped {folder}: Already exists in {metrics_used_dir}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                shutil.move(source_path, target_path)\n",
        "                print(f\"Moved {folder} to {metrics_used_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving {folder}: {e}\")\n",
        "\n",
        "        # Verify observation remains\n",
        "        observation_path = os.path.join(input_dir, \"observation\")\n",
        "        if os.path.exists(observation_path):\n",
        "            print(f\"Confirmed: observation folder remains in {input_dir}\")\n",
        "        else:\n",
        "            print(f\"Warning: observation folder not found in {input_dir}\")\n",
        "\n",
        "        print(\"All specified folders moved successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing directories: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    move_folders_to_metrics_used(\".\", \".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iswEGZfXoDhU"
      },
      "source": [
        "In the metrics_used folder, you may find:\n",
        "  - the \"Combined\" folder, a copy of the observation vectors with for each line its meaning\n",
        "  - the \"Organized\" folder, with all the plots of price evolution, and a subfolder with all the correlation matrices heatmaps.\n",
        "  - the \"Price\" folder with everything purely price related (histrorical prices (raw data), cleaned up prices, normalized, log evolution normalized, and the eqwp(equal weights portfolio) evolution)\n",
        "  - the \"Usage\" folder is a bit of a trash folder, with every component of the observation vectors that were later processed and put together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMms689RRRlO"
      },
      "source": [
        "Create the monthly observation both for Metrics and NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHLWiP8qLHqr"
      },
      "source": [
        "# Sb3 agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyudIDFJLLUD"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9v0TJT2sYOs",
        "outputId": "9dab5740-150b-4b38-aea7-be26784c8f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable_baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (11.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3[extra]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable_baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable_baselines3[extra]) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install stable_baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q56kQpl6wmSy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "lDFoyeapsCmH",
        "outputId": "7ac87d57-3677-498c-a8b1-ec87ebbbe014"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  36%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3,649/10,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:09</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:16</span> , <span style=\"color: #800000; text-decoration-color: #800000\">404 it/s</span> ]\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[35m  36%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3,649/10,000 \u001b[0m [ \u001b[33m0:00:09\u001b[0m < \u001b[36m0:00:16\u001b[0m , \u001b[31m404 it/s\u001b[0m ]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import os\n",
        "from stable_baselines3 import PPO, SAC, DDPG, TD3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import random\n",
        "\n",
        "class CustomPortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gymnasium environment for portfolio management.\n",
        "\n",
        "    - Observation: 189-dimensional vector from organized/observation/metrics/{yyyy}_{mm}_combined.csv\n",
        "    - Action: 14-dimensional weight allocations for 14 assets (sum to 1)\n",
        "    - Reward: 2 * ROI - 0.7 * volatility - 0.5 * MDD, computed over the next month\n",
        "    \"\"\"\n",
        "    def __init__(self, price_dir=\"metrics_used/price\", metrics_dir=\"organized/observation/metrics\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define tickers\n",
        "        self.tickers = [\n",
        "            'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "            '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "        ]\n",
        "\n",
        "        # Load daily price data for reward calculation\n",
        "        price_file = os.path.join(price_dir, \"clean_data.csv\")\n",
        "        if not os.path.exists(price_file):\n",
        "            raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
        "        self.daily_prices = pd.read_csv(price_file, index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "        # Load monthly observations\n",
        "        self.metrics_dir = metrics_dir\n",
        "        months = pd.date_range(start=\"2003-01-01\", end=\"2017-12-31\", freq=\"ME\")\n",
        "        self.observations = []\n",
        "        self.month_files = []\n",
        "        for month in months:\n",
        "            file_path = os.path.join(metrics_dir, f\"{month.year}_{month.month:02d}_combined.csv\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, header=None)\n",
        "                    if len(df) == 189:  # Ensure correct observation size\n",
        "                        self.observations.append(df.iloc[:, 0].values.astype(np.float32))\n",
        "                        self.month_files.append(file_path)\n",
        "                    else:\n",
        "                        print(f\"Warning: {file_path} has {len(df)} rows, expected 189. Skipping.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "        if not self.observations:\n",
        "            raise ValueError(\"No valid observation files found.\")\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(189,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        # Get monthly last trading days\n",
        "        self.monthly_last_days = self.daily_prices.resample(\"ME\").last().index.tolist()\n",
        "        self.monthly_last_days = [d for d in self.monthly_last_days\n",
        "                                if d >= pd.Timestamp(\"2003-01-01\") and d <= pd.Timestamp(\"2024-12-31\")]\n",
        "\n",
        "        # Align observations with price data\n",
        "        self.total_steps = min(len(self.observations), len(self.monthly_last_days) - 1)\n",
        "        if self.total_steps < 1:\n",
        "            raise ValueError(\"Insufficient data for training.\")\n",
        "\n",
        "        # Initialize state\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state.\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take an action (weights) and compute the reward for the next month.\n",
        "        \"\"\"\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode is over\")\n",
        "\n",
        "        # Normalize action to sum to 1\n",
        "        action_sum = np.sum(action)\n",
        "        if action_sum > 0:\n",
        "            weights = action / action_sum\n",
        "        else:\n",
        "            weights = np.ones(14) / 14  # Equal weights if invalid\n",
        "\n",
        "        # Get start and end dates for the month\n",
        "        start_date = self.monthly_last_days[self.current_step]\n",
        "        end_date = self.monthly_last_days[self.current_step + 1]\n",
        "\n",
        "        # Get daily prices for the period\n",
        "        try:\n",
        "            daily_prices = self.daily_prices.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing price data for period {start_date} to {end_date}. Using zeros for reward.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute portfolio values\n",
        "        initial_prices = daily_prices.iloc[0].values\n",
        "        daily_portfolio_values = np.sum(daily_prices.values * weights, axis=1)\n",
        "\n",
        "        # Compute ROI\n",
        "        if daily_portfolio_values[0] > 0:\n",
        "            roi = (daily_portfolio_values[-1] / daily_portfolio_values[0]) - 1\n",
        "        else:\n",
        "            roi = 0\n",
        "\n",
        "        # Compute daily returns\n",
        "        daily_returns = daily_portfolio_values[1:] / daily_portfolio_values[:-1] - 1 if len(daily_portfolio_values) > 1 else np.array([0])\n",
        "\n",
        "        # Compute volatility\n",
        "        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Compute MDD\n",
        "        cummax = np.maximum.accumulate(daily_portfolio_values)\n",
        "        drawdown = (cummax - daily_portfolio_values) / cummax if cummax[0] > 0 else np.zeros_like(daily_portfolio_values)\n",
        "        mdd = np.max(drawdown) if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Move to next step\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check if done\n",
        "        done = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Get next observation\n",
        "        obs = self.observations[self.current_step] if not done else self.observations[-1]\n",
        "        info = {\"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_rl_agents(price_dir=\"metrics_used/price\", metrics_dir=\"organized/observation/metrics\",\n",
        "                   output_dir=\"results\", seeds=[1, 2, 3, 4, 5], total_timesteps=10000):\n",
        "    \"\"\"\n",
        "    Train PPO, SAC, DDPG, TD3 models with specified seeds and save results.\n",
        "\n",
        "    Parameters:\n",
        "    price_dir (str): Directory containing clean_data.csv\n",
        "    metrics_dir (str): Directory containing observation vectors\n",
        "    output_dir (str): Directory to save models and evaluation\n",
        "    seeds (list): List of random seeds\n",
        "    total_timesteps (int): Number of timesteps for training\n",
        "\n",
        "    Saves:\n",
        "    - Models: results/{model}_seed_{seed}.zip\n",
        "    - Evaluation: results/evaluation.csv\n",
        "    \"\"\"\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "    # Initialize evaluation results\n",
        "    evaluation_results = []\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        \"ppo\": PPO,\n",
        "        \"sac\": SAC,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    # Create vectorized environment\n",
        "    def make_env():\n",
        "        return CustomPortfolioEnv(price_dir=price_dir, metrics_dir=metrics_dir)\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\nTraining with seed {seed}\")\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        # Create environment\n",
        "        env = make_vec_env(make_env, n_envs=1, seed=seed)\n",
        "\n",
        "        for model_name, model_class in models.items():\n",
        "            print(f\"Training {model_name.upper()}...\")\n",
        "\n",
        "            try:\n",
        "                # Initialize model\n",
        "                model = model_class(\n",
        "                    policy=\"MlpPolicy\",\n",
        "                    env=env,\n",
        "                    verbose=0,\n",
        "                    seed=seed\n",
        "                )\n",
        "\n",
        "                # Train model\n",
        "                model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
        "\n",
        "                # Evaluate model\n",
        "                mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5)\n",
        "\n",
        "                # Save model\n",
        "                model_path = os.path.join(output_dir, f\"{model_name}_seed_{seed}.zip\")\n",
        "                model.save(model_path)\n",
        "                print(f\"Saved model: {model_path}\")\n",
        "\n",
        "                # Store evaluation results\n",
        "                evaluation_results.append({\n",
        "                    \"model\": model_name,\n",
        "                    \"seed\": seed,\n",
        "                    \"mean_reward\": mean_reward,\n",
        "                    \"std_reward\": std_reward\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error training {model_name} with seed {seed}: {e}\")\n",
        "\n",
        "        # Reset environment seed for next iteration\n",
        "        env.reset()\n",
        "\n",
        "    # Save evaluation results\n",
        "    eval_df = pd.DataFrame(evaluation_results)\n",
        "    eval_path = os.path.join(output_dir, \"evaluation.csv\")\n",
        "    eval_df.to_csv(eval_path, index=False)\n",
        "    print(f\"Saved evaluation results: {eval_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_rl_agents(\n",
        "        price_dir=\"./metrics_used/price\",\n",
        "        metrics_dir=\"./observation/metrics\",\n",
        "        output_dir=\"./results\",\n",
        "        seeds=[1, 2, 3, 4, 5],\n",
        "        total_timesteps=10000\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RePLGJ0_LUiX"
      },
      "source": [
        "### Backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bScF8hSQuVKU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import os\n",
        "from stable_baselines3 import PPO, SAC, DDPG, TD3\n",
        "\n",
        "class CustomPortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gymnasium environment for portfolio management backtesting.\n",
        "\n",
        "    - Observation: 189-dimensional vector from observation/metrics/{yyyy}_{mm}_combined.csv\n",
        "    - Action: 14-dimensional weight allocations for 14 assets (sum to 1)\n",
        "    - Reward: 2 * ROI - 0.7 * volatility - 0.5 * MDD, computed over the next month\n",
        "    \"\"\"\n",
        "    def __init__(self, price_dir=\"metrics_used/price\", metrics_dir=\"observation/metrics\", start_month=None, end_month=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define tickers\n",
        "        self.tickers = [\n",
        "            'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "            '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "        ]\n",
        "\n",
        "        # Load daily price data\n",
        "        price_file = os.path.join(price_dir, \"clean_data.csv\")\n",
        "        if not os.path.exists(price_file):\n",
        "            raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
        "        self.daily_prices = pd.read_csv(price_file, index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "        # Load monthly observations into a dictionary\n",
        "        months = pd.date_range(start=\"2003-01-01\", end=\"2024-12-31\", freq=\"ME\")\n",
        "        self.observations = {}\n",
        "        for month in months:\n",
        "            file_path = os.path.join(metrics_dir, f\"{month.year}_{month.month:02d}_combined.csv\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, header=None)\n",
        "                    if len(df) == 189:\n",
        "                        self.observations[month.strftime(\"%Y-%m\")] = df.iloc[:, 0].values.astype(np.float32)\n",
        "                    else:\n",
        "                        print(f\"Warning: {file_path} has {len(df)} rows, expected 189. Skipping.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "        # Get monthly last trading days\n",
        "        self.monthly_last_days = self.daily_prices.resample(\"ME\").last().index.tolist()\n",
        "        self.month_list = [d.strftime(\"%Y-%m\") for d in self.monthly_last_days]\n",
        "\n",
        "        # Define backtest period\n",
        "        self.backtest_months = pd.date_range(start=\"2003-01-01\", end=\"2024-11-01\", freq=\"ME\").strftime(\"%Y-%m\").tolist()\n",
        "        for month in self.backtest_months:\n",
        "            if month not in self.observations:\n",
        "                raise ValueError(f\"Observation file for {month} not found.\")\n",
        "\n",
        "        self.total_steps = len(self.backtest_months) - 1  # 83 steps for 84 months of performance\n",
        "\n",
        "        # Define spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(189,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        # Initialize state\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state (December 2017 for January 2018 allocations).\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[self.backtest_months[0]]\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take an action (weights) and compute the reward for the next month.\n",
        "        \"\"\"\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode is over\")\n",
        "\n",
        "        # Normalize action\n",
        "        action_sum = np.sum(action)\n",
        "        if action_sum > 0:\n",
        "            weights = action / action_sum\n",
        "        else:\n",
        "            weights = np.ones(14) / 14\n",
        "\n",
        "        # Get allocation month (the month for which these weights apply)\n",
        "        allocation_month = self.backtest_months[self.current_step + 1]\n",
        "\n",
        "        # Get start and end dates for the allocation month\n",
        "        start_date = pd.to_datetime(allocation_month + \"-01\")\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Get daily prices for the period\n",
        "        try:\n",
        "            daily_prices = self.daily_prices.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing price data for period {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute portfolio values\n",
        "        initial_prices = daily_prices.iloc[0].values\n",
        "        daily_portfolio_values = np.sum(daily_prices.values * weights, axis=1)\n",
        "\n",
        "        # Compute ROI\n",
        "        if daily_portfolio_values[0] > 0:\n",
        "            roi = (daily_portfolio_values[-1] / daily_portfolio_values[0]) - 1\n",
        "        else:\n",
        "            roi = 0\n",
        "\n",
        "        # Compute daily returns\n",
        "        daily_returns = daily_portfolio_values[1:] / daily_portfolio_values[:-1] - 1 if len(daily_portfolio_values) > 1 else np.array([0])\n",
        "\n",
        "        # Compute volatility\n",
        "        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Compute MDD\n",
        "        cummax = np.maximum.accumulate(daily_portfolio_values)\n",
        "        drawdown = (cummax - daily_portfolio_values) / cummax if cummax[0] > 0 else np.zeros_like(daily_portfolio_values)\n",
        "        mdd = np.max(drawdown) if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check done\n",
        "        done = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Get next observation\n",
        "        if not done:\n",
        "            obs = self.observations[self.backtest_months[self.current_step]]\n",
        "        else:\n",
        "            obs = np.zeros(189)\n",
        "\n",
        "        # Info\n",
        "        info = {\n",
        "            \"allocation_month\": allocation_month,\n",
        "            \"roi\": roi,\n",
        "            \"volatility\": volatility,\n",
        "            \"mdd\": mdd\n",
        "        }\n",
        "\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def backtest_model(model, env, output_file):\n",
        "    \"\"\"\n",
        "    Backtest a trained RL model over the specified period, recording monthly allocations and performance.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained Stable Baselines 3 model\n",
        "    env: CustomPortfolioEnv instance\n",
        "    output_file (str): Path to save backtest results CSV\n",
        "\n",
        "    Saves:\n",
        "    - CSV with columns: month, weight_{ticker}, roi, volatility, mdd, reward\n",
        "    \"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "    while not done:\n",
        "        # Predict action\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action\n",
        "        if np.sum(action) > 0:\n",
        "            weights = action / np.sum(action)\n",
        "        else:\n",
        "            weights = np.ones(len(env.tickers)) / len(env.tickers)\n",
        "        # Record allocation\n",
        "        record = {\"month\": env.backtest_months[env.current_step + 1]}\n",
        "        for i, ticker in enumerate(env.tickers):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        # Take step\n",
        "        obs, reward, done, truncated, info = env.step(weights)\n",
        "        # Update record\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "    # Save results\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Saved backtest results to {output_file}\")\n",
        "\n",
        "def run_backtests(price_dir=\"metrics_used/price\", metrics_dir=\"observation/metrics\",\n",
        "                 results_dir=\"results\", output_dir=\"backtest_results\", seeds=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Run backtests for all models and seeds over 2018-2024, saving monthly allocations and performance.\n",
        "\n",
        "    Parameters:\n",
        "    price_dir (str): Directory containing clean_data.csv\n",
        "    metrics_dir (str): Directory containing observation vectors\n",
        "    results_dir (str): Directory containing trained models\n",
        "    output_dir (str): Directory to save backtest results\n",
        "    seeds (list): List of random seeds\n",
        "\n",
        "    Saves:\n",
        "    - Backtest results: backtest_results/{model}/seed_{seed}.csv\n",
        "    \"\"\"\n",
        "    # Define model classes\n",
        "    model_classes = {\n",
        "        \"ppo\": PPO,\n",
        "        \"sac\": SAC,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    # Run backtests\n",
        "    for model_type in model_classes:\n",
        "        model_dir = os.path.join(output_dir, model_type)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        for seed in seeds:\n",
        "            model_path = os.path.join(results_dir, f\"{model_type}_seed_{seed}.zip\")\n",
        "            if not os.path.exists(model_path):\n",
        "                print(f\"Model {model_path} not found. Skipping. Please re-run training for this seed.\")\n",
        "                continue\n",
        "            try:\n",
        "                model = model_classes[model_type].load(model_path)\n",
        "                env = CustomPortfolioEnv(\n",
        "                    price_dir=price_dir,\n",
        "                    metrics_dir=metrics_dir\n",
        "                )\n",
        "                output_file = os.path.join(model_dir, f\"seed_{seed}.csv\")\n",
        "                backtest_model(model, env, output_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error backtesting {model_type} seed {seed}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_backtests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IElQYEsMLfg4"
      },
      "source": [
        "## NLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqads75q-eSC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import os\n",
        "from stable_baselines3 import PPO, SAC, DDPG, TD3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import random\n",
        "\n",
        "# Define directories and parameters\n",
        "PRICE_DIR = \"metrics_used/price\"\n",
        "METRICS_DIR = \"metrics_used/sim_nlp\"\n",
        "RESULTS_DIR = \"results_NLP\"\n",
        "BACKTEST_DIR = \"backtest_results_NLP\"\n",
        "SEEDS = [1, 2, 3, 4, 5]\n",
        "TOTAL_TIMESTEPS = 20000\n",
        "TRAIN_START_MONTH = \"2003-01\"\n",
        "TRAIN_END_MONTH = \"2017-11\"\n",
        "BACKTEST_START_MONTH = \"2003-01\"\n",
        "BACKTEST_END_MONTH = \"2024-11\"\n",
        "\n",
        "# Define the list of tickers (14 assets)\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomPortfolioEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for portfolio optimization with NLP vectors.\"\"\"\n",
        "    def __init__(self, price_dir, metrics_dir, obs_months):\n",
        "        super().__init__()\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Load daily price data\n",
        "        price_file = os.path.join(price_dir, \"clean_data.csv\")\n",
        "        if not os.path.exists(price_file):\n",
        "            raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
        "        self.daily_prices = pd.read_csv(price_file, index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "        # Load observation vectors\n",
        "        self.observations = {}\n",
        "        for month in obs_months:\n",
        "            file_path = os.path.join(metrics_dir, f\"{month[:4]}{month[5:]}.csv\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    if df.shape[1] == 28:\n",
        "                        self.observations[month] = df.iloc[0].values.astype(np.float32)\n",
        "                    else:\n",
        "                        print(f\"Warning: {file_path} has {df.shape[1]} columns, expected 28.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"Warning: Observation file for {month} not found.\")\n",
        "\n",
        "        self.obs_months = obs_months\n",
        "        self.total_steps = len(obs_months) - 1  # Steps = number of observations - 1\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(28,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[self.obs_months[0]]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Perform one step: allocate weights and compute reward.\"\"\"\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize weights to sum to 1\n",
        "        action_sum = np.sum(action)\n",
        "        weights = action / action_sum if action_sum > 0 else np.ones(14) / 14\n",
        "\n",
        "        # Determine the allocation month (next month)\n",
        "        allocation_month = self.obs_months[self.current_step + 1]\n",
        "        start_date = pd.to_datetime(allocation_month + \"-01\")\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices for the allocation month\n",
        "        try:\n",
        "            daily_prices = self.daily_prices.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute portfolio performance\n",
        "        initial_prices = daily_prices.iloc[0].values\n",
        "        daily_values = np.sum(daily_prices.values * weights, axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values[-1] / daily_values[0] - 1) if daily_values[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values[1:] / daily_values[:-1] - 1 if len(daily_values) > 1 else np.array([0])\n",
        "        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = np.maximum.accumulate(daily_values)\n",
        "        drawdown = (cummax - daily_values) / cummax if cummax[0] > 0 else np.zeros_like(daily_values)\n",
        "        mdd = np.max(drawdown) if len(daily_values) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.obs_months[self.current_step]] if not done else np.zeros(28)\n",
        "\n",
        "        info = {\"allocation_month\": allocation_month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, done, truncated, info\n",
        "\n",
        "def get_obs_months(start_month, end_month):\n",
        "    \"\"\"Generate a list of month strings between start and end months.\"\"\"\n",
        "    start = pd.to_datetime(start_month + \"-01\")\n",
        "    end = pd.to_datetime(end_month + \"-01\")\n",
        "    return pd.date_range(start=start, end=end, freq=\"ME\").strftime(\"%Y-%m\").tolist()\n",
        "\n",
        "def train_models():\n",
        "    \"\"\"Train RL models for each seed and save them.\"\"\"\n",
        "    obs_months = get_obs_months(TRAIN_START_MONTH, TRAIN_END_MONTH)\n",
        "    env = make_vec_env(lambda: CustomPortfolioEnv(PRICE_DIR, METRICS_DIR, obs_months), n_envs=1)\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        for model_name in [\"ppo\", \"sac\", \"ddpg\", \"td3\"]:\n",
        "            model_class = globals()[model_name.upper()]\n",
        "            model = model_class(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
        "            model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
        "            model_path = os.path.join(RESULTS_DIR, f\"{model_name}_seed_{seed}.zip\")\n",
        "            model.save(model_path)\n",
        "            print(f\"Trained and saved {model_name} with seed {seed} to {model_path}\")\n",
        "\n",
        "def backtest_model(model, env, output_file):\n",
        "    \"\"\"Run backtest for a single model and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        weights = action / np.sum(action) if np.sum(action) > 0 else np.ones(len(TICKERS)) / len(TICKERS)\n",
        "        record = {\"month\": env.obs_months[env.current_step + 1]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\"roi\": info[\"roi\"], \"volatility\": info[\"volatility\"], \"mdd\": info[\"mdd\"], \"reward\": reward})\n",
        "        records.append(record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file}\")\n",
        "\n",
        "def run_backtests():\n",
        "    \"\"\"Backtest all trained models and save results.\"\"\"\n",
        "    obs_months = get_obs_months(BACKTEST_START_MONTH, BACKTEST_END_MONTH)\n",
        "    env = CustomPortfolioEnv(PRICE_DIR, METRICS_DIR, obs_months)\n",
        "\n",
        "    for model_type in [\"ppo\", \"sac\", \"ddpg\", \"td3\"]:\n",
        "        model_dir = os.path.join(BACKTEST_DIR, model_type)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        for seed in SEEDS:\n",
        "            model_path = os.path.join(RESULTS_DIR, f\"{model_type}_seed_{seed}.zip\")\n",
        "            if not os.path.exists(model_path):\n",
        "                print(f\"Model {model_path} not found. Skipping.\")\n",
        "                continue\n",
        "            try:\n",
        "                model = globals()[model_type.upper()].load(model_path)\n",
        "                output_file = os.path.join(model_dir, f\"seed_{seed}.csv\")\n",
        "                backtest_model(model, env, output_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error backtesting {model_type} seed {seed}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting training phase...\")\n",
        "    train_models()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    run_backtests()\n",
        "    print(\"Pipeline completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRuPjs6sLyE7"
      },
      "source": [
        "### Visualizing and evaluating results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWo5Zi8iBGxm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg7PbYYVBK_X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK1wQPkgMA4_"
      },
      "source": [
        "# Pytorch 1: Meta-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIAxFCxPB31n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def create_meta_folders():\n",
        "    \"\"\"\n",
        "    Create a Meta folder with two subfolders: NLP_obs and Metrics_obs.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Creates empty folders\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the folder paths\n",
        "        meta_dir = \"Meta\"\n",
        "        nlp_obs_dir = os.path.join(meta_dir, \"NLP_obs\")\n",
        "        metrics_obs_dir = os.path.join(meta_dir, \"Metrics_obs\")\n",
        "\n",
        "        # Create the Meta folder\n",
        "        os.makedirs(meta_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {meta_dir}\")\n",
        "\n",
        "        # Create the NLP_obs subfolder\n",
        "        os.makedirs(nlp_obs_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {nlp_obs_dir}\")\n",
        "\n",
        "        # Create the Metrics_obs subfolder\n",
        "        os.makedirs(metrics_obs_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {metrics_obs_dir}\")\n",
        "\n",
        "        print(\"Folder structure created successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating folders: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_meta_folders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCTq627yC9Dp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_backtest_results(backtest_dir=\"backtest_results_NLP\", output_dir=\"Meta/NLP_obs\", seeds=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Merge backtest CSV files for each agent in backtest_results_NLP, combining all seeds side by side.\n",
        "\n",
        "    Parameters:\n",
        "    backtest_dir (str): Directory containing backtest results (backtest_results_NLP)\n",
        "    output_dir (str): Directory to save merged CSV files (Meta/NLP_obs)\n",
        "    seeds (list): List of seeds to process\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV files in output_dir as {agent}_merged.csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents (subfolders in backtest_results_NLP)\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Process each agent\n",
        "        for agent in agents:\n",
        "            agent_dir = os.path.join(backtest_dir, agent)\n",
        "            if not os.path.exists(agent_dir):\n",
        "                print(f\"Agent directory {agent_dir} not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize list to hold DataFrames for merging\n",
        "            merged_dfs = []\n",
        "\n",
        "            # Process each seed\n",
        "            for seed in seeds:\n",
        "                csv_file = os.path.join(agent_dir, f\"seed_{seed}.csv\")\n",
        "                if not os.path.exists(csv_file):\n",
        "                    print(f\"CSV file {csv_file} not found. Skipping seed {seed} for agent {agent}.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Read the CSV file\n",
        "                    df = pd.read_csv(csv_file)\n",
        "\n",
        "                    # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                    expected_rows = 261\n",
        "                    if len(df) != expected_rows:\n",
        "                        print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Rename columns to include seed identifier (except for 'month' in the first seed)\n",
        "                    if seed == seeds[0]:\n",
        "                        # For the first seed, keep the 'month' column as is\n",
        "                        renamed_columns = {'month': 'month'}\n",
        "                        for col in df.columns[1:]:  # Skip 'month'\n",
        "                            renamed_columns[col] = f\"{col}_seed_{seed}\"\n",
        "                    else:\n",
        "                        # For other seeds, exclude 'month' and rename all columns\n",
        "                        renamed_columns = {col: f\"{col}_seed_{seed}\" for col in df.columns if col != 'month'}\n",
        "                        df = df.drop(columns=['month'])\n",
        "\n",
        "                    df = df.rename(columns=renamed_columns)\n",
        "                    merged_dfs.append(df)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {csv_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Check if any DataFrames were loaded\n",
        "            if not merged_dfs:\n",
        "                print(f\"No valid CSV files found for agent {agent}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "            merged_df = merged_dfs[0]\n",
        "            for df in merged_dfs[1:]:\n",
        "                merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "            # Verify the number of columns (should be 20 * number of seeds)\n",
        "            expected_columns = 1 + (20 * len(merged_dfs) - (len(merged_dfs) - 1))  # 1 for 'month', 19 additional per seed\n",
        "            if len(merged_df.columns) != expected_columns:\n",
        "                print(f\"Warning: Merged DataFrame for {agent} has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "            # Save the merged DataFrame\n",
        "            output_file = os.path.join(output_dir, f\"{agent}_merged.csv\")\n",
        "            merged_df.to_csv(output_file, index=False)\n",
        "            print(f\"Merged CSV for agent {agent} saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_backtest_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KUp1TmIDap3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_nlp_obs_results(input_dir=\"Meta/NLP_obs\", output_dir=\"Meta/NLP_obs\"):\n",
        "    \"\"\"\n",
        "    Merge the four agent-specific merged CSV files in Meta/NLP_obs into a single CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the agent-specific merged CSV files (Meta/NLP_obs)\n",
        "    output_dir (str): Directory to save the final merged CSV (Meta/NLP_obs)\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV file as nlp_obs_unclean.csv in output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Initialize list to hold DataFrames for merging\n",
        "        merged_dfs = []\n",
        "\n",
        "        # Process each agent's merged CSV\n",
        "        for agent in agents:\n",
        "            csv_file = os.path.join(input_dir, f\"{agent}_merged.csv\")\n",
        "            if not os.path.exists(csv_file):\n",
        "                print(f\"CSV file {csv_file} not found. Skipping agent {agent}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read the CSV file\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Rename columns to include agent identifier (except for 'month' in the first agent)\n",
        "                if agent == agents[0]:\n",
        "                    # For the first agent, keep the 'month' column as is\n",
        "                    renamed_columns = {'month': 'month'}\n",
        "                    for col in df.columns[1:]:  # Skip 'month'\n",
        "                        renamed_columns[col] = f\"{col}_{agent}\"\n",
        "                else:\n",
        "                    # For other agents, exclude 'month' and rename all columns\n",
        "                    renamed_columns = {col: f\"{col}_{agent}\" for col in df.columns if col != 'month'}\n",
        "                    df = df.drop(columns=['month'])\n",
        "\n",
        "                df = df.rename(columns=renamed_columns)\n",
        "                merged_dfs.append(df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {csv_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Check if any DataFrames were loaded\n",
        "        if not merged_dfs:\n",
        "            print(\"No valid CSV files found to merge.\")\n",
        "            return\n",
        "\n",
        "        # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "        merged_df = merged_dfs[0]\n",
        "        for df in merged_dfs[1:]:\n",
        "            merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (100 columns per agent × 4 agents))\n",
        "        expected_columns = 1 + (100 * len(merged_dfs))  # 1 for 'month', 100 columns per agent\n",
        "        if len(merged_df.columns) != expected_columns:\n",
        "            print(f\"Warning: Merged DataFrame has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Save the final merged DataFrame\n",
        "        output_file = os.path.join(output_dir, \"nlp_obs_unclean.csv\")\n",
        "        merged_df.to_csv(output_file, index=False)\n",
        "        print(f\"Final merged CSV saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_nlp_obs_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCYb2YVADvi8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def clean_nlp_obs_results(input_file=\"Meta/NLP_obs/nlp_obs_unclean.csv\", output_file=\"Meta/NLP_obs/nlp_obs_clean.csv\"):\n",
        "    \"\"\"\n",
        "    Clean the merged NLP observation CSV by keeping only the 'month' and weight columns.\n",
        "\n",
        "    Parameters:\n",
        "    input_file (str): Path to the input merged CSV file (nlp_obs_unclean.csv)\n",
        "    output_file (str): Path to save the cleaned CSV file (nlp_obs_clean.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Cleaned CSV file with only 'month' and weight columns in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the input CSV\n",
        "        if not os.path.exists(input_file):\n",
        "            raise FileNotFoundError(f\"Input file {input_file} not found.\")\n",
        "\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "        expected_rows = 263\n",
        "        if len(df) != expected_rows:\n",
        "            print(f\"Warning: {input_file} has {len(df)} rows, expected {expected_rows}.\")\n",
        "\n",
        "        # Expected number of columns (401: 1 month + 100 per agent × 4 agents)\n",
        "        expected_columns = 401\n",
        "        if len(df.columns) != expected_columns:\n",
        "            print(f\"Warning: {input_file} has {len(df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Identify columns to keep: 'month' and all columns containing 'weight'\n",
        "        columns_to_keep = ['month']\n",
        "        for col in df.columns:\n",
        "            if 'weight' in col:\n",
        "                columns_to_keep.append(col)\n",
        "\n",
        "        # Create the cleaned DataFrame\n",
        "        cleaned_df = df[columns_to_keep]\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (14 weights × 5 seeds × 4 agents) = 281)\n",
        "        expected_cleaned_columns = 1 + (14 * 5 * 4)  # 1 month + 14 weights × 5 seeds × 4 agents\n",
        "        if len(cleaned_df.columns) != expected_cleaned_columns:\n",
        "            print(f\"Warning: Cleaned DataFrame has {len(cleaned_df.columns)} columns, expected {expected_cleaned_columns}.\")\n",
        "\n",
        "        # Save the cleaned DataFrame\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        cleaned_df.to_csv(output_file, index=False)\n",
        "        print(f\"Cleaned CSV saved to {output_file} with {len(cleaned_df)} rows and {len(cleaned_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Cleaning process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cleaning process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_nlp_obs_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALV2tMNgEz88"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_backtest_results(backtest_dir=\"backtest_results\", output_dir=\"Meta/Metrics_obs\", seeds=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Merge backtest CSV files for each agent in backtest_results_NLP, combining all seeds side by side.\n",
        "\n",
        "    Parameters:\n",
        "    backtest_dir (str): Directory containing backtest results (backtest_results_NLP)\n",
        "    output_dir (str): Directory to save merged CSV files (Meta/NLP_obs)\n",
        "    seeds (list): List of seeds to process\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV files in output_dir as {agent}_merged.csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents (subfolders in backtest_results_NLP)\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Process each agent\n",
        "        for agent in agents:\n",
        "            agent_dir = os.path.join(backtest_dir, agent)\n",
        "            if not os.path.exists(agent_dir):\n",
        "                print(f\"Agent directory {agent_dir} not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize list to hold DataFrames for merging\n",
        "            merged_dfs = []\n",
        "\n",
        "            # Process each seed\n",
        "            for seed in seeds:\n",
        "                csv_file = os.path.join(agent_dir, f\"seed_{seed}.csv\")\n",
        "                if not os.path.exists(csv_file):\n",
        "                    print(f\"CSV file {csv_file} not found. Skipping seed {seed} for agent {agent}.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Read the CSV file\n",
        "                    df = pd.read_csv(csv_file)\n",
        "\n",
        "                    # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                    expected_rows = 261\n",
        "                    if len(df) != expected_rows:\n",
        "                        print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Rename columns to include seed identifier (except for 'month' in the first seed)\n",
        "                    if seed == seeds[0]:\n",
        "                        # For the first seed, keep the 'month' column as is\n",
        "                        renamed_columns = {'month': 'month'}\n",
        "                        for col in df.columns[1:]:  # Skip 'month'\n",
        "                            renamed_columns[col] = f\"{col}_seed_{seed}\"\n",
        "                    else:\n",
        "                        # For other seeds, exclude 'month' and rename all columns\n",
        "                        renamed_columns = {col: f\"{col}_seed_{seed}\" for col in df.columns if col != 'month'}\n",
        "                        df = df.drop(columns=['month'])\n",
        "\n",
        "                    df = df.rename(columns=renamed_columns)\n",
        "                    merged_dfs.append(df)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {csv_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Check if any DataFrames were loaded\n",
        "            if not merged_dfs:\n",
        "                print(f\"No valid CSV files found for agent {agent}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "            merged_df = merged_dfs[0]\n",
        "            for df in merged_dfs[1:]:\n",
        "                merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "            # Verify the number of columns (should be 20 * number of seeds)\n",
        "            expected_columns = 1 + (20 * len(merged_dfs) - (len(merged_dfs) - 1))  # 1 for 'month', 19 additional per seed\n",
        "            if len(merged_df.columns) != expected_columns:\n",
        "                print(f\"Warning: Merged DataFrame for {agent} has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "            # Save the merged DataFrame\n",
        "            output_file = os.path.join(output_dir, f\"{agent}_merged.csv\")\n",
        "            merged_df.to_csv(output_file, index=False)\n",
        "            print(f\"Merged CSV for agent {agent} saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_backtest_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kZlYMoSFr5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_nlp_obs_results(input_dir=\"Meta/Metrics_obs\", output_dir=\"Meta/Metrics_obs\"):\n",
        "    \"\"\"\n",
        "    Merge the four agent-specific merged CSV files in Meta/NLP_obs into a single CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the agent-specific merged CSV files (Meta/NLP_obs)\n",
        "    output_dir (str): Directory to save the final merged CSV (Meta/NLP_obs)\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV file as nlp_obs_unclean.csv in output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Initialize list to hold DataFrames for merging\n",
        "        merged_dfs = []\n",
        "\n",
        "        # Process each agent's merged CSV\n",
        "        for agent in agents:\n",
        "            csv_file = os.path.join(input_dir, f\"{agent}_merged.csv\")\n",
        "            if not os.path.exists(csv_file):\n",
        "                print(f\"CSV file {csv_file} not found. Skipping agent {agent}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read the CSV file\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Rename columns to include agent identifier (except for 'month' in the first agent)\n",
        "                if agent == agents[0]:\n",
        "                    # For the first agent, keep the 'month' column as is\n",
        "                    renamed_columns = {'month': 'month'}\n",
        "                    for col in df.columns[1:]:  # Skip 'month'\n",
        "                        renamed_columns[col] = f\"{col}_{agent}\"\n",
        "                else:\n",
        "                    # For other agents, exclude 'month' and rename all columns\n",
        "                    renamed_columns = {col: f\"{col}_{agent}\" for col in df.columns if col != 'month'}\n",
        "                    df = df.drop(columns=['month'])\n",
        "\n",
        "                df = df.rename(columns=renamed_columns)\n",
        "                merged_dfs.append(df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {csv_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Check if any DataFrames were loaded\n",
        "        if not merged_dfs:\n",
        "            print(\"No valid CSV files found to merge.\")\n",
        "            return\n",
        "\n",
        "        # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "        merged_df = merged_dfs[0]\n",
        "        for df in merged_dfs[1:]:\n",
        "            merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (100 columns per agent × 4 agents))\n",
        "        expected_columns = 1 + (100 * len(merged_dfs))  # 1 for 'month', 100 columns per agent\n",
        "        if len(merged_df.columns) != expected_columns:\n",
        "            print(f\"Warning: Merged DataFrame has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Save the final merged DataFrame\n",
        "        output_file = os.path.join(output_dir, \"nlp_obs_unclean.csv\")\n",
        "        merged_df.to_csv(output_file, index=False)\n",
        "        print(f\"Final merged CSV saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_nlp_obs_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGLpUayMF0C4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def clean_nlp_obs_results(input_file=\"Meta/Metrics_obs/nlp_obs_unclean.csv\", output_file=\"Meta/Metrics_obs/metrics_obs_clean.csv\"):\n",
        "    \"\"\"\n",
        "    Clean the merged NLP observation CSV by keeping only the 'month' and weight columns.\n",
        "\n",
        "    Parameters:\n",
        "    input_file (str): Path to the input merged CSV file (nlp_obs_unclean.csv)\n",
        "    output_file (str): Path to save the cleaned CSV file (nlp_obs_clean.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Cleaned CSV file with only 'month' and weight columns in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the input CSV\n",
        "        if not os.path.exists(input_file):\n",
        "            raise FileNotFoundError(f\"Input file {input_file} not found.\")\n",
        "\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "        expected_rows = 263\n",
        "        if len(df) != expected_rows:\n",
        "            print(f\"Warning: {input_file} has {len(df)} rows, expected {expected_rows}.\")\n",
        "\n",
        "        # Expected number of columns (401: 1 month + 100 per agent × 4 agents)\n",
        "        expected_columns = 401\n",
        "        if len(df.columns) != expected_columns:\n",
        "            print(f\"Warning: {input_file} has {len(df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Identify columns to keep: 'month' and all columns containing 'weight'\n",
        "        columns_to_keep = ['month']\n",
        "        for col in df.columns:\n",
        "            if 'weight' in col:\n",
        "                columns_to_keep.append(col)\n",
        "\n",
        "        # Create the cleaned DataFrame\n",
        "        cleaned_df = df[columns_to_keep]\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (14 weights × 5 seeds × 4 agents) = 281)\n",
        "        expected_cleaned_columns = 1 + (14 * 5 * 4)  # 1 month + 14 weights × 5 seeds × 4 agents\n",
        "        if len(cleaned_df.columns) != expected_cleaned_columns:\n",
        "            print(f\"Warning: Cleaned DataFrame has {len(cleaned_df.columns)} columns, expected {expected_cleaned_columns}.\")\n",
        "\n",
        "        # Save the cleaned DataFrame\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        cleaned_df.to_csv(output_file, index=False)\n",
        "        print(f\"Cleaned CSV saved to {output_file} with {len(cleaned_df)} rows and {len(cleaned_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Cleaning process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cleaning process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_nlp_obs_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0lkIC6aMZZl"
      },
      "source": [
        "### Defining neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXEAgSOSGXYr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import os\n",
        "\n",
        "# Define tickers\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomMetaEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for meta-agent portfolio optimization.\"\"\"\n",
        "    def __init__(self, csv_path=\"Meta/Metrics_obs/metrics_obs_clean.csv\", price_path=\"metrics_used/price/clean_data.csv\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load observation data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.months = df['month'].tolist()\n",
        "        self.observations = df.iloc[:, 1:].astype(np.float32).values  # Shape: (263, 280)\n",
        "\n",
        "        # Load price data\n",
        "        self.price_data = pd.read_csv(price_path, index_col=\"Date\", parse_dates=True)\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Validate tickers\n",
        "        for ticker in self.tickers:\n",
        "            if ticker not in self.price_data.columns:\n",
        "                raise ValueError(f\"Ticker {ticker} not in price data\")\n",
        "\n",
        "        self.total_steps = len(self.months)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(280,), dtype=np.float32)\n",
        "        # Define action space with finite bounds\n",
        "        self.action_space = spaces.Box(low=-10, high=10, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "\n",
        "        # Get current month\n",
        "        month = self.months[self.current_step]\n",
        "\n",
        "        # Determine start and end dates\n",
        "        start_date = pd.to_datetime(month + '-01')\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices\n",
        "        try:\n",
        "            daily_prices = self.price_data.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute daily portfolio values\n",
        "        daily_values = (daily_prices * weights).sum(axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values.iloc[-1] / daily_values.iloc[0] - 1) if daily_values.iloc[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values.pct_change().dropna()\n",
        "        volatility = daily_returns.std() if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = daily_values.cummax()\n",
        "        drawdown = (cummax - daily_values) / cummax\n",
        "        mdd = drawdown.max() if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.current_step] if not terminated else np.zeros(280)\n",
        "\n",
        "        info = {\"allocation_month\": month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_meta_agent():\n",
        "    \"\"\"Train the meta-agent and save the model.\"\"\"\n",
        "    env = make_vec_env(lambda: CustomMetaEnv(), n_envs=1, seed=1)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={'net_arch': [256, 256, 256]},\n",
        "        verbose=1,\n",
        "        seed=1\n",
        "    )\n",
        "    model.learn(total_timesteps=20000)\n",
        "    os.makedirs(\"meta_results\", exist_ok=True)\n",
        "    model_path = \"meta_results/meta_agent_seed1\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Meta-agent trained and saved to {model_path}.zip\")\n",
        "    return model\n",
        "\n",
        "def backtest_meta_agent(model, env, output_file=\"Meta/Metrics_obs/backtest_meta_seed_1.csv\"):\n",
        "    \"\"\"Backtest the trained meta-agent and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "        record = {\"month\": env.months[env.current_step]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "\n",
        "    # Save backtest results\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file} with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the full pipeline: train and backtest the meta-agent.\"\"\"\n",
        "    print(\"Starting training phase...\")\n",
        "    model = train_meta_agent()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    env = CustomMetaEnv()\n",
        "    backtest_meta_agent(model, env)\n",
        "    print(\"Pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgteTGYVlNwn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import os\n",
        "\n",
        "# Define tickers\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomMetaEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for meta-agent portfolio optimization.\"\"\"\n",
        "    def __init__(self, csv_path=\"Meta/NLP_obs/nlp_obs_clean.csv\", price_path=\"metrics_used/price/clean_data.csv\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load observation data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.months = df['month'].tolist()\n",
        "        self.observations = df.iloc[:, 1:].astype(np.float32).values  # Shape: (263, 280)\n",
        "\n",
        "        # Load price data\n",
        "        self.price_data = pd.read_csv(price_path, index_col=\"Date\", parse_dates=True)\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Validate tickers\n",
        "        for ticker in self.tickers:\n",
        "            if ticker not in self.price_data.columns:\n",
        "                raise ValueError(f\"Ticker {ticker} not in price data\")\n",
        "\n",
        "        self.total_steps = len(self.months)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(280,), dtype=np.float32)\n",
        "        # Define action space with finite bounds\n",
        "        self.action_space = spaces.Box(low=-10, high=10, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "\n",
        "        # Get current month\n",
        "        month = self.months[self.current_step]\n",
        "\n",
        "        # Determine start and end dates\n",
        "        start_date = pd.to_datetime(month + '-01')\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices\n",
        "        try:\n",
        "            daily_prices = self.price_data.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute daily portfolio values\n",
        "        daily_values = (daily_prices * weights).sum(axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values.iloc[-1] / daily_values.iloc[0] - 1) if daily_values.iloc[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values.pct_change().dropna()\n",
        "        volatility = daily_returns.std() if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = daily_values.cummax()\n",
        "        drawdown = (cummax - daily_values) / cummax\n",
        "        mdd = drawdown.max() if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.current_step] if not terminated else np.zeros(280)\n",
        "\n",
        "        info = {\"allocation_month\": month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_meta_agent():\n",
        "    \"\"\"Train the meta-agent and save the model.\"\"\"\n",
        "    env = make_vec_env(lambda: CustomMetaEnv(), n_envs=1, seed=1)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={'net_arch': [256, 256, 256]},\n",
        "        verbose=1,\n",
        "        seed=1\n",
        "    )\n",
        "    model.learn(total_timesteps=20000)\n",
        "    os.makedirs(\"meta_results\", exist_ok=True)\n",
        "    model_path = \"meta_results/meta_agent_seed1_nlp\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Meta-agent trained and saved to {model_path}.zip\")\n",
        "    return model\n",
        "\n",
        "def backtest_meta_agent(model, env, output_file=\"Meta/NLP_obs/backtest_meta_seed_1.csv\"):\n",
        "    \"\"\"Backtest the trained meta-agent and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "        record = {\"month\": env.months[env.current_step]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "\n",
        "    # Save backtest results\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file} with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the full pipeline: train and backtest the meta-agent.\"\"\"\n",
        "    print(\"Starting training phase...\")\n",
        "    model = train_meta_agent()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    env = CustomMetaEnv()\n",
        "    backtest_meta_agent(model, env)\n",
        "    print(\"Pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvwMD8OM0TJ"
      },
      "source": [
        "# Super agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkvTpK9_l5jQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def create_super_folder():\n",
        "    \"\"\"\n",
        "    Create an empty SUPER folder.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Creates an empty SUPER folder\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the folder path\n",
        "        super_dir = \"SUPER\"\n",
        "\n",
        "        # Create the SUPER folder\n",
        "        os.makedirs(super_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {super_dir}\")\n",
        "\n",
        "        print(\"SUPER folder created successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating SUPER folder: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_super_folder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK9VhvKjmhmm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def copy_backtests_to_super():\n",
        "    \"\"\"\n",
        "    Copy backtest files from Meta/NLP_obs and Meta/Metrics_obs to the SUPER folder.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Copies files to the SUPER folder\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define source and destination directories\n",
        "        nlp_source_dir = \"Meta/NLP_obs\"\n",
        "        metrics_source_dir = \"Meta/Metrics_obs\"\n",
        "        dest_dir = \"SUPER\"\n",
        "\n",
        "        # Validate source directories\n",
        "        if not os.path.exists(nlp_source_dir):\n",
        "            print(f\"Source directory {nlp_source_dir} does not exist.\")\n",
        "            return\n",
        "        if not os.path.exists(metrics_source_dir):\n",
        "            print(f\"Source directory {metrics_source_dir} does not exist.\")\n",
        "            return\n",
        "\n",
        "        # Validate destination directory\n",
        "        if not os.path.exists(dest_dir):\n",
        "            print(f\"Destination directory {dest_dir} does not exist. Please create it first.\")\n",
        "            return\n",
        "\n",
        "        # Copy files from NLP_obs\n",
        "        for filename in os.listdir(nlp_source_dir):\n",
        "            if filename.endswith(\".csv\"):\n",
        "                src_path = os.path.join(nlp_source_dir, filename)\n",
        "                dest_filename = f\"nlp_{filename}\"\n",
        "                dest_path = os.path.join(dest_dir, dest_filename)\n",
        "                try:\n",
        "                    shutil.copy2(src_path, dest_path)\n",
        "                    print(f\"Copied {src_path} to {dest_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "        # Copy files from Metrics_obs\n",
        "        for filename in os.listdir(metrics_source_dir):\n",
        "            if filename.endswith(\".csv\") and \"backtest_meta_seed_1\" in filename:\n",
        "                src_path = os.path.join(metrics_source_dir, filename)\n",
        "                dest_filename = f\"metrics_{filename}\"\n",
        "                dest_path = os.path.join(dest_dir, dest_filename)\n",
        "                try:\n",
        "                    shutil.copy2(src_path, dest_path)\n",
        "                    print(f\"Copied {src_path} to {dest_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "        print(\"Copy process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during copy process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    copy_backtests_to_super()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0brWG-enE04"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "files_to_delete = [\"SUPER/nlp_ddpg_merged.csv\", \"SUPER/nlp_nlp_obs_clean.csv\", \"SUPER/nlp_nlp_obs_unclean.csv\", \"SUPER/nlp_ppo_merged.csv\", \"SUPER/nlp_td3_merged.csv\", \"SUPER/nlp_sac_merged.csv\"]\n",
        "\n",
        "for file in files_to_delete:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "        print(f\"Deleted: {file}\")\n",
        "    else:\n",
        "        print(f\"Not found: {file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ8SmpmhoMMQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def concatenate_super_backtests(super_dir=\"SUPER\", output_file=\"SUPER/super_weights_clean.csv\"):\n",
        "    \"\"\"\n",
        "    Concatenate the backtest CSV files in the SUPER folder, keeping only the 'month' and weight columns.\n",
        "\n",
        "    Parameters:\n",
        "    super_dir (str): Directory containing the backtest CSV files (SUPER)\n",
        "    output_file (str): Path to save the concatenated and cleaned CSV file (SUPER/super_weights_clean.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Concatenated CSV file with only 'month' and weight columns in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the input files\n",
        "        nlp_file = os.path.join(super_dir, \"nlp_backtest_meta_seed_1.csv\")\n",
        "        metrics_file = os.path.join(super_dir, \"metrics_backtest_meta_seed_1.csv\")\n",
        "\n",
        "        # Validate input files\n",
        "        if not os.path.exists(nlp_file):\n",
        "            raise FileNotFoundError(f\"NLP backtest file {nlp_file} not found.\")\n",
        "        if not os.path.exists(metrics_file):\n",
        "            raise FileNotFoundError(f\"Metrics backtest file {metrics_file} not found.\")\n",
        "\n",
        "        # Read the CSV files\n",
        "        nlp_df = pd.read_csv(nlp_file)\n",
        "        metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "        # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "        expected_rows = 263\n",
        "        if len(nlp_df) != expected_rows or len(metrics_df) != expected_rows:\n",
        "            print(f\"Warning: Expected {expected_rows} rows. NLP has {len(nlp_df)}, Metrics has {len(metrics_df)}.\")\n",
        "\n",
        "        # Keep only 'month' and weight columns\n",
        "        weight_cols = [col for col in nlp_df.columns if col.startswith(\"weight_\")]\n",
        "        nlp_df = nlp_df[['month'] + weight_cols]\n",
        "        metrics_df = metrics_df[['month'] + weight_cols]\n",
        "\n",
        "        # Rename weight columns to include source identifier\n",
        "        nlp_df = nlp_df.rename(columns={col: f\"{col}_nlp\" for col in weight_cols})\n",
        "        metrics_df = metrics_df.rename(columns={col: f\"{col}_metrics\" for col in weight_cols})\n",
        "\n",
        "        # Drop 'month' from metrics_df to avoid duplication\n",
        "        metrics_df = metrics_df.drop(columns=['month'])\n",
        "\n",
        "        # Concatenate side by side on 'month'\n",
        "        concatenated_df = pd.concat([nlp_df, metrics_df], axis=1)\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (14 weights × 2 sources) = 29)\n",
        "        expected_columns = 1 + (14 * 2)\n",
        "        if len(concatenated_df.columns) != expected_columns:\n",
        "            print(f\"Warning: Concatenated DataFrame has {len(concatenated_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Save the concatenated DataFrame\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        concatenated_df.to_csv(output_file, index=False)\n",
        "        print(f\"Concatenated and cleaned CSV saved to {output_file} with {len(concatenated_df)} rows and {len(concatenated_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Concatenation process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during concatenation process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    concatenate_super_backtests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUExmrmLo5uc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import os\n",
        "\n",
        "# Define tickers\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomSuperMetaEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for super meta-agent portfolio optimization.\"\"\"\n",
        "    def __init__(self, csv_path=\"SUPER/super_weights_clean.csv\", price_path=\"metrics_used/price/clean_data.csv\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load observation data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.months = df['month'].tolist()\n",
        "        self.observations = df.iloc[:, 1:].astype(np.float32).values  # Shape: (263, 28)\n",
        "\n",
        "        # Load price data\n",
        "        self.price_data = pd.read_csv(price_path, index_col=\"Date\", parse_dates=True)\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Validate tickers\n",
        "        for ticker in self.tickers:\n",
        "            if ticker not in self.price_data.columns:\n",
        "                raise ValueError(f\"Ticker {ticker} not in price data\")\n",
        "\n",
        "        self.total_steps = len(self.months)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(28,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=-10, high=10, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "\n",
        "        # Get current month\n",
        "        month = self.months[self.current_step]\n",
        "\n",
        "        # Determine start and end dates\n",
        "        start_date = pd.to_datetime(month + '-01')\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices\n",
        "        try:\n",
        "            daily_prices = self.price_data.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute daily portfolio values\n",
        "        daily_values = (daily_prices * weights).sum(axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values.iloc[-1] / daily_values.iloc[0] - 1) if daily_values.iloc[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values.pct_change().dropna()\n",
        "        volatility = daily_returns.std() if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = daily_values.cummax()\n",
        "        drawdown = (cummax - daily_values) / cummax\n",
        "        mdd = drawdown.max() if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.current_step] if not terminated else np.zeros(28)\n",
        "\n",
        "        info = {\"allocation_month\": month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_super_meta_agent():\n",
        "    \"\"\"Train the super meta-agent and save the model.\"\"\"\n",
        "    env = make_vec_env(lambda: CustomSuperMetaEnv(), n_envs=1, seed=1)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={'net_arch': [256, 256, 256]},\n",
        "        verbose=1,\n",
        "        seed=1\n",
        "    )\n",
        "    model.learn(total_timesteps=1000)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    model_path = \"results/super_meta_agent_seed1\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Super meta-agent trained and saved to {model_path}.zip\")\n",
        "    return model\n",
        "\n",
        "def backtest_super_meta_agent(model, env, output_file=\"SUPER/backtest_super_meta_seed_1.csv\"):\n",
        "    \"\"\"Backtest the trained super meta-agent and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "        record = {\"month\": env.months[env.current_step]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "\n",
        "    # Save backtest results\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file} with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the full pipeline: train and backtest the super meta-agent.\"\"\"\n",
        "    print(\"Starting training phase...\")\n",
        "    model = train_super_meta_agent()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    env = CustomSuperMetaEnv()\n",
        "    backtest_super_meta_agent(model, env)\n",
        "    print(\"Pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3cJjm2wpm2x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def organize_all_backtests():\n",
        "    \"\"\"\n",
        "    Create the ALL_BACKTEST folder with subfolders Meta, Super, Metrics, NLP,\n",
        "    and copy all backtest CSV files into the appropriate subfolders with consistent structure.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Creates the folder structure and copies files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the main directory and subfolders\n",
        "        all_backtest_dir = \"ALL_BACKTEST\"\n",
        "        subfolders = [\"Meta\", \"Super\", \"Metrics\", \"NLP\"]\n",
        "\n",
        "        # Create the main directory and subfolders\n",
        "        os.makedirs(all_backtest_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {all_backtest_dir}\")\n",
        "\n",
        "        for subfolder in subfolders:\n",
        "            subfolder_path = os.path.join(all_backtest_dir, subfolder)\n",
        "            os.makedirs(subfolder_path, exist_ok=True)\n",
        "            print(f\"Created/Verified folder: {subfolder_path}\")\n",
        "\n",
        "        # Define agents and seeds for Metrics and NLP pipelines\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "        seeds = [1, 2, 3, 4, 5]\n",
        "\n",
        "        # Copy Metrics pipeline files\n",
        "        metrics_source_dir = \"backtest_results\"\n",
        "        if not os.path.exists(metrics_source_dir):\n",
        "            print(f\"Metrics source directory {metrics_source_dir} does not exist.\")\n",
        "        else:\n",
        "            for agent in agents:\n",
        "                agent_source_dir = os.path.join(metrics_source_dir, agent)\n",
        "                agent_dest_dir = os.path.join(all_backtest_dir, \"Metrics\", agent)\n",
        "                os.makedirs(agent_dest_dir, exist_ok=True)\n",
        "                for seed in seeds:\n",
        "                    src_file = os.path.join(agent_source_dir, f\"seed_{seed}.csv\")\n",
        "                    if os.path.exists(src_file):\n",
        "                        dest_file = os.path.join(agent_dest_dir, f\"seed_{seed}.csv\")\n",
        "                        shutil.copy2(src_file, dest_file)\n",
        "                        print(f\"Copied {src_file} to {dest_file}\")\n",
        "                    else:\n",
        "                        print(f\"File {src_file} does not exist. Skipping.\")\n",
        "\n",
        "        # Copy NLP pipeline files\n",
        "        nlp_source_dir = \"backtest_results_NLP\"\n",
        "        if not os.path.exists(nlp_source_dir):\n",
        "            print(f\"NLP source directory {nlp_source_dir} does not exist.\")\n",
        "        else:\n",
        "            for agent in agents:\n",
        "                agent_source_dir = os.path.join(nlp_source_dir, agent)\n",
        "                agent_dest_dir = os.path.join(all_backtest_dir, \"NLP\", agent)\n",
        "                os.makedirs(agent_dest_dir, exist_ok=True)\n",
        "                for seed in seeds:\n",
        "                    src_file = os.path.join(agent_source_dir, f\"seed_{seed}.csv\")\n",
        "                    if os.path.exists(src_file):\n",
        "                        dest_file = os.path.join(agent_dest_dir, f\"seed_{seed}.csv\")\n",
        "                        shutil.copy2(src_file, dest_file)\n",
        "                        print(f\"Copied {src_file} to {dest_file}\")\n",
        "                    else:\n",
        "                        print(f\"File {src_file} does not exist. Skipping.\")\n",
        "\n",
        "        # Copy Meta pipeline file\n",
        "        meta_source_dir = \"Meta/Metrics_obs\"\n",
        "        meta_file = os.path.join(meta_source_dir, \"backtest_meta_seed_1.csv\")\n",
        "        meta_dest_dir = os.path.join(all_backtest_dir, \"Meta\", \"meta\")\n",
        "        os.makedirs(meta_dest_dir, exist_ok=True)\n",
        "        if os.path.exists(meta_file):\n",
        "            dest_file = os.path.join(meta_dest_dir, \"seed_1.csv\")\n",
        "            shutil.copy2(meta_file, dest_file)\n",
        "            print(f\"Copied {meta_file} to {dest_file}\")\n",
        "        else:\n",
        "            print(f\"Meta backtest file {meta_file} does not exist. Skipping.\")\n",
        "\n",
        "                # Copy Meta pipeline file\n",
        "        meta_source_dir = \"Meta/NLP_obs\"\n",
        "        meta_file = os.path.join(meta_source_dir, \"backtest_meta_seed_1.csv\")\n",
        "        meta_dest_dir = os.path.join(all_backtest_dir, \"Meta\", \"meta\")\n",
        "        os.makedirs(meta_dest_dir, exist_ok=True)\n",
        "        if os.path.exists(meta_file):\n",
        "            dest_file = os.path.join(meta_dest_dir, \"seed_1_nlp.csv\")\n",
        "            shutil.copy2(meta_file, dest_file)\n",
        "            print(f\"Copied {meta_file} to {dest_file}\")\n",
        "        else:\n",
        "            print(f\"Meta backtest file {meta_file} does not exist. Skipping.\")\n",
        "\n",
        "        # Copy Super pipeline file\n",
        "        super_source_dir = \"SUPER\"\n",
        "        super_file = os.path.join(super_source_dir, \"backtest_super_meta_seed_1.csv\")\n",
        "        super_dest_dir = os.path.join(all_backtest_dir, \"Super\", \"super_meta\")\n",
        "        os.makedirs(super_dest_dir, exist_ok=True)\n",
        "        if os.path.exists(super_file):\n",
        "            dest_file = os.path.join(super_dest_dir, \"seed_1.csv\")\n",
        "            shutil.copy2(super_file, dest_file)\n",
        "            print(f\"Copied {super_file} to {dest_file}\")\n",
        "        else:\n",
        "            print(f\"Super backtest file {super_file} does not exist. Skipping.\")\n",
        "\n",
        "        print(\"Backtest organization process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during backtest organization process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    organize_all_backtests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7K8bO6G2V3r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def create_roi_matrix(all_backtest_dir=\"ALL_BACKTEST\", output_file=\"ALL_BACKTEST/ROI.csv\"):\n",
        "    \"\"\"\n",
        "    Create a matrix with rows as months and columns as models and seeds,\n",
        "    where each cell contains the ROI of the model at that month.\n",
        "\n",
        "    Parameters:\n",
        "    all_backtest_dir (str): Directory containing the backtest files (ALL_BACKTEST)\n",
        "    output_file (str): Path to save the ROI matrix (SUPER/ROI.csv)\n",
        "\n",
        "    Saves:\n",
        "    - ROI matrix as a CSV file in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define all files and their identifiers\n",
        "        all_files = [\n",
        "            # Metrics\n",
        "            (f\"{all_backtest_dir}/Metrics/{agent}/seed_{seed}.csv\", f\"Metrics_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1,2,3,4,5]\n",
        "        ] + [\n",
        "            # NLP\n",
        "            (f\"{all_backtest_dir}/NLP/{agent}/seed_{seed}.csv\", f\"NLP_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1,2,3,4,5]\n",
        "        ] + [\n",
        "            # Meta Metrics\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1.csv\", \"Meta_meta_metrics_1\"),\n",
        "            # Meta NLP\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1_nlp.csv\", \"Meta_meta_nlp_1\"),\n",
        "            # Super\n",
        "            (f\"{all_backtest_dir}/Super/super_meta/seed_1.csv\", \"Super_super_meta_1\")\n",
        "        ]\n",
        "\n",
        "        # Initialize list for DataFrames\n",
        "        dfs = []\n",
        "\n",
        "        # Read each file and add model_seed column\n",
        "        for file_path, identifier in all_files:\n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                # Validate the number of rows\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {file_path} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "                # Validate the presence of required columns\n",
        "                if 'month' not in df.columns or 'roi' not in df.columns:\n",
        "                    print(f\"Warning: 'month' or 'roi' column not found in {file_path}. Skipping.\")\n",
        "                    continue\n",
        "                df['model_seed'] = identifier\n",
        "                df = df[['month', 'roi', 'model_seed']]\n",
        "                dfs.append(df)\n",
        "            else:\n",
        "                print(f\"File {file_path} does not exist. Skipping.\")\n",
        "\n",
        "        # Concatenate all DataFrames\n",
        "        if dfs:\n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        else:\n",
        "            print(\"No dataframes to concatenate. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Pivot the combined DataFrame\n",
        "        roi_matrix = combined_df.pivot(index='month', columns='model_seed', values='roi')\n",
        "\n",
        "        # Sort the index (months)\n",
        "        roi_matrix = roi_matrix.sort_index()\n",
        "\n",
        "        # Save to CSV\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        roi_matrix.to_csv(output_file)\n",
        "        print(f\"ROI matrix saved to {output_file} with {len(roi_matrix)} rows and {len(roi_matrix.columns)} columns.\")\n",
        "\n",
        "        print(\"ROI matrix creation completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ROI matrix creation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_roi_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9wIXBle7ERW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_portfolio_value_matrix(all_backtest_dir=\"ALL_BACKTEST\", output_file=\"ALL_BACKTEST/portfolio_value.csv\"):\n",
        "    \"\"\"\n",
        "    Create a matrix with rows as months and columns as models and seeds,\n",
        "    where each cell contains the cumulative portfolio value of the model at that month.\n",
        "\n",
        "    Parameters:\n",
        "    all_backtest_dir (str): Directory containing the backtest files (ALL_BACKTEST)\n",
        "    output_file (str): Path to save the portfolio value matrix (SUPER/portfolio_value.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Portfolio value matrix as a CSV file in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define all files and their identifiers\n",
        "        all_files = [\n",
        "            # Metrics\n",
        "            (f\"{all_backtest_dir}/Metrics/{agent}/seed_{seed}.csv\", f\"Metrics_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1, 2, 3, 4, 5]\n",
        "        ] + [\n",
        "            # NLP\n",
        "            (f\"{all_backtest_dir}/NLP/{agent}/seed_{seed}.csv\", f\"NLP_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1, 2, 3, 4, 5]\n",
        "        ] + [\n",
        "            # Meta Metrics\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1.csv\", \"Meta_meta_metrics_1\"),\n",
        "            # Meta NLP\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1_nlp.csv\", \"Meta_meta_nlp_1\"),\n",
        "            # Super\n",
        "            (f\"{all_backtest_dir}/Super/super_meta/seed_1.csv\", \"Super_super_meta_1\")\n",
        "        ]\n",
        "\n",
        "        # Initialize list for DataFrames\n",
        "        dfs = []\n",
        "\n",
        "        # Read each file and compute cumulative ROI\n",
        "        for file_path, identifier in all_files:\n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                # Validate the number of rows\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {file_path} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "                # Validate the presence of required columns\n",
        "                if 'month' not in df.columns or 'roi' not in df.columns:\n",
        "                    print(f\"Warning: 'month' or 'roi' column not found in {file_path}. Skipping.\")\n",
        "                    continue\n",
        "                # Compute cumulative ROI\n",
        "                df['cumulative_roi'] = (1 + df['roi']).cumprod()\n",
        "                df['model_seed'] = identifier\n",
        "                dfs.append(df[['month', 'cumulative_roi', 'model_seed']])\n",
        "            else:\n",
        "                print(f\"File {file_path} does not exist. Skipping.\")\n",
        "\n",
        "        # Concatenate all DataFrames\n",
        "        if dfs:\n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        else:\n",
        "            print(\"No dataframes to concatenate. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Pivot the combined DataFrame\n",
        "        portfolio_value_matrix = combined_df.pivot(index='month', columns='model_seed', values='cumulative_roi')\n",
        "\n",
        "        # Sort the index (months)\n",
        "        portfolio_value_matrix = portfolio_value_matrix.sort_index()\n",
        "\n",
        "        # Save to CSV\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        portfolio_value_matrix.to_csv(output_file)\n",
        "        print(f\"Portfolio value matrix saved to {output_file} with {len(portfolio_value_matrix)} rows and {len(portfolio_value_matrix.columns)} columns.\")\n",
        "\n",
        "        print(\"Portfolio value matrix creation completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during portfolio value matrix creation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_portfolio_value_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITvKxxuuD2EJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_portfolio_value_evolution(csv_file=\"ALL_BACKTEST/portfolio_value.csv\", output_file=\"ALL_BACKTEST/portfolio_value_evolution.png\"):\n",
        "    \"\"\"\n",
        "    Plot the portfolio value evolution for all model-seed combinations in a single graph.\n",
        "\n",
        "    Parameters:\n",
        "    csv_file (str): Path to the portfolio value CSV file\n",
        "    output_file (str): Path to save the plot image\n",
        "\n",
        "    Saves:\n",
        "    - Plot image as a PNG file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the CSV file exists\n",
        "        if not os.path.exists(csv_file):\n",
        "            raise FileNotFoundError(f\"The file {csv_file} does not exist. Please generate it using create_portfolio_value_matrix.py.\")\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(csv_file, index_col=\"month\")\n",
        "\n",
        "        # Validate the number of rows\n",
        "        expected_rows = 263\n",
        "        if len(df) != expected_rows:\n",
        "            print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}.\")\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for column in df.columns:\n",
        "            plt.plot(df.index, df[column], label=column)\n",
        "\n",
        "        # Customize the plot\n",
        "        plt.xlabel(\"Month\")\n",
        "        plt.ylabel(\"Portfolio Value\")\n",
        "        plt.title(\"Portfolio Value Evolution for All Models and Seeds\")\n",
        "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        plt.savefig(output_file)\n",
        "        print(f\"Plot saved to {output_file}\")\n",
        "\n",
        "        # Show the plot (optional, for interactive environments)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Plotting completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during plotting: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plot_portfolio_value_evolution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S69bVkHeFfDo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_model_seeds(df, models, title, output_file):\n",
        "    \"\"\"\n",
        "    Plot portfolio value evolution for specified models and seeds.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for model in models:\n",
        "        model_columns = [col for col in df.columns if col.startswith(model)]\n",
        "        for column in model_columns:\n",
        "            plt.plot(df.index, df[column], label=column)\n",
        "\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Portfolio Value\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {output_file}\")\n",
        "\n",
        "def plot_meta_super(df, title, output_file):\n",
        "    \"\"\"\n",
        "    Plot portfolio value evolution for meta and super agents.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    meta_super_columns = [col for col in df.columns if col.startswith('Meta_') or col.startswith('Super_')]\n",
        "    for column in meta_super_columns:\n",
        "        plt.plot(df.index, df[column], label=column)\n",
        "\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Portfolio Value\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {output_file}\")\n",
        "\n",
        "def create_plots():\n",
        "    \"\"\"\n",
        "    Create and save plots for portfolio value evolution in specified time periods,\n",
        "    with 2018-2024 plots normalized to start at 1 in January 2018.\n",
        "    \"\"\"\n",
        "    # Define directories\n",
        "    plot_dir = \"ALL_BACKTEST/plot\"\n",
        "    full_period_dir = os.path.join(plot_dir, \"2003-2024\")\n",
        "    recent_period_dir = os.path.join(plot_dir, \"2018-2024\")\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(full_period_dir, exist_ok=True)\n",
        "    os.makedirs(recent_period_dir, exist_ok=True)\n",
        "\n",
        "    # Load the portfolio value data\n",
        "    csv_file = \"SUPER/portfolio_value.csv\"\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise FileNotFoundError(f\"File {csv_file} not found.\")\n",
        "\n",
        "    df = pd.read_csv(csv_file, index_col=\"month\")\n",
        "\n",
        "    # Verify that '2018-01' exists in the index\n",
        "    if '2018-01' not in df.index:\n",
        "        raise ValueError(\"The date '2018-01' is not found in the data.\")\n",
        "\n",
        "    # Filter data for the 2018-2024 period\n",
        "    df_recent = df.loc['2018-01':].copy()\n",
        "\n",
        "    # Normalize the 2018-2024 data to start at 1 in January 2018\n",
        "    start_values = df_recent.iloc[0]  # Values at '2018-01'\n",
        "    normalized_df = df_recent / start_values  # Divide all values by January 2018 values\n",
        "\n",
        "    # Define models to plot\n",
        "    models = [\"ppo\", \"sac\", \"td3\", \"ddpg\"]\n",
        "\n",
        "    # Generate plots for the full period (2003-2024)\n",
        "    for model in models:\n",
        "        plot_model_seeds(\n",
        "            df,\n",
        "            [f\"Metrics_{model}\", f\"NLP_{model}\"],\n",
        "            f\"Portfolio Value Evolution - {model.upper()} Seeds (2003-2024)\",\n",
        "            os.path.join(full_period_dir, f\"{model}_seeds_2003-2024.png\")\n",
        "        )\n",
        "\n",
        "    plot_meta_super(\n",
        "        df,\n",
        "        \"Portfolio Value Evolution - Meta and Super Agents (2003-2024)\",\n",
        "        os.path.join(full_period_dir, \"meta_super_2003-2024.png\")\n",
        "    )\n",
        "\n",
        "    # Generate normalized plots for the recent period (2018-2024)\n",
        "    for model in models:\n",
        "        plot_model_seeds(\n",
        "            normalized_df,\n",
        "            [f\"Metrics_{model}\", f\"NLP_{model}\"],\n",
        "            f\"Portfolio Value Evolution - {model.upper()} Seeds (2018-2024, Normalized to Start at 1 in Jan 2018)\",\n",
        "            os.path.join(recent_period_dir, f\"{model}_seeds_2018-2024.png\")\n",
        "        )\n",
        "\n",
        "    plot_meta_super(\n",
        "        normalized_df,\n",
        "        \"Portfolio Value Evolution - Meta and Super Agents (2018-2024, Normalized to Start at 1 in Jan 2018)\",\n",
        "        os.path.join(recent_period_dir, \"meta_super_2018-2024.png\")\n",
        "    )\n",
        "\n",
        "    print(\"All plots generated successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        create_plots()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}