{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up imports and pips"
      ],
      "metadata": {
        "id": "8rxbCk1_be0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKbqeW0WaZ6N"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3[extra]\n",
        "!pip install --upgrade yfinance\n",
        "!pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import time\n",
        "import yfinance as yf\n",
        "from glob import glob\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import DDPG, PPO, SAC, TD3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy*\n",
        "from urllib.parse import urlencode\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "djc-diZhapQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_all_except(directory: str):\n",
        "    if not os.path.exists(directory):\n",
        "        logging.warning(f\"Directory '{directory}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.remove(file_path)\n",
        "                logging.info(f\"Deleted file: {file_path}\")\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "                logging.info(f\"Deleted directory and its contents: {file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "current_directory\n",
        "delete_all_except(current_directory)\n"
      ],
      "metadata": {
        "id": "iGAqGndnbrUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data collection"
      ],
      "metadata": {
        "id": "57Oc1JhWbi4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining ticker list"
      ],
      "metadata": {
        "id": "9yoFvttobucv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = [\n",
        "\"^GSPC\", # S&P 500\n",
        "\"^IXIC\", # NASDAQ Composite\n",
        "\"^DJI\", # Dow Jones Industrial Average\n",
        "\"^FCHI\", # CAC 40 (France)\n",
        "\"^FTSE\", # FTSE 100 (UK)\n",
        "\"^STOXX50E\",# EuroStoxx 50\n",
        "\"^HSI\", # Hang Seng Index (Hong Kong)\n",
        "\"000001.SS\",# Shanghai Composite (China)\n",
        "\"^BSESN\", # BSE Sensex (India)\n",
        "\"^NSEI\", # Nifty 50 (India)\n",
        "\"^KS11\", # KOSPI (South Korea)\n",
        "\"GC=F\", # Gold\n",
        "\"SI=F\", # Silver\n",
        "\"CL=F\", # WTI Crude Oil Futures\n",
        "]"
      ],
      "metadata": {
        "id": "8HkfajN4bdzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data fetching"
      ],
      "metadata": {
        "id": "UhMPWBRtbwQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = \"2003-01-01\"  # Example start date\n",
        "end_date = \"2024-12-31\"   # Example end date\n",
        "\n",
        "# Function to fetch data with rate limit handling\n",
        "def fetch_data(tickers, start, end):\n",
        "    retry = True\n",
        "    delay = 60  # Delay for rate limit errors\n",
        "    data = None  # Initialize data to None\n",
        "    while retry:\n",
        "        try:\n",
        "            # Batch download, specify actions=False to avoid extra data\n",
        "            data = yf.download(tickers, start=start, end=end, actions=False)\n",
        "            # If multi-ticker, extract 'Close' from the multi-level columns\n",
        "            if len(tickers) > 1:\n",
        "                data = data['Close']\n",
        "            else:\n",
        "                data = data[['Close']]  # Single ticker case\n",
        "            retry = False\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                print(f\"Rate limit hit. Retrying after {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Error fetching data: {e}\")\n",
        "                retry = False\n",
        "                data = pd.DataFrame()  # Return empty DataFrame on failure\n",
        "    return data\n",
        "\n",
        "# Download closing prices\n",
        "data = fetch_data(tickers, start_date, end_date)\n",
        "\n",
        "# Check if data is not empty\n",
        "if not data.empty:\n",
        "    # Save to CSV\n",
        "    data.to_csv(\"stock_closing_prices.csv\")\n",
        "    print(\"Closing prices saved to stock_closing_prices.csv\")\n",
        "else:\n",
        "    print(\"No data to save.\")\n",
        "\n",
        "# Add delay to avoid rate limits for subsequent requests\n",
        "time.sleep(2)"
      ],
      "metadata": {
        "id": "0xjLTIZwbxe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "3oiIbwZ9b6MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_stock_data(input_csv, output_csv):\n",
        "    \"\"\"\n",
        "    Clean stock closing prices by applying backward fill and linear interpolation.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'stock_closing_prices.csv')\n",
        "    output_csv (str): Path to the output CSV file (e.g., 'clean_data.csv')\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the cleaned data to output_csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Apply backward fill to handle missing data at start/end\n",
        "        df = df.bfill()\n",
        "\n",
        "        # Apply linear interpolation to fill small gaps\n",
        "        df = df.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        # Save the cleaned data to a new CSV\n",
        "        df.to_csv(output_csv)\n",
        "        print(f\"Cleaned data saved to {output_csv}\")\n",
        "\n",
        "        # Report any remaining NaN values\n",
        "        if df.isna().any().any():\n",
        "            print(\"Warning: Some NaN values remain after cleaning.\")\n",
        "            print(df.isna().sum())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    clean_stock_data(\"stock_closing_prices.csv\", \"clean_data.csv\")"
      ],
      "metadata": {
        "id": "aFTYqeLBb2H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize and log normalize"
      ],
      "metadata": {
        "id": "Lau9SBLdb8QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_stock_data(input_csv, normalized_output_csv, log_output_csv):\n",
        "    \"\"\"\n",
        "    Process cleaned stock data to create normalized and log evolution CSVs.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'clean_data.csv')\n",
        "    normalized_output_csv (str): Path to the output CSV for normalized data\n",
        "    log_output_csv (str): Path to the output CSV for log evolution data\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the processed data to the specified output CSVs\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"Error: Input CSV is empty.\")\n",
        "            return\n",
        "\n",
        "        # Initialize DataFrames for normalized and log evolution data\n",
        "        normalized_df = df.copy()\n",
        "        log_evolution_df = df.copy()\n",
        "\n",
        "        # Process each ticker (column)\n",
        "        for ticker in df.columns:\n",
        "            # Get the first non-null value for the ticker\n",
        "            first_valid = df[ticker].dropna().iloc[0] if df[ticker].dropna().size > 0 else None\n",
        "\n",
        "            if first_valid is not None and first_valid > 0:\n",
        "                # Normalize: divide by the first non-null value (set first point to 1)\n",
        "                normalized_df[ticker] = df[ticker] / first_valid\n",
        "\n",
        "                # Log evolution: ln(price / first_valid)\n",
        "                log_evolution_df[ticker] = np.log(df[ticker] / first_valid)\n",
        "            else:\n",
        "                # If no valid first value or first value is zero, set to NaN\n",
        "                print(f\"Warning: No valid first value for {ticker} or first value is zero.\")\n",
        "                normalized_df[ticker] = np.nan\n",
        "                log_evolution_df[ticker] = np.nan\n",
        "\n",
        "        # Save the normalized data\n",
        "        normalized_df.to_csv(normalized_output_csv)\n",
        "        print(f\"Normalized data saved to {normalized_output_csv}\")\n",
        "\n",
        "        # Save the log evolution data\n",
        "        log_evolution_df.to_csv(log_output_csv)\n",
        "        print(f\"Log evolution data saved to {log_output_csv}\")\n",
        "\n",
        "        # Report any columns with all NaN values\n",
        "        if normalized_df.isna().all().any():\n",
        "            print(\"Warning: Some tickers have all NaN in normalized data:\",\n",
        "                  normalized_df.columns[normalized_df.isna().all()].tolist())\n",
        "        if log_evolution_df.isna().all().any():\n",
        "            print(\"Warning: Some tickers have all NaN in log evolution data:\",\n",
        "                  log_evolution_df.columns[log_evolution_df.isna().all()].tolist())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    process_stock_data(\"clean_data.csv\", \"normalized_data.csv\", \"log_evolution_data.csv\")"
      ],
      "metadata": {
        "id": "qDfz3blYcA_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulate EQWP (equal weights portflio)"
      ],
      "metadata": {
        "id": "B-LpJm-TcDSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def simulate_equal_weights_portfolio(input_csv, output_csv, initial_value=1):\n",
        "    \"\"\"\n",
        "    Simulate an equal weights portfolio and save its evolution and log evolution.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'clean_data.csv')\n",
        "    output_csv (str): Path to the output CSV file (e.g., 'portfolio_evolution.csv')\n",
        "    initial_value (float): Initial portfolio value in dollars (default: 1000)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the portfolio evolution and log evolution to output_csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"Error: Input CSV is empty.\")\n",
        "            return\n",
        "\n",
        "        # Drop columns with all NaN values (e.g., failed tickers)\n",
        "        df = df.dropna(axis=1, how='all')\n",
        "        if df.empty:\n",
        "            print(\"Error: No valid ticker data after dropping NaN columns.\")\n",
        "            return\n",
        "\n",
        "        # Number of tickers\n",
        "        n_tickers = len(df.columns)\n",
        "        if n_tickers == 0:\n",
        "            print(\"Error: No valid tickers found.\")\n",
        "            return\n",
        "\n",
        "        # Initial allocation: equal weight for each ticker\n",
        "        weight = 1.0 / n_tickers\n",
        "        initial_allocation = initial_value * weight\n",
        "\n",
        "        # Calculate number of shares for each ticker (based on first valid price)\n",
        "        first_valid_prices = df.apply(lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else np.nan)\n",
        "        if first_valid_prices.isna().any():\n",
        "            print(\"Warning: Some tickers have no valid prices:\",\n",
        "                  first_valid_prices.index[first_valid_prices.isna()].tolist())\n",
        "            df = df.loc[:, ~first_valid_prices.isna()]\n",
        "            first_valid_prices = first_valid_prices.dropna()\n",
        "            n_tickers = len(df.columns)\n",
        "            weight = 1.0 / n_tickers\n",
        "            initial_allocation = initial_value * weight\n",
        "\n",
        "        shares = initial_allocation / first_valid_prices\n",
        "\n",
        "        # Calculate portfolio value: sum of (shares * price) for each ticker\n",
        "        portfolio_values = (df * shares).sum(axis=1)\n",
        "\n",
        "        # Calculate log evolution: ln(portfolio_value / initial_value)\n",
        "        log_evolution = np.log(portfolio_values / initial_value)\n",
        "\n",
        "        # Create output DataFrame\n",
        "        output_df = pd.DataFrame({\n",
        "            'Portfolio_Value': portfolio_values,\n",
        "            'Log_Evolution': log_evolution\n",
        "        })\n",
        "\n",
        "        # Save to CSV\n",
        "        output_df.to_csv(output_csv)\n",
        "        print(f\"Portfolio evolution saved to {output_csv}\")\n",
        "\n",
        "        # Report any NaN values in the output\n",
        "        if output_df.isna().any().any():\n",
        "            print(\"Warning: Some NaN values in portfolio evolution.\")\n",
        "            print(output_df.isna().sum())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    simulate_equal_weights_portfolio(\"clean_data.csv\", \"portfolio_evolution.csv\")"
      ],
      "metadata": {
        "id": "W6r8zwj-cGbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly Split"
      ],
      "metadata": {
        "id": "LDfzAyYvcLXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def split_normalized_by_month(input_csv, output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Split normalized data CSV into monthly CSVs named {yyyy}_{mm}_closing.csv.\n",
        "\n",
        "    Parameters:\n",
        "    input_csv (str): Path to the input CSV file (e.g., 'normalized_data.csv')\n",
        "    output_dir (str): Directory to save output CSVs (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves monthly CSVs to the specified directory\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV, assuming Date is the index\n",
        "        df = pd.read_csv(input_csv, index_col='Date', parse_dates=True)\n",
        "\n",
        "        # Check if DataFrame is empty\n",
        "        if df.empty:\n",
        "            print(\"Error: Input CSV is empty.\")\n",
        "            return\n",
        "\n",
        "        # Group data by year and month\n",
        "        grouped = df.groupby([df.index.year, df.index.month])\n",
        "\n",
        "        # Iterate through each year-month group\n",
        "        for (year, month), group_data in grouped:\n",
        "            # Format the output filename (e.g., 2003_01_closing.csv)\n",
        "            output_file = f\"{output_dir}/{year}_{month:02d}_closing.csv\"\n",
        "\n",
        "            # Save the group data to a CSV\n",
        "            group_data.to_csv(output_file)\n",
        "            print(f\"Saved {output_file} with {len(group_data)} rows\")\n",
        "\n",
        "            # Warn if the group has NaN values\n",
        "            if group_data.isna().any().any():\n",
        "                print(f\"Warning: {output_file} contains NaN values.\")\n",
        "                nan_columns = group_data.columns[group_data.isna().any()].tolist()\n",
        "                print(f\"Columns with NaN: {nan_columns}\")\n",
        "\n",
        "        print(\"All monthly CSVs generated successfully.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file {input_csv} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    split_normalized_by_month(\"normalized_data.csv\")"
      ],
      "metadata": {
        "id": "CDS-NGN6cKCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly metrics computation"
      ],
      "metadata": {
        "id": "LUdN3iLucPbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_monthly_metrics(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Generate monthly observation and correlation CSVs from normalized closing data.\n",
        "\n",
        "    For each {yyyy}_{mm}_closing.csv, creates:\n",
        "    - {yyyy}_{mm}_obs.csv: First close, last close, volatility, Sharpe, Sortino, Calmar, MDD\n",
        "    - {yyyy}_{mm}_corr.csv: Correlation matrix of daily log returns\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing input CSVs (default: current directory)\n",
        "    output_dir (str): Directory to save output CSVs (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves observation and correlation CSVs to output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find all monthly closing CSVs\n",
        "        closing_files = glob(f\"{input_dir}/*_closing.csv\")\n",
        "        if not closing_files:\n",
        "            print(f\"Error: No *_closing.csv files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        for file in closing_files:\n",
        "            # Extract year and month from filename (e.g., 2003_01_closing.csv)\n",
        "            filename = os.path.basename(file)\n",
        "            year_month = filename.replace(\"_closing.csv\", \"\")\n",
        "            year, month = map(int, year_month.split(\"_\"))\n",
        "\n",
        "            # Read the monthly CSV\n",
        "            df = pd.read_csv(file, index_col='Date', parse_dates=True)\n",
        "\n",
        "            if df.empty:\n",
        "                print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Drop columns with all NaN values\n",
        "            df = df.dropna(axis=1, how='all')\n",
        "            if df.empty:\n",
        "                print(f\"Warning: {filename} has no valid data after dropping NaN columns. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Calculate daily log returns: ln(price_t / price_{t-1})\n",
        "            log_returns = np.log(df / df.shift(1)).dropna()\n",
        "\n",
        "            # Initialize metrics DataFrame\n",
        "            metrics = pd.DataFrame(index=df.columns)\n",
        "\n",
        "            # Month First Close: First non-null normalized price\n",
        "            metrics['Month_First_Close'] = df.apply(lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else np.nan)\n",
        "\n",
        "            # Month Last Close: Last non-null normalized price\n",
        "            metrics['Month_Last_Close'] = df.apply(lambda x: x.dropna().iloc[-1] if x.dropna().size > 0 else np.nan)\n",
        "\n",
        "            # Volatility: Annualized standard deviation of daily log returns\n",
        "            metrics['Volatility'] = log_returns.std() * np.sqrt(252)\n",
        "\n",
        "            # Sharpe Ratio: Annualized mean log return / volatility (risk-free rate = 0)\n",
        "            metrics['Sharpe_Ratio'] = (log_returns.mean() * 252) / metrics['Volatility']\n",
        "\n",
        "            # Sortino Ratio: Annualized mean log return / downside volatility\n",
        "            downside_returns = log_returns.where(log_returns < 0, 0)\n",
        "            downside_vol = downside_returns.std() * np.sqrt(252)\n",
        "            metrics['Sortino_Ratio'] = (log_returns.mean() * 252) / downside_vol\n",
        "\n",
        "            # Maximum Drawdown (MDD): Max peak-to-trough decline\n",
        "            def calculate_mdd(series):\n",
        "                cumulative = series / series.iloc[0]  # Normalize to start at 1\n",
        "                peak = cumulative.cummax()\n",
        "                drawdown = (peak - cumulative) / peak\n",
        "                return drawdown.max() if drawdown.size > 0 else np.nan\n",
        "\n",
        "            metrics['MDD'] = df.apply(calculate_mdd)\n",
        "\n",
        "            # Calmar Ratio: Annualized return / MDD\n",
        "            monthly_return = (metrics['Month_Last_Close'] / metrics['Month_First_Close']) - 1\n",
        "            annualized_return = (1 + monthly_return) ** (12 / 1) - 1  # Annualize monthly return\n",
        "            metrics['Calmar_Ratio'] = annualized_return / metrics['MDD']\n",
        "\n",
        "            # Replace infinities with NaN (e.g., zero volatility or MDD)\n",
        "            metrics = metrics.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "            # Save observation CSV\n",
        "            obs_file = f\"{output_dir}/{year_month}_obs.csv\"\n",
        "            metrics.to_csv(obs_file)\n",
        "            print(f\"Saved {obs_file} with metrics for {len(metrics)} tickers\")\n",
        "\n",
        "            # Warn about NaN values in metrics\n",
        "            if metrics.isna().any().any():\n",
        "                print(f\"Warning: {obs_file} contains NaN values.\")\n",
        "                nan_columns = metrics.columns[metrics.isna().any()].tolist()\n",
        "                print(f\"Metrics with NaN: {nan_columns}\")\n",
        "\n",
        "            # Correlation matrix of daily log returns\n",
        "            if not log_returns.empty:\n",
        "                corr_matrix = log_returns.corr()\n",
        "                corr_file = f\"{output_dir}/{year_month}_corr.csv\"\n",
        "                corr_matrix.to_csv(corr_file)\n",
        "                print(f\"Saved {corr_file} with correlation matrix\")\n",
        "\n",
        "                # Warn about NaN values in correlation matrix\n",
        "                if corr_matrix.isna().any().any():\n",
        "                    print(f\"Warning: {corr_file} contains NaN values.\")\n",
        "                    nan_columns = corr_matrix.columns[corr_matrix.isna().any()].tolist()\n",
        "                    print(f\"Columns with NaN: {nan_columns}\")\n",
        "            else:\n",
        "                print(f\"Warning: No valid log returns for {year_month}. Skipping correlation matrix.\")\n",
        "\n",
        "        print(\"All observation and correlation CSVs generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    calculate_monthly_metrics()"
      ],
      "metadata": {
        "id": "lYscqJGHcUBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adressing the issue of NaN"
      ],
      "metadata": {
        "id": "7rO7iZArcVW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "def replace_nan_with_zeros(input_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Replace NaN values with 0s in specified CSV files.\n",
        "\n",
        "    Targets:\n",
        "    - normalized_data.csv\n",
        "    - log_evolution_data.csv\n",
        "    - portfolio_evolution.csv\n",
        "    - {yyyy}_{mm}_closing.csv\n",
        "    - {yyyy}_{mm}_obs.csv\n",
        "    - {yyyy}_{mm}_corr.csv\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the CSV files (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Overwrites original CSVs with NaN replaced by 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # List of specific files to process\n",
        "        specific_files = [\n",
        "            \"normalized_data.csv\",\n",
        "            \"log_evolution_data.csv\",\n",
        "            \"portfolio_evolution.csv\"\n",
        "        ]\n",
        "\n",
        "        # Add monthly files using glob\n",
        "        monthly_patterns = [\n",
        "            \"*_closing.csv\",\n",
        "            \"*_obs.csv\",\n",
        "            \"*_corr.csv\"\n",
        "        ]\n",
        "\n",
        "        # Collect all files to process\n",
        "        all_files = []\n",
        "        for file in specific_files:\n",
        "            file_path = os.path.join(input_dir, file)\n",
        "            if os.path.exists(file_path):\n",
        "                all_files.append(file_path)\n",
        "\n",
        "        for pattern in monthly_patterns:\n",
        "            all_files.extend(glob(os.path.join(input_dir, pattern)))\n",
        "\n",
        "        if not all_files:\n",
        "            print(f\"Error: No matching CSV files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each file\n",
        "        for file in all_files:\n",
        "            try:\n",
        "                # Determine index column based on file type\n",
        "                filename = os.path.basename(file)\n",
        "                if filename.endswith((\"_obs.csv\", \"_corr.csv\")):\n",
        "                    # _obs.csv and _corr.csv use first column as index (tickers)\n",
        "                    df = pd.read_csv(file, index_col=0)\n",
        "                else:\n",
        "                    # Other CSVs use Date as index\n",
        "                    df = pd.read_csv(file, index_col='Date', parse_dates=True)\n",
        "\n",
        "                # Replace NaN with 0\n",
        "                df = df.fillna(0)\n",
        "\n",
        "                # Save back to the original file\n",
        "                df.to_csv(file)\n",
        "                print(f\"Processed {filename}: Replaced NaN with 0s\")\n",
        "\n",
        "                # Verify no NaNs remain\n",
        "                if df.isna().any().any():\n",
        "                    print(f\"Warning: {filename} still contains NaN values after processing.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All specified CSVs processed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    replace_nan_with_zeros()"
      ],
      "metadata": {
        "id": "uY_u8r2RcYIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining metrics"
      ],
      "metadata": {
        "id": "gPhTXxz0cdIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def combine_monthly_metrics(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Combine monthly observation and correlation data into a single-column CSV.\n",
        "\n",
        "    For each {yyyy}_{mm}_obs.csv and {yyyy}_{mm}_corr.csv, creates:\n",
        "    - {yyyy}_{mm}_combined.csv: Single column with metrics and flattened correlations\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing input CSVs (default: current directory)\n",
        "    output_dir (str): Directory to save output CSVs (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves combined CSVs to output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find all observation CSVs\n",
        "        obs_files = glob(f\"{input_dir}/*_obs.csv\")\n",
        "        if not obs_files:\n",
        "            print(f\"Error: No *_obs.csv files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        for obs_file in obs_files:\n",
        "            # Extract year and month from filename (e.g., 2003_01_obs.csv)\n",
        "            filename = os.path.basename(obs_file)\n",
        "            year_month = filename.replace(\"_obs.csv\", \"\")\n",
        "\n",
        "            # Corresponding correlation file\n",
        "            corr_file = f\"{input_dir}/{year_month}_corr.csv\"\n",
        "\n",
        "            # Check if correlation file exists\n",
        "            if not os.path.exists(corr_file):\n",
        "                print(f\"Warning: {corr_file} not found. Skipping {year_month}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read observation CSV (tickers as index, metrics as columns)\n",
        "                obs_df = pd.read_csv(obs_file, index_col=0)\n",
        "\n",
        "                # Read correlation CSV (tickers as both index and columns)\n",
        "                corr_df = pd.read_csv(corr_file, index_col=0)\n",
        "\n",
        "                # Validate DataFrames\n",
        "                if obs_df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping {year_month}.\")\n",
        "                    continue\n",
        "                if corr_df.empty:\n",
        "                    print(f\"Warning: {corr_file} is empty. Skipping {year_month}.\")\n",
        "                    continue\n",
        "\n",
        "                # Initialize list for combined data\n",
        "                combined_data = []\n",
        "\n",
        "                # Process observation metrics\n",
        "                for ticker in obs_df.index:\n",
        "                    for metric in obs_df.columns:\n",
        "                        value = obs_df.at[ticker, metric]\n",
        "                        row_label = f\"{ticker}_{metric}\"\n",
        "                        combined_data.append((row_label, value))\n",
        "\n",
        "                # Flatten correlation matrix (upper triangle, exclude diagonal)\n",
        "                tickers = corr_df.index\n",
        "                for i in range(len(tickers)):\n",
        "                    for j in range(i + 1, len(tickers)):  # Upper triangle\n",
        "                        ticker1, ticker2 = tickers[i], tickers[j]\n",
        "                        value = corr_df.at[ticker1, ticker2]\n",
        "                        row_label = f\"{ticker1}_{ticker2}_Correlation\"\n",
        "                        combined_data.append((row_label, value))\n",
        "\n",
        "                # Create single-column DataFrame\n",
        "                combined_df = pd.DataFrame(\n",
        "                    [x[1] for x in combined_data],\n",
        "                    index=[x[0] for x in combined_data],\n",
        "                    columns=['Value']\n",
        "                )\n",
        "\n",
        "                # Save to combined CSV\n",
        "                output_file = f\"{output_dir}/{year_month}_combined.csv\"\n",
        "                combined_df.to_csv(output_file)\n",
        "                print(f\"Saved {output_file} with {len(combined_df)} rows\")\n",
        "\n",
        "                # Warn about any unexpected values (e.g., NaNs, though replaced with 0s)\n",
        "                if combined_df['Value'].eq(0).any():\n",
        "                    print(f\"Note: {output_file} contains zero values (from prior NaN replacement).\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {year_month}: {e}\")\n",
        "\n",
        "        print(\"All combined CSVs generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    combine_monthly_metrics()"
      ],
      "metadata": {
        "id": "2fC6c6OOcZZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File Organization"
      ],
      "metadata": {
        "id": "AohuKsIAce48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def organize_csv_files(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Organize CSV files into three folders: combined, price, and usage.\n",
        "\n",
        "    Folders:\n",
        "    - combined: {yyyy}_{mm}_combined.csv\n",
        "    - price: clean_data.csv, normalized_data.csv, log_evolution_data.csv, portfolio_evolution.csv\n",
        "    - usage: {yyyy}_{mm}_closing.csv, {yyyy}_{mm}_obs.csv, {yyyy}_{mm}_corr.csv\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the CSV files (default: current directory)\n",
        "    output_dir (str): Directory to create folders and move files (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    None: Moves CSV files to appropriate folders\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define folder names and their corresponding files\n",
        "        folders = {\n",
        "            \"combined\": glob(os.path.join(input_dir, \"*_combined.csv\")),\n",
        "            \"price\": [\n",
        "                os.path.join(input_dir, f) for f in [\n",
        "                    \"clean_data.csv\",\n",
        "                    \"normalized_data.csv\",\n",
        "                    \"log_evolution_data.csv\",\n",
        "                    \"portfolio_evolution.csv\",\n",
        "                    \"stock_closing_prices.csv\"\n",
        "                ] if os.path.exists(os.path.join(input_dir, f))\n",
        "            ],\n",
        "            \"usage\": (\n",
        "                glob(os.path.join(input_dir, \"*_closing.csv\")) +\n",
        "                glob(os.path.join(input_dir, \"*_obs.csv\")) +\n",
        "                glob(os.path.join(input_dir, \"*_corr.csv\"))\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Check if any files were found\n",
        "        total_files = sum(len(files) for files in folders.values())\n",
        "        if total_files == 0:\n",
        "            print(f\"Error: No matching CSV files found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Create folders if they don't exist\n",
        "        for folder in folders:\n",
        "            folder_path = os.path.join(output_dir, folder)\n",
        "            os.makedirs(folder_path, exist_ok=True)\n",
        "            print(f\"Created/Verified folder: {folder_path}\")\n",
        "\n",
        "        # Move files to their respective folders\n",
        "        for folder, files in folders.items():\n",
        "            target_dir = os.path.join(output_dir, folder)\n",
        "            for file in files:\n",
        "                filename = os.path.basename(file)\n",
        "                target_path = os.path.join(target_dir, filename)\n",
        "\n",
        "                # Skip if file already exists in target to avoid overwriting\n",
        "                if os.path.exists(target_path):\n",
        "                    print(f\"Skipped {filename}: Already exists in {target_dir}\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    shutil.move(file, target_path)\n",
        "                    print(f\"Moved {filename} to {target_dir}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error moving {filename}: {e}\")\n",
        "\n",
        "        print(\"All CSV files organized successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error organizing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    organize_csv_files()"
      ],
      "metadata": {
        "id": "4X1OMwlScrq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Plots"
      ],
      "metadata": {
        "id": "kJKEIxLLcva2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_plots(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Generate plots and correlation heatmaps from CSV files.\n",
        "\n",
        "    Creates:\n",
        "    - plots/normalized_evolution.png: Normalized prices for all tickers\n",
        "    - plots/log_evolution.png: Log evolution for all tickers\n",
        "    - plots/log_evolution_portfolio.png: Log evolution + portfolio log evolution\n",
        "    - plots/normalized_portfolio.png: Normalized prices + normalized portfolio\n",
        "    - plots/corr/{yyyy}_{mm}_corr_heatmap.png: Heatmaps for monthly correlations\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing price and usage folders (default: current)\n",
        "    output_dir (str): Directory to save plots folder (default: current)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves plots and heatmaps to plots and plots/corr folders\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create plots and corr folders\n",
        "        plots_dir = os.path.join(output_dir, \"plots\")\n",
        "        corr_dir = os.path.join(plots_dir, \"corr\")\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        os.makedirs(corr_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folders: {plots_dir}, {corr_dir}\")\n",
        "\n",
        "        # Paths to input files\n",
        "        price_dir = os.path.join(input_dir, \"price\")\n",
        "        usage_dir = os.path.join(input_dir, \"usage\")\n",
        "\n",
        "        # Load data for plots, handling missing files\n",
        "        normalized_df = None\n",
        "        log_evolution_df = None\n",
        "        portfolio_df = None\n",
        "\n",
        "        normalized_file = os.path.join(price_dir, \"normalized_data.csv\")\n",
        "        if os.path.exists(normalized_file):\n",
        "            try:\n",
        "                normalized_df = pd.read_csv(normalized_file, index_col=\"Date\", parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {normalized_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {normalized_file} not found. Skipping normalized plots.\")\n",
        "\n",
        "        log_evolution_file = os.path.join(price_dir, \"log_evolution_data.csv\")\n",
        "        if os.path.exists(log_evolution_file):\n",
        "            try:\n",
        "                log_evolution_df = pd.read_csv(log_evolution_file, index_col=\"Date\", parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {log_evolution_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {log_evolution_file} not found. Skipping log evolution plots.\")\n",
        "\n",
        "        portfolio_file = os.path.join(price_dir, \"portfolio_evolution.csv\")\n",
        "        if os.path.exists(portfolio_file):\n",
        "            try:\n",
        "                portfolio_df = pd.read_csv(portfolio_file, index_col=\"Date\", parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {portfolio_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {portfolio_file} not found. Skipping portfolio-related plots.\")\n",
        "\n",
        "        # Plot 1: Normalized Evolution\n",
        "        if normalized_df is not None and not normalized_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in normalized_df.columns:\n",
        "                if normalized_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in normalized_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(normalized_df.index, normalized_df[column], label=column)\n",
        "            plt.title(\"Normalized Evolution of Tickers\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Normalized Price (First Point = 1)\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"normalized_evolution.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/normalized_evolution.png\")\n",
        "        else:\n",
        "            print(\"Skipping normalized_evolution.png: No valid normalized data.\")\n",
        "\n",
        "        # Plot 2: Log Evolution\n",
        "        if log_evolution_df is not None and not log_evolution_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in log_evolution_df.columns:\n",
        "                if log_evolution_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in log_evolution_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(log_evolution_df.index, log_evolution_df[column], label=column)\n",
        "            plt.title(\"Log Evolution of Tickers\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Log Evolution (ln(price / price_0))\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"log_evolution.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/log_evolution.png\")\n",
        "        else:\n",
        "            print(\"Skipping log_evolution.png: No valid log evolution data.\")\n",
        "\n",
        "        # Plot 3: Log Evolution + Portfolio\n",
        "        if log_evolution_df is not None and portfolio_df is not None and not log_evolution_df.empty and not portfolio_df.empty:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in log_evolution_df.columns:\n",
        "                if log_evolution_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in log_evolution_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(log_evolution_df.index, log_evolution_df[column], label=column)\n",
        "            plt.plot(portfolio_df.index, portfolio_df[\"Log_Evolution\"],\n",
        "                    label=\"Equal Weights Portfolio\", linewidth=2, linestyle=\"--\")\n",
        "            plt.title(\"Log Evolution of Tickers and Equal Weights Portfolio\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Log Evolution\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"log_evolution_portfolio.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/log_evolution_portfolio.png\")\n",
        "        else:\n",
        "            print(\"Skipping log_evolution_portfolio.png: Missing log evolution or portfolio data.\")\n",
        "\n",
        "        # Plot 4: Normalized + Portfolio\n",
        "        if normalized_df is not None and portfolio_df is not None and not normalized_df.empty and not portfolio_df.empty:\n",
        "            # Normalize portfolio value to start at 1\n",
        "            portfolio_normalized = portfolio_df[\"Portfolio_Value\"] / portfolio_df[\"Portfolio_Value\"].iloc[0]\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            for column in normalized_df.columns:\n",
        "                if normalized_df[column].eq(0).all():\n",
        "                    print(f\"Warning: {column} has all zero values in normalized_data.csv. Skipping.\")\n",
        "                    continue\n",
        "                plt.plot(normalized_df.index, normalized_df[column], label=column)\n",
        "            plt.plot(portfolio_normalized.index, portfolio_normalized,\n",
        "                    label=\"Equal Weights Portfolio\", linewidth=2, linestyle=\"--\")\n",
        "            plt.title(\"Normalized Evolution of Tickers and Equal Weights Portfolio\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Normalized Value (First Point = 1)\")\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(plots_dir, \"normalized_portfolio.png\"))\n",
        "            plt.close()\n",
        "            print(\"Saved plots/normalized_portfolio.png\")\n",
        "        else:\n",
        "            print(\"Skipping normalized_portfolio.png: Missing normalized or portfolio data.\")\n",
        "\n",
        "        # Generate correlation matrix heatmaps\n",
        "        corr_files = glob(os.path.join(usage_dir, \"*_corr.csv\"))\n",
        "        if not corr_files:\n",
        "            print(f\"Warning: No *_corr.csv files found in {usage_dir}.\")\n",
        "        else:\n",
        "            for corr_file in corr_files:\n",
        "                try:\n",
        "                    # Extract year and month from filename\n",
        "                    filename = os.path.basename(corr_file)\n",
        "                    year_month = filename.replace(\"_corr.csv\", \"\")\n",
        "\n",
        "                    # Read correlation matrix\n",
        "                    corr_df = pd.read_csv(corr_file, index_col=0)\n",
        "\n",
        "                    # Skip empty or invalid matrices\n",
        "                    if corr_df.empty or corr_df.eq(0).all().all():\n",
        "                        print(f\"Warning: {filename} is empty or all zeros. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Plot heatmap\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    sns.heatmap(corr_df, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1,\n",
        "                               center=0, fmt=\".2f\")\n",
        "                    plt.title(f\"Correlation Matrix - {year_month}\")\n",
        "                    output_file = os.path.join(corr_dir, f\"{year_month}_corr_heatmap.png\")\n",
        "                    plt.savefig(output_file, bbox_inches=\"tight\")\n",
        "                    plt.close()\n",
        "                    print(f\"Saved {output_file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All available plots and heatmaps generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating plots: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming organized structure from previous step\n",
        "    generate_plots(\".\", \"./organized\")"
      ],
      "metadata": {
        "id": "RbHamN8sczQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract combined metrics"
      ],
      "metadata": {
        "id": "IZ3zKMEWc4IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_combined_metrics(input_dir=\".\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Extract the second column (Value) from combined CSVs and save to observation/metrics.\n",
        "\n",
        "    Processes:\n",
        "    - All {yyyy}_{mm}_combined.csv from input_dir/organized/combined/\n",
        "    - Saves single-column CSVs to output_dir/observation/metrics/\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing organized/combined folder (default: current)\n",
        "    output_dir (str): Directory to create observation/metrics folder (default: current)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves extracted CSVs to observation/metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define input and output paths\n",
        "        combined_dir = os.path.join(input_dir, \"combined\")\n",
        "        metrics_dir = os.path.join(output_dir, \"observation\", \"metrics\")\n",
        "\n",
        "        # Create observation/metrics folder\n",
        "        os.makedirs(metrics_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {metrics_dir}\")\n",
        "\n",
        "        # Find all combined CSV files\n",
        "        combined_files = glob(os.path.join(combined_dir, \"*_combined.csv\"))\n",
        "        if not combined_files:\n",
        "            print(f\"Error: No *_combined.csv files found in {combined_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each combined CSV\n",
        "        for file in combined_files:\n",
        "            try:\n",
        "                # Extract filename\n",
        "                filename = os.path.basename(file)\n",
        "\n",
        "                # Read CSV (index is first column, Value is second)\n",
        "                df = pd.read_csv(file, index_col=0)\n",
        "\n",
        "                if df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Verify the second column (Value)\n",
        "                if len(df.columns) != 1 or df.columns[0] != \"Value\":\n",
        "                    print(f\"Warning: {filename} does not have exactly one data column (Value). Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract the Value column with the index\n",
        "                output_df = df[[\"Value\"]]\n",
        "\n",
        "                # Save to observation/metrics with the same filename\n",
        "                output_file = os.path.join(metrics_dir, filename)\n",
        "                output_df.to_csv(output_file)\n",
        "                print(f\"Saved {output_file} with {len(output_df)} rows\")\n",
        "\n",
        "                # Note if all values are zero\n",
        "                if output_df[\"Value\"].eq(0).all():\n",
        "                    print(f\"Note: {filename} contains all zero values.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All combined metrics CSVs processed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming organized structure from previous steps\n",
        "    extract_combined_metrics(\".\", \".\")"
      ],
      "metadata": {
        "id": "aDEsb9bvc-Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove index collumn"
      ],
      "metadata": {
        "id": "zqx7hGZXdEFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_index_column(metrics_dir):\n",
        "    \"\"\"\n",
        "    Remove the first column (index) from CSVs in observation/metrics, keeping only the Value column.\n",
        "\n",
        "    Processes:\n",
        "    - All CSVs in metrics_dir (e.g., 2003_01_combined.csv)\n",
        "    - Overwrites each CSV with a single-column version (Value only, no index or header)\n",
        "\n",
        "    Parameters:\n",
        "    metrics_dir (str): Path to observation/metrics directory\n",
        "\n",
        "    Returns:\n",
        "    None: Overwrites CSVs in metrics_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find all CSV files in metrics_dir\n",
        "        csv_files = glob(os.path.join(metrics_dir, \"*.csv\"))\n",
        "        if not csv_files:\n",
        "            print(f\"Error: No CSV files found in {metrics_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Process each CSV\n",
        "        for file in csv_files:\n",
        "            try:\n",
        "                # Extract filename\n",
        "                filename = os.path.basename(file)\n",
        "\n",
        "                # Read CSV (index is first column, Value is second)\n",
        "                df = pd.read_csv(file, index_col=0)\n",
        "\n",
        "                if df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Verify the second column (Value)\n",
        "                if len(df.columns) != 1 or df.columns[0] != \"Value\":\n",
        "                    print(f\"Warning: {filename} does not have exactly one data column (Value). Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract the Value column without index\n",
        "                values = df[\"Value\"]\n",
        "\n",
        "                # Save as single-column CSV without index or header\n",
        "                values.to_csv(file, index=False, header=False)\n",
        "                print(f\"Processed {filename}: Removed index column, kept Value column ({len(values)} rows)\")\n",
        "\n",
        "                # Note if all values are zero\n",
        "                if values.eq(0).all():\n",
        "                    print(f\"Note: {filename} contains all zero values.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All CSVs in observation/metrics processed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming organized structure from previous steps\n",
        "    metrics_dir = \"observation/metrics\"\n",
        "    remove_index_column(metrics_dir)"
      ],
      "metadata": {
        "id": "Pgi3-VXldBXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: NLP"
      ],
      "metadata": {
        "id": "s_z-5lpqdHLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib.parse import urlencode\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "import shutil\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_google_news_link(query, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Generate a Google News search URL with custom date range.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The search query.\n",
        "        start_date (str): Start date in MM/DD/YYYY format.\n",
        "        end_date (str): End date in MM/DD/YYYY format.\n",
        "\n",
        "    Returns:\n",
        "        str: Google News search URL.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.google.com/search\"\n",
        "\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"tbs\": f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\",\n",
        "        \"tbm\": \"nws\"\n",
        "    }\n",
        "\n",
        "    return f\"{base_url}?{urlencode(params)}\"\n",
        "\n",
        "# List of tickers and their homologous terms\n",
        "search_queries = [\n",
        "    [\"SP 500\", \"S&P 500\", \"Standard and Poor's 500\", \"^GSPC\", \"S&P 500 Index\", \"SPX\", \"S&P 500 ETF\", \"S&P500\", \"S&P Index\", \"Standard & Poor's 500 Index\", \"S&P 500 Stock Market\", \"US Stock Market\", \"American Stocks\", \"USA Economy\", \"U.S. Markets\", \"U.S. Economy\", \"Wall Street Index\", \"US Equity Market\", \"U.S. Stock Exchange\", \"S&P 500 Companies\"],\n",
        "    [\"NASDAQ\", \"NASDAQ Composite\", \"NASDAQ Index\", \"^IXIC\", \"Nasdaq Composite Index\", \"NASDAQ-100\", \"Nasdaq 100\", \"NASDAQ-100 Index\", \"NASDAQ stocks\", \"NASDAQ Index ETF\", \"American Technology Stocks\", \"U.S. Tech Stocks\", \"Tech-heavy Index\", \"USA Stock Market\", \"Nasdaq Tech Index\", \"NASDAQ Growth\", \"Silicon Valley Stocks\", \"NASDAQ 100 Tech\", \"USA Technology\", \"Tech Stocks Index\"],\n",
        "    [\"Dow Jones\", \"DJIA\", \"Dow Jones Industrial Average\", \"^DJI\", \"Dow Jones Index\", \"Dow Jones Average\", \"DJIA Index\", \"Dow Jones Industrial\", \"Dow Jones Industrial Stocks\", \"DJIA ETF\", \"US Blue-Chip Stocks\", \"USA Industrial Stocks\", \"U.S. Market Leaders\", \"Wall Street Benchmark\", \"Dow Jones Companies\", \"US Industrials\", \"American Economy\", \"US Industrial Market\", \"US Stock Index\", \"Wall Street Giants\"],\n",
        "    [\"CAC 40\", \"Paris Stock Exchange\", \"Euronext Paris\", \"^FCHI\", \"French Stock Market\", \"Paris Index\", \"French Economy\", \"France Stock Exchange\", \"CAC 40 Companies\", \"Paris Bourse\", \"French Markets\", \"Paris Exchange\", \"Euronext Index\", \"French Blue Chip Stocks\", \"French Equity Market\", \"France Stock Index\", \"Paris Market\", \"Eurozone Stocks\", \"France Economy\", \"Eurozone Market\"],\n",
        "    [\"FTSE 100\", \"London Stock Exchange\", \"UK 100 Index\", \"^FTSE\", \"FTSE 100 Index\", \"London Index\", \"UK Stock Market\", \"British Stock Exchange\", \"UK Economy\", \"FTSE 100 Companies\", \"London Market\", \"UK Economy Stocks\", \"FTSE Index ETF\", \"British Blue Chips\", \"London Exchange\", \"UK Markets\", \"British Economy\", \"UK Financial Markets\", \"London Stock Index\", \"UK Stock Exchange\", \"FTSE 100 Stocks\"],\n",
        "    [\"^STOXX50E\", \"EuroStoxx 50\", \"EuroStoxx 50 Index\", \"European Stock Market\", \"Eurozone Stocks\", \"European Economy\", \"Stoxx Europe 50\", \"Eurozone 50 Index\", \"EuroStoxx Index\", \"Europe Market Leaders\", \"Eurozone Leaders\", \"Top European Stocks\", \"Eurozone Top Companies\", \"European Blue Chips\", \"European Equity Market\", \"Eurozone Financials\", \"Eurozone Benchmark\", \"European Blue Chip Stocks\", \"Eurozone Economic Index\", \"European Market Index\"],\n",
        "    [\"^N225\", \"Nikkei 225\", \"Nikkei Index\", \"Japanese Stock Market\", \"Japan Economy\", \"Nikkei Average\", \"Tokyo Stock Exchange\", \"Japan Top 225 Stocks\", \"Japan Stock Index\", \"Japanese Equity Market\", \"Nikkei 225 Companies\", \"Japan's Leading Stocks\", \"Japanese Financial Market\", \"Tokyo Exchange\", \"Nikkei Market\", \"Japan Economic Index\", \"Japanese Economy\", \"Japan's Stock Exchange\", \"Top Japanese Companies\", \"Nikkei 225 ETF\"],\n",
        "    [\"^HSI\", \"Hang Seng\", \"Hang Seng Index\", \"Hong Kong Stock Market\", \"Hong Kong Economy\", \"Hong Kong Exchange\", \"HSI Index\", \"Hang Seng Index ETF\", \"Hong Kong Financial Market\", \"Chinese Stock Market\", \"HSI Stocks\", \"Asia-Pacific Stocks\", \"Hong Kong Blue Chips\", \"Hong Kong's Leading Stocks\", \"HSI Index Stocks\", \"Asian Financial Market\", \"Hong Kong Leading Companies\", \"Asian Market Leaders\", \"Hang Seng Companies\", \"Hong Kong Stock Index\"],\n",
        "    [\"000001.SS\", \"Shanghai Composite\", \"Shanghai Stock Exchange\", \"Chinese Stock Market\", \"China Economy\", \"Shanghai Index\", \"China Financial Market\", \"Shanghai Composite Index\", \"Shanghai Exchange\", \"Chinese Stock Index\", \"China's Leading Stocks\", \"Shanghai Exchange Companies\", \"China Blue Chip Stocks\", \"Chinese Equity Market\", \"Shanghai Financial Index\", \"China Benchmark\", \"Shanghai Composite ETF\", \"China's Leading Companies\", \"Chinese Market Leaders\", \"China's Economic Stocks\"],\n",
        "    [\"^BSESN\", \"Bombay Sensex\", \"S&P BSE Sensex\", \"Indian Stock Market\", \"India Economy\", \"BSE Sensex\", \"Mumbai Stock Exchange\", \"Sensex Companies\", \"Indian Financial Market\", \"Sensex 30\", \"BSE 30 Index\", \"India's Leading Stocks\", \"Indian Market Leaders\", \"Indian Equity Market\", \"Bombay Exchange\", \"Indian Stock Index\", \"Sensex Index ETF\", \"BSE India\", \"Indian Economy Stocks\", \"Bombay Financial Index\"],\n",
        "    [\"^NSEI\", \"Nifty 50\", \"National Stock Exchange of India\", \"Indian Stock Index\", \"India Stock Market\", \"Nifty Index\", \"India Economy\", \"Indian Market Leaders\", \"Nifty 50 Stocks\", \"NSE India\", \"Indian Blue Chips\", \"Indian Financial Market\", \"Indian Stock Market\", \"India 50 Index\", \"Nifty Index ETF\", \"India's Top 50\", \"India's Leading Stocks\", \"Nifty 50 Companies\", \"India Economic Stocks\", \"Indian Market Index\"],\n",
        "    [\"^KS11\", \"KOSPI\", \"Korea Composite Stock Price Index\", \"Korean Stock Market\", \"Korea Economy\", \"KOSPI Index\", \"South Korean Stock Market\", \"Korean Market Leaders\", \"KOSPI 200\", \"South Korea Financial Market\", \"Korean Exchange\", \"KOSPI ETF\", \"Korean Leading Stocks\", \"South Korean Economy\", \"Korean Market Index\", \"South Korea Stock Index\", \"Korean Economy Stocks\", \"South Korea Exchange\", \"Korean Equity Market\", \"Korea Stock Index\"],\n",
        "    [\"Gold\", \"XAU\", \"Gold Price\", \"Gold Market\", \"Precious Metals\", \"Gold Spot\", \"Gold Bullion\", \"Gold ETF\", \"Gold Investment\", \"Gold Mining\", \"Gold Futures\", \"Gold Stocks\", \"Gold Index\", \"Gold Commodity\", \"Gold Trading\", \"Gold Bullion ETF\", \"Gold Commodity Index\", \"Gold Market Trends\", \"Gold Investment Funds\", \"Gold Prices Today\"],\n",
        "    [\"Silver\", \"XAG\", \"Silver Price\", \"Silver Market\", \"Precious Metals\", \"Silver Spot\", \"Silver Bullion\", \"Silver ETF\", \"Silver Investment\", \"Silver Mining\", \"Silver Futures\", \"Silver Stocks\", \"Silver Index\", \"Silver Commodity\", \"Silver Trading\", \"Silver Bullion ETF\", \"Silver Commodity Index\", \"Silver Market Trends\", \"Silver Investment Funds\", \"Silver Prices Today\"],\n",
        "    [\"Oil\", \"Crude Oil\", \"WTI\", \"Brent Crude\", \"Oil Price\", \"Crude Oil Price\", \"OPEC\", \"Oil Futures\", \"Oil Market\", \"Oil Stocks\", \"Oil ETF\", \"Global Oil Supply\", \"Oil Trading\", \"Oil Production\", \"Brent Oil Futures\", \"Oil Price Index\", \"Oil Investment\", \"Oil Exploration\", \"Oil Trading Market\", \"Oil Price Trends\"]\n",
        "]\n",
        "\n",
        "\n",
        "# Define the year range\n",
        "start_year = 2003\n",
        "end_year = 2024\n",
        "\n",
        "# Store all generated links\n",
        "links = []\n",
        "\n",
        "for query_group in search_queries:  # Loop through each group of related terms\n",
        "    for query in query_group:  # Loop through each term in the group\n",
        "        for year in range(start_year, end_year + 1):  # Loop through years\n",
        "            for month in range(1, 13):  # Loop through months\n",
        "                start_date = f\"{month}/1/{year}\"\n",
        "                # Calculate the last day of the month\n",
        "                next_month = month % 12 + 1\n",
        "                next_year = year if month < 12 else year + 1\n",
        "                end_date = (datetime(next_year, next_month, 1) - timedelta(days=1)).strftime(\"%m/%d/%Y\")\n",
        "\n",
        "                # Generate the search link\n",
        "                link = generate_google_news_link(query, start_date, end_date)\n",
        "                links.append([query, start_date, end_date, link])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(links, columns=[\"Query\", \"Start Date\", \"End Date\", \"Google News Link\"])\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"links.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Create the NLP_data folder if it doesn't exist\n",
        "os.makedirs(\"NLP_data\", exist_ok=True)\n",
        "\n",
        "# Move the links.csv file into the NLP_data folder\n",
        "shutil.move(\"links.csv\", os.path.join(\"NLP_data\", \"links.csv\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "UnZGi2XLdvjl",
        "outputId": "50554d3e-25a2-4a37-ab6e-0b665c4acc83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'datetime' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-43b745e1131a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mnext_month\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mnext_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_month\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m/%d/%Y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# Generate the search link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "from urllib.parse import urlencode\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_google_news_link(query, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Generate a Google News search URL with custom date range.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.google.com/search\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"tbs\": f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\",\n",
        "        \"tbm\": \"nws\"\n",
        "    }\n",
        "    return f\"{base_url}?{urlencode(params)}\"\n",
        "\n",
        "def get_news_links(search_url):\n",
        "    \"\"\"\n",
        "    Fetch news article links from a Google News search results page.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(search_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract all links from the search results\n",
        "        links = []\n",
        "        for a_tag in soup.find_all(\"a\", href=True):\n",
        "            href = a_tag[\"href\"]\n",
        "            if \"https://\" in href and \"google.com\" not in href:\n",
        "                links.append(href)\n",
        "        return set(links)  # Remove duplicates\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve {search_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load CSV file with search queries from the NLP_data folder\n",
        "csv_file_path = os.path.join(\"NLP_data\", \"links.csv\")\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Base folder to save results\n",
        "output_base_folder = \"google_news_results\"\n",
        "os.makedirs(output_base_folder, exist_ok=True)\n",
        "\n",
        "# Process each row in the CSV\n",
        "for index, row in df.iterrows():\n",
        "    query = row[\"Query\"].replace(\" \", \"_\")  # Sanitize folder name\n",
        "    search_url = row[\"Google News Link\"]\n",
        "\n",
        "    # Create directory for the query\n",
        "    query_folder = os.path.join(output_base_folder, query)\n",
        "    os.makedirs(query_folder, exist_ok=True)\n",
        "\n",
        "    # Get news article links\n",
        "    news_links = get_news_links(search_url)\n",
        "\n",
        "    # Sanitize file name by replacing slashes with underscores\n",
        "    start_date_sanitized = row['Start Date'].replace(\"/\", \"_\")\n",
        "    end_date_sanitized = row['End Date'].replace(\"/\", \"_\")\n",
        "    file_path = os.path.join(query_folder, f\"{start_date_sanitized}_to_{end_date_sanitized}.txt\")\n",
        "\n",
        "    # Save links to a text file\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for link in news_links:\n",
        "            f.write(link + \"\\n\")\n",
        "\n",
        "    print(f\"Saved {len(news_links)} links for {query} ({row['Start Date']} to {row['End Date']})\")\n",
        "\n",
        "    # Respect Google's policies, wait between requests\n",
        "    time.sleep(3)\n"
      ],
      "metadata": {
        "id": "jgfInheBguqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "# Base folder where raw results are stored\n",
        "output_base_folder = \"google_news_results\"\n",
        "\n",
        "# Move non-empty files and delete empty ones\n",
        "for query in os.listdir(output_base_folder):\n",
        "    query_path = os.path.join(output_base_folder, query)\n",
        "    if os.path.isdir(query_path):\n",
        "        for file_name in os.listdir(query_path):\n",
        "            file_path = os.path.join(query_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                # Delete file if it is empty\n",
        "                if os.path.getsize(file_path) == 0:\n",
        "                    os.remove(file_path)\n",
        "                    print(f\"Deleted empty file: {file_path}\")\n",
        "                else:\n",
        "                    # Extract the date part from the filename (assumes format: MM_DD_YYYY_to_...)\n",
        "                    try:\n",
        "                        date_part = file_name.split(\"_to_\")[0]  # Get the start date part\n",
        "                        date_obj = datetime.strptime(date_part, \"%m_%d_%Y\")  # Convert to datetime\n",
        "                        year = date_obj.year\n",
        "                        month = date_obj.month\n",
        "                        month_name = date_obj.strftime(\"%B\")  # Full month name\n",
        "\n",
        "                        # New folder structure: output_base_folder/year/MonthName/query\n",
        "                        new_location = os.path.join(output_base_folder, str(year), month_name, query)\n",
        "                        os.makedirs(new_location, exist_ok=True)\n",
        "\n",
        "                        # Move the file to the new location\n",
        "                        os.rename(file_path, os.path.join(new_location, file_name))\n",
        "                        print(f\"Moved file to {new_location}\")\n",
        "                    except ValueError:\n",
        "                        print(f\"Skipping file due to incorrect date format: {file_name}\")\n",
        "\n",
        "# Delete any now-empty query folders at the top level of output_base_folder\n",
        "for query in os.listdir(output_base_folder):\n",
        "    query_path = os.path.join(output_base_folder, query)\n",
        "    if os.path.isdir(query_path) and not os.listdir(query_path):\n",
        "        os.rmdir(query_path)\n",
        "        print(f\"Deleted empty group folder: {query_path}\")\n",
        "\n",
        "print(\"Cleanup and organization complete!\")\n"
      ],
      "metadata": {
        "id": "O5MrxVaDgwBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Dictionary mapping standardized query groups to their variants\n",
        "query_groups = {\n",
        "    \"SP_500\": [\"SP_500\", \"S&P_500\", \"Standard_and_Poor's_500\", \"^GSPC\", \"S&P_500_Index\", \"SPX\", \"S&P_500_ETF\", \"S&P500\", \"S&P_Index\", \"Standard_&_Poor's_500_Index\", \"S&P_500_Stock_Market\", \"US_Stock_Market\", \"American_Stocks\", \"USA_Economy\", \"U.S._Markets\", \"U.S._Economy\", \"Wall_Street_Index\", \"US_Equity_Market\", \"U.S._Stock_Exchange\", \"S&P_500_Companies\"],\n",
        "    \"NASDAQ\": [\"NASDAQ\", \"NASDAQ_Composite\", \"NASDAQ_Index\", \"^IXIC\", \"Nasdaq_Composite_Index\", \"NASDAQ-100\", \"Nasdaq_100\", \"NASDAQ-100_Index\", \"NASDAQ_stocks\", \"NASDAQ_Index_ETF\", \"American_Technology_Stocks\", \"U.S._Tech_Stocks\", \"Tech-heavy_Index\", \"USA_Stock_Market\", \"Nasdaq_Tech_Index\", \"NASDAQ_Growth\", \"Silicon_Valley_Stocks\", \"NASDAQ_100_Tech\", \"USA_Technology\", \"Tech_Stocks_Index\"],\n",
        "    \"Dow_Jones\": [\"Dow_Jones\", \"DJIA\", \"Dow_Jones_Industrial_Average\", \"^DJI\", \"Dow_Jones_Index\", \"Dow_Jones_Average\", \"DJIA_Index\", \"Dow_Jones_Industrial\", \"Dow_Jones_Industrial_Stocks\", \"DJIA_ETF\", \"US_Blue-Chip_Stocks\", \"USA_Industrial_Stocks\", \"U.S._Market_Leaders\", \"Wall_Street_Benchmark\", \"Dow_Jones_Companies\", \"US_Industrials\", \"American_Economy\", \"US_Industrial_Market\", \"US_Stock_Index\", \"Wall_Street_Giants\"],\n",
        "    \"CAC_40\": [\"CAC_40\", \"Paris_Stock_Exchange\", \"Euronext_Paris\", \"^FCHI\", \"French_Stock_Market\", \"Paris_Index\", \"French_Economy\", \"France_Stock_Exchange\", \"CAC_40_Companies\", \"Paris_Bourse\", \"French_Markets\", \"Paris_Exchange\", \"Euronext_Index\", \"French_Blue_Chip_Stocks\", \"French_Equity_Market\", \"France_Stock_Index\", \"Paris_Market\", \"Eurozone_Stocks\", \"France_Economy\", \"Eurozone_Market\"],\n",
        "    \"FTSE_100\": [\"FTSE_100\", \"London_Stock_Exchange\", \"UK_100_Index\", \"^FTSE\", \"FTSE_100_Index\", \"London_Index\", \"UK_Stock_Market\", \"British_Stock_Exchange\", \"UK_Economy\", \"FTSE_100_Companies\", \"London_Market\", \"UK_Economy_Stocks\", \"FTSE_Index_ETF\", \"British_Blue_Chips\", \"London_Exchange\", \"UK_Markets\", \"British_Economy\", \"UK_Financial_Markets\", \"London_Stock_Index\", \"UK_Stock_Exchange\", \"FTSE_100_Stocks\"],\n",
        "    \"EuroStoxx_50\": [\"^STOXX50E\", \"EuroStoxx_50\", \"EuroStoxx_50_Index\", \"European_Stock_Market\", \"Eurozone_Stocks\", \"European_Economy\", \"Stoxx_Europe_50\", \"Eurozone_50_Index\", \"EuroStoxx_Index\", \"Europe_Market_Leaders\", \"Eurozone_Leaders\", \"Top_European_Stocks\", \"Eurozone_Top_Companies\", \"European_Blue_Chips\", \"European_Equity_Market\", \"Eurozone_Financials\", \"Eurozone_Benchmark\", \"European_Blue_Chip_Stocks\", \"Eurozone_Economic_Index\", \"European_Market_Index\"],\n",
        "    \"Nikkei_225\": [\"^N225\", \"Nikkei_225\", \"Nikkei_Index\", \"Japanese_Stock_Market\", \"Japan_Economy\", \"Nikkei_Average\", \"Tokyo_Stock_Exchange\", \"Japan_Top_225_Stocks\", \"Japan_Stock_Index\", \"Japanese_Equity_Market\", \"Nikkei_225_Companies\", \"Japan's_Leading_Stocks\", \"Japanese_Financial_Market\", \"Tokyo_Exchange\", \"Nikkei_Market\", \"Japan_Economic_Index\", \"Japanese_Economy\", \"Japan's_Stock_Exchange\", \"Top_Japanese_Companies\", \"Nikkei_225_ETF\"],\n",
        "    \"Hang_Seng\": [\"^HSI\", \"Hang_Seng\", \"Hang_Seng_Index\", \"Hong_Kong_Stock_Market\", \"Hong_Kong_Economy\", \"Hong_Kong_Exchange\", \"HSI_Index\", \"Hang_Seng_Index_ETF\", \"Hong_Kong_Financial_Market\", \"Chinese_Stock_Market\", \"HSI_Stocks\", \"Asia-Pacific_Stocks\", \"Hong_Kong_Blue_Chips\", \"Hong_Kong's_Leading_Stocks\", \"HSI_Index_Stocks\", \"Asian_Financial_Market\", \"Hong_Kong_Leading_Companies\", \"Asian_Market_Leaders\", \"Hang_Seng_Companies\", \"Hong_Kong_Stock_Index\"],\n",
        "    \"Shanghai_Composite\": [\"000001.SS\", \"Shanghai_Composite\", \"Shanghai_Stock_Exchange\", \"Chinese_Stock_Market\", \"China_Economy\", \"Shanghai_Index\", \"China_Financial_Market\", \"Shanghai_Composite_Index\", \"Shanghai_Exchange\", \"Chinese_Stock_Index\", \"China's_Leading_Stocks\", \"Shanghai_Exchange_Companies\", \"China_Blue_Chip_Stocks\", \"Chinese_Equity_Market\", \"Shanghai_Financial_Index\", \"China_Benchmark\", \"Shanghai_Composite_ETF\", \"China's_Leading_Companies\", \"Chinese_Market_Leaders\", \"China's_Economic_Stocks\"],\n",
        "    \"Bombay_Sensex\": [\"^BSESN\", \"Bombay_Sensex\", \"S&P_BSE_Sensex\", \"Indian_Stock_Market\", \"India_Economy\", \"BSE_Sensex\", \"Mumbai_Stock_Exchange\", \"Sensex_Companies\", \"Indian_Financial_Market\", \"Sensex_30\", \"BSE_30_Index\", \"India's_Leading_Stocks\", \"Indian_Market_Leaders\", \"Indian_Equity_Market\", \"Bombay_Exchange\", \"Indian_Stock_Index\", \"Sensex_Index_ETF\", \"BSE_India\", \"Indian_Economy_Stocks\", \"Bombay_Financial_Index\"],\n",
        "    \"Nifty_50\": [\"^NSEI\", \"Nifty_50\", \"National_Stock_Exchange_of_India\", \"Indian_Stock_Index\", \"India_Stock_Market\", \"Nifty_Index\", \"India_Economy\", \"Indian_Market_Leaders\", \"Nifty_50_Stocks\", \"NSE_India\", \"Indian_Blue_Chips\", \"Indian_Financial_Market\", \"Indian_Stock_Market\", \"India_50_Index\", \"Nifty_Index_ETF\", \"India's_Top_50\", \"India's_Leading_Stocks\", \"Nifty_50_Companies\", \"India_Economic_Stocks\", \"Indian_Market_Index\"],\n",
        "    \"KOSPI\": [\"^KS11\", \"KOSPI\", \"Korea_Composite_Stock_Price_Index\", \"Korean_Stock_Market\", \"Korea_Economy\", \"KOSPI_Index\", \"South_Korean_Stock_Market\", \"Korean_Market_Leaders\", \"KOSPI_200\", \"South_Korea_Financial_Market\", \"Korean_Exchange\", \"KOSPI_ETF\", \"Korean_Leading_Stocks\", \"South_Korean_Economy\", \"Korean_Market_Index\", \"South_Korea_Stock_Index\", \"Korean_Economy_Stocks\", \"South_Korea_Exchange\", \"Korean_Equity_Market\", \"Korea_Stock_Index\"],\n",
        "    \"Gold\": [\"Gold\", \"XAU\", \"Gold_Price\", \"Gold_Market\", \"Precious_Metals\", \"Gold_Spot\", \"Gold_Bullion\", \"Gold_ETF\", \"Gold_Investment\", \"Gold_Mining\", \"Gold_Futures\", \"Gold_Stocks\", \"Gold_Index\", \"Gold_Commodity\", \"Gold_Trading\", \"Gold_Bullion_ETF\", \"Gold_Commodity_Index\", \"Gold_Market_Trends\", \"Gold_Investment_Funds\", \"Gold_Prices_Today\"],\n",
        "    \"Silver\": [\"Silver\", \"XAG\", \"Silver_Price\", \"Silver_Market\", \"Precious_Metals\", \"Silver_Spot\", \"Silver_Bullion\", \"Silver_ETF\", \"Silver_Investment\", \"Silver_Mining\", \"Silver_Futures\", \"Silver_Stocks\", \"Silver_Index\", \"Silver_Commodity\", \"Silver_Trading\", \"Silver_Bullion_ETF\", \"Silver_Commodity_Index\", \"Silver_Market_Trends\", \"Silver_Investment_Funds\", \"Silver_Prices_Today\"],\n",
        "    \"Oil\": [\"Oil\", \"Crude_Oil\", \"WTI\", \"Brent_Crude\", \"Oil_Price\", \"Crude_Oil_Price\", \"OPEC\", \"Oil_Futures\", \"Oil_Market\", \"Oil_Stocks\", \"Oil_ETF\", \"Global_Oil_Supply\", \"Oil_Trading\", \"Oil_Production\", \"Brent_Oil_Futures\", \"Oil_Price_Index\", \"Oil_Investment\", \"Oil_Exploration\", \"Oil_Trading_Market\", \"Oil_Price_Trends\"]\n",
        "}\n",
        "\n",
        "# Loop through each year/month folder and unify text files for each query group\n",
        "for year_folder in os.listdir(output_base_folder):\n",
        "    year_path = os.path.join(output_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for query_group, query_list in query_groups.items():\n",
        "                    unified_file_path = os.path.join(month_path, f\"{query_group}_united.txt\")\n",
        "                    with open(unified_file_path, \"w\", encoding=\"utf-8\") as unified_file:\n",
        "                        for query in query_list:\n",
        "                            query_folder_path = os.path.join(month_path, query)\n",
        "                            if os.path.isdir(query_folder_path):\n",
        "                                for file_name in os.listdir(query_folder_path):\n",
        "                                    file_path = os.path.join(query_folder_path, file_name)\n",
        "                                    if file_path.endswith(\".txt\") and os.path.isfile(file_path):\n",
        "                                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                                            content = f.read()\n",
        "                                            unified_file.write(content + \"\\n\")\n",
        "                                        os.remove(file_path)\n",
        "                                        print(f\"Deleted {file_path}\")\n",
        "                                # Remove the now-empty query folder\n",
        "                                os.rmdir(query_folder_path)\n",
        "                                print(f\"Deleted empty folder: {query_folder_path}\")\n",
        "                    print(f\"Created unified file: {unified_file_path}\")\n"
      ],
      "metadata": {
        "id": "5Zw1hF8Fgw5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def remove_empty_lines(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(non_empty_lines)\n",
        "    print(f\"Removed empty lines from: {file_path}\")\n",
        "\n",
        "# Loop through each year/month folder and clean text files\n",
        "for year_folder in os.listdir(output_base_folder):\n",
        "    year_path = os.path.join(output_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for query_folder in os.listdir(month_path):\n",
        "                    query_folder_path = os.path.join(month_path, query_folder)\n",
        "                    if os.path.isdir(query_folder_path):\n",
        "                        for file_name in os.listdir(query_folder_path):\n",
        "                            file_path = os.path.join(query_folder_path, file_name)\n",
        "                            if file_path.endswith(\".txt\") and os.path.isfile(file_path):\n",
        "                                remove_empty_lines(file_path)\n",
        "print(\"Empty lines removal completed.\")\n"
      ],
      "metadata": {
        "id": "mnJ7u2aKgxot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_article_details(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string if soup.title else \"No title found\"\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = \" \".join([p.get_text() for p in paragraphs])\n",
        "        first_500_words = \" \".join(article_text.split()[:500])\n",
        "        return title, first_500_words\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve {url}: {e}\")\n",
        "        return \"Error\", \"Error\"\n",
        "\n",
        "# Process unified text files to extract article details and save as CSV\n",
        "for year_folder in os.listdir(output_base_folder):\n",
        "    year_path = os.path.join(output_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\"_united.txt\"):\n",
        "                        file_path = os.path.join(month_path, file_name)\n",
        "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                            urls = f.readlines()\n",
        "                        article_data = []\n",
        "                        for url in urls:\n",
        "                            url = url.strip()\n",
        "                            title, first_500_words = extract_article_details(url)\n",
        "                            article_data.append([url, title, first_500_words])\n",
        "                        csv_file_path = os.path.splitext(file_path)[0] + \".csv\"\n",
        "                        df = pd.DataFrame(article_data, columns=[\"URL\", \"Title\", \"First_500_Words\"])\n",
        "                        df.to_csv(csv_file_path, index=False, encoding=\"utf-8\")\n",
        "                        print(f\"Created CSV for {file_name}: {csv_file_path}\")\n",
        "print(\"Data extraction and CSV creation completed.\")\n"
      ],
      "metadata": {
        "id": "PbnMCDmlgyc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define source and target base folders\n",
        "source_base_folder = \"google_news_results\"\n",
        "target_base_folder = \"sentiment_analysis\"\n",
        "os.makedirs(target_base_folder, exist_ok=True)\n",
        "\n",
        "# Move CSV files from source to target structure, preserving year/month hierarchy\n",
        "for year_folder in os.listdir(source_base_folder):\n",
        "    year_path = os.path.join(source_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        target_year_path = os.path.join(target_base_folder, year_folder)\n",
        "        os.makedirs(target_year_path, exist_ok=True)\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                target_month_path = os.path.join(target_year_path, month_folder)\n",
        "                os.makedirs(target_month_path, exist_ok=True)\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\".csv\"):\n",
        "                        file_path = os.path.join(month_path, file_name)\n",
        "                        target_file_path = os.path.join(target_month_path, file_name)\n",
        "                        shutil.move(file_path, target_file_path)\n",
        "                        print(f\"Moved {file_name} to {target_file_path}\")\n",
        "print(\"CSV files have been moved to the sentiment_analysis folder.\")\n"
      ],
      "metadata": {
        "id": "5Z4HTgiigzNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "# Load FinBERT sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
        "def add_sentiment_labels(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if 'First_500_Words' not in df.columns:\n",
        "        print(f\"Skipping {csv_path} (no 'First_500_Words' column found)\")\n",
        "        return\n",
        "    sentiment_labels = []\n",
        "    sentiment_scores = []\n",
        "    for text in df['First_500_Words']:\n",
        "        if isinstance(text, str):\n",
        "            truncated_text = text[:500]  # Truncate text if needed\n",
        "            sentiment = sentiment_pipeline(truncated_text)[0]\n",
        "            sentiment_labels.append(sentiment['label'])\n",
        "            sentiment_scores.append(sentiment['score'])\n",
        "        else:\n",
        "            sentiment_labels.append(\"Error\")\n",
        "            sentiment_scores.append(0.0)\n",
        "    df['Sentiment_Label'] = sentiment_labels\n",
        "    df['Sentiment_Score'] = sentiment_scores\n",
        "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Updated sentiment for {csv_path}\")\n",
        "\n",
        "# Apply sentiment analysis to each CSV in sentiment_analysis folder\n",
        "sentiment_base_folder = \"sentiment_analysis\"\n",
        "for year_folder in os.listdir(sentiment_base_folder):\n",
        "    year_path = os.path.join(sentiment_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\".csv\"):\n",
        "                        csv_path = os.path.join(month_path, file_name)\n",
        "                        add_sentiment_labels(csv_path)\n",
        "print(\"Sentiment analysis labels and scores have been added.\")\n"
      ],
      "metadata": {
        "id": "_pb708k1gz-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "def calculate_average_score(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if 'Sentiment_Label' not in df.columns or 'Sentiment_Score' not in df.columns:\n",
        "        print(f\"Skipping {csv_path} (required columns missing)\")\n",
        "        return None\n",
        "    positive_scores = df[df['Sentiment_Label'] == 'positive']['Sentiment_Score']\n",
        "    negative_scores = df[df['Sentiment_Label'] == 'negative']['Sentiment_Score']\n",
        "    sum_positive = positive_scores.sum()\n",
        "    sum_negative = negative_scores.sum()\n",
        "    num_positive = len(positive_scores)\n",
        "    num_negative = len(negative_scores)\n",
        "    if num_positive + num_negative == 0:\n",
        "        return None\n",
        "    average_score = (sum_positive - sum_negative) / (num_positive + num_negative)\n",
        "    return average_score\n",
        "# Process each month folder to create a summary CSV\n",
        "sentiment_base_folder = \"sentiment_analysis\"\n",
        "for year_folder in os.listdir(sentiment_base_folder):\n",
        "    year_path = os.path.join(sentiment_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                summary_data = []\n",
        "                print(f\"Processing {month_folder}...\")\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\".csv\"):\n",
        "                        csv_path = os.path.join(month_path, file_name)\n",
        "                        # Extract the query group name from filename (remove _united.csv)\n",
        "                        query_group_name = file_name.replace(\"_united.csv\", \"\")\n",
        "                        avg_score = calculate_average_score(csv_path)\n",
        "                        if avg_score is not None:\n",
        "                            print(f\"Adding {query_group_name} with score: {avg_score}\")\n",
        "                            summary_data.append([query_group_name, avg_score])\n",
        "                if summary_data:\n",
        "                    summary_df = pd.DataFrame(summary_data, columns=[\"Query Group\", \"Average Score\"])\n",
        "                    summary_csv_path = os.path.join(month_path, \"monthly_summary.csv\")\n",
        "                    summary_df.to_csv(summary_csv_path, index=False, encoding=\"utf-8\")\n",
        "                    print(f\"Created monthly summary for {month_folder}: {summary_csv_path}\")\n",
        "                else:\n",
        "                    print(f\"No data for {month_folder}, skipping.\")\n",
        "print(\"Monthly summaries have been created.\")\n"
      ],
      "metadata": {
        "id": "DmXWxXpug0ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define base folders for final results\n",
        "sentiment_base_folder = \"sentiment_analysis\"\n",
        "final_results_folder = \"final_results\"\n",
        "os.makedirs(final_results_folder, exist_ok=True)\n",
        "\n",
        "# Process monthly summaries and copy them to final_results, while aggregating data for unified matrix\n",
        "all_months_data = []\n",
        "\n",
        "for year_folder in os.listdir(sentiment_base_folder):\n",
        "    year_path = os.path.join(sentiment_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        target_year_path = os.path.join(final_results_folder, year_folder)\n",
        "        os.makedirs(target_year_path, exist_ok=True)\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                summary_csv_path = os.path.join(month_path, \"monthly_summary.csv\")\n",
        "                if os.path.exists(summary_csv_path):\n",
        "                    target_month_path = os.path.join(target_year_path, month_folder)\n",
        "                    os.makedirs(target_month_path, exist_ok=True)\n",
        "                    shutil.copy(summary_csv_path, target_month_path)\n",
        "                    month_data = pd.read_csv(summary_csv_path)\n",
        "                    month_data['Month'] = month_folder\n",
        "                    month_data['Year'] = year_folder\n",
        "                    all_months_data.append(month_data)\n",
        "\n",
        "# Create unified matrix if data is available\n",
        "if all_months_data:\n",
        "    unified_df = pd.concat(all_months_data)\n",
        "    unified_matrix = unified_df.pivot_table(index=['Year', 'Month'], columns='Query Group', values='Average Score')\n",
        "    unified_matrix_path = os.path.join(final_results_folder, \"unified_matrix.csv\")\n",
        "    unified_matrix.to_csv(unified_matrix_path, encoding=\"utf-8\")\n",
        "    print(f\"Unified matrix has been created at: {unified_matrix_path}\")\n",
        "else:\n",
        "    print(\"No data available to create the unified matrix.\")\n",
        "\n",
        "# (Optional) Save a simulated unified matrix as a separate file\n",
        "unified_matrix.to_csv(\"simulated_unified_matrix.csv\", encoding=\"utf-8\")\n",
        "print(\"Simulated unified matrix saved to simulated_unified_matrix.csv\")\n"
      ],
      "metadata": {
        "id": "G2Yo6o4eg3Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def generate_sim_nlp_vectors(input_dir=\"organized\", output_dir=\"organized\", sentiment_data_path=None):\n",
        "    \"\"\"\n",
        "    Create monthly CSVs with 28-dimensional vectors: 14 volatilities + 14 sentiments (from data if provided, else random).\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing organized/observation/metrics (default: 'organized')\n",
        "    output_dir (str): Directory to create sim_nlp folder (default: 'organized')\n",
        "    sentiment_data_path (str): Path to unified_matrix.csv containing sentiment data (default: None, uses random)\n",
        "\n",
        "    Returns:\n",
        "    None: Saves CSVs to sim_nlp\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define tickers (in order)\n",
        "        tickers = [\n",
        "            'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "            '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "        ]\n",
        "\n",
        "        # Define paths\n",
        "        metrics_dir = os.path.join(input_dir, \"observation\", \"metrics\")\n",
        "        sim_nlp_dir = os.path.join(output_dir, \"sim_nlp\")\n",
        "\n",
        "        # Create sim_nlp folder\n",
        "        os.makedirs(sim_nlp_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {sim_nlp_dir}\")\n",
        "\n",
        "        # Find all combined CSV files\n",
        "        combined_files = glob(os.path.join(metrics_dir, \"*_combined.csv\"))\n",
        "        if not combined_files:\n",
        "            print(f\"Error: No *_combined.csv files found in {metrics_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Load sentiment data if provided\n",
        "        if sentiment_data_path:\n",
        "            sentiment_df = pd.read_csv(sentiment_data_path)\n",
        "            # Define mapping from tickers to Query_Groups\n",
        "            ticker_to_query = {\n",
        "                'GC=F': 'Gold',\n",
        "                'SI=F': 'Silver',\n",
        "                '^DJI': 'Dow Jones',\n",
        "                '^IXIC': 'NASDAQ',\n",
        "                'CL=F': 'Crude Oil',\n",
        "                '^GSPC': 'S&P 500',\n",
        "                '^STOXX50E': 'EURO STOXX 50',\n",
        "                '^FCHI': 'CAC 40',\n",
        "                '^FTSE': 'FTSE 100',\n",
        "                '^HSI': 'Hang Seng',\n",
        "                '000001.SS': 'Shanghai Composite',\n",
        "                '^KS11': 'KOSPI',\n",
        "                '^BSESN': 'BSE Sensex',\n",
        "                '^NSEI': 'Nifty 50'\n",
        "            }\n",
        "        else:\n",
        "            print(\"No sentiment data provided, using random sentiments.\")\n",
        "\n",
        "        # Process each combined CSV\n",
        "        for file in combined_files:\n",
        "            try:\n",
        "                # Extract year and month from filename (e.g., 2003_01_combined.csv)\n",
        "                filename = os.path.basename(file)\n",
        "                year_month = filename.replace(\"_combined.csv\", \"\")\n",
        "                year, month = year_month.split(\"_\")\n",
        "                output_filename = f\"{year}{month}.csv\"\n",
        "\n",
        "                # Read CSV (single-column, no header)\n",
        "                df = pd.read_csv(file, header=None)\n",
        "\n",
        "                if df.empty:\n",
        "                    print(f\"Warning: {filename} is empty. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Ensure correct number of rows\n",
        "                expected_rows = 14 * 7 + (14 * 13 // 2)  # 98 metrics + 91 correlations\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {filename} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract volatilities (rows 3rd to 16th, assuming 7 metrics per ticker)\n",
        "                volatility_indices = [2 + i * 7 for i in range(14)]  # Volatility is 3rd metric (index 2)\n",
        "                volatilities = df.iloc[volatility_indices, 0].values\n",
        "\n",
        "                # Get sentiments\n",
        "                if sentiment_data_path:\n",
        "                    y = int(year)\n",
        "                    m = int(month)\n",
        "                    sentiments = []\n",
        "                    for ticker in tickers:\n",
        "                        query_group = ticker_to_query.get(ticker, None)\n",
        "                        if query_group:\n",
        "                            df_sent = sentiment_df[(sentiment_df['Year'] == y) & (sentiment_df['Month'] == m) & (sentiment_df['Query_Group'] == query_group)]\n",
        "                            if not df_sent.empty:\n",
        "                                sentiments.append(df_sent['Average_Sentiment'].values[0])\n",
        "                            else:\n",
        "                                sentiments.append(0)  # Default to 0 if no data\n",
        "                        else:\n",
        "                            sentiments.append(0)  # Default to 0 if no mapping\n",
        "                    sentiments = np.array(sentiments)\n",
        "                else:\n",
        "                    sentiments = np.random.uniform(-1, 1, 14)\n",
        "\n",
        "                # Combine into 28-dimensional vector\n",
        "                vector = np.concatenate([volatilities, sentiments])\n",
        "\n",
        "                # Create column names\n",
        "                columns = [f\"Vol_{ticker}\" for ticker in tickers] + [f\"Sent_{ticker}\" for ticker in tickers]\n",
        "\n",
        "                # Create single-row DataFrame\n",
        "                output_df = pd.DataFrame([vector], columns=columns)\n",
        "\n",
        "                # Save to sim_nlp\n",
        "                output_file = os.path.join(sim_nlp_dir, output_filename)\n",
        "                output_df.to_csv(output_file, index=False)\n",
        "                print(f\"Saved {output_file} with 28-dimensional vector\")\n",
        "\n",
        "                # Note if volatilities are all zero\n",
        "                if np.all(volatilities == 0):\n",
        "                    print(f\"Note: {filename} has all zero volatilities.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "        print(\"All sim_nlp CSVs generated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing files: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    generate_sim_nlp_vectors(\".\", \".\", sentiment_data_path=\"path/to/unified_matrix.csv\")"
      ],
      "metadata": {
        "id": "FpuJkvqrexWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. The rest"
      ],
      "metadata": {
        "id": "h6lJINTqg5zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_folders_to_metrics_used(input_dir=\"organized\", output_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Create a metrics_used folder and move all folders from input_dir except observation.\n",
        "\n",
        "    Moves:\n",
        "    - combined, price, usage, plots to metrics_used\n",
        "    - Leaves observation in input_dir\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing organized folders (default: 'organized')\n",
        "    output_dir (str): Directory to create metrics_used folder (default: current)\n",
        "\n",
        "    Returns:\n",
        "    None: Moves folders to metrics_used\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define paths\n",
        "        metrics_used_dir = os.path.join(output_dir, \"metrics_used\")\n",
        "\n",
        "        # Create metrics_used folder\n",
        "        os.makedirs(metrics_used_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {metrics_used_dir}\")\n",
        "\n",
        "        # Check if input_dir exists\n",
        "        if not os.path.exists(input_dir):\n",
        "            print(f\"Error: Input directory {input_dir} does not exist.\")\n",
        "            return\n",
        "\n",
        "        # Get list of folders in input_dir\n",
        "        folders = [f for f in os.listdir(input_dir)\n",
        "                  if os.path.isdir(os.path.join(input_dir, f))]\n",
        "\n",
        "        if not folders:\n",
        "            print(f\"Error: No folders found in {input_dir}.\")\n",
        "            return\n",
        "\n",
        "        # Exclude observation folder\n",
        "        folders_to_move = [f for f in folders if f != \"observation\"]\n",
        "\n",
        "        if not folders_to_move:\n",
        "            print(f\"Warning: No folders to move (only observation found in {input_dir}).\")\n",
        "            return\n",
        "\n",
        "        # Move each folder to metrics_used\n",
        "        for folder in folders_to_move:\n",
        "            source_path = os.path.join(input_dir, folder)\n",
        "            target_path = os.path.join(metrics_used_dir, folder)\n",
        "\n",
        "            # Skip if folder already exists in target\n",
        "            if os.path.exists(target_path):\n",
        "                print(f\"Skipped {folder}: Already exists in {metrics_used_dir}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                shutil.move(source_path, target_path)\n",
        "                print(f\"Moved {folder} to {metrics_used_dir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error moving {folder}: {e}\")\n",
        "\n",
        "        # Verify observation remains\n",
        "        observation_path = os.path.join(input_dir, \"observation\")\n",
        "        if os.path.exists(observation_path):\n",
        "            print(f\"Confirmed: observation folder remains in {input_dir}\")\n",
        "        else:\n",
        "            print(f\"Warning: observation folder not found in {input_dir}\")\n",
        "\n",
        "        print(\"All specified folders moved successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing directories: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    move_folders_to_metrics_used(\".\", \".\")"
      ],
      "metadata": {
        "id": "5ZvAHtxWg5R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stablebaseline3"
      ],
      "metadata": {
        "id": "hvhDxVakiMkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomPortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gymnasium environment for portfolio management.\n",
        "\n",
        "    - Observation: 189-dimensional vector from organized/observation/metrics/{yyyy}_{mm}_combined.csv\n",
        "    - Action: 14-dimensional weight allocations for 14 assets (sum to 1)\n",
        "    - Reward: 2 * ROI - 0.7 * volatility - 0.5 * MDD, computed over the next month\n",
        "    \"\"\"\n",
        "    def __init__(self, price_dir=\"metrics_used/price\", metrics_dir=\"organized/observation/metrics\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define tickers\n",
        "        self.tickers = [\n",
        "            'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "            '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "        ]\n",
        "\n",
        "        # Load daily price data for reward calculation\n",
        "        price_file = os.path.join(price_dir, \"clean_data.csv\")\n",
        "        if not os.path.exists(price_file):\n",
        "            raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
        "        self.daily_prices = pd.read_csv(price_file, index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "        # Load monthly observations\n",
        "        self.metrics_dir = metrics_dir\n",
        "        months = pd.date_range(start=\"2003-01-01\", end=\"2017-12-31\", freq=\"ME\")\n",
        "        self.observations = []\n",
        "        self.month_files = []\n",
        "        for month in months:\n",
        "            file_path = os.path.join(metrics_dir, f\"{month.year}_{month.month:02d}_combined.csv\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, header=None)\n",
        "                    if len(df) == 189:  # Ensure correct observation size\n",
        "                        self.observations.append(df.iloc[:, 0].values.astype(np.float32))\n",
        "                        self.month_files.append(file_path)\n",
        "                    else:\n",
        "                        print(f\"Warning: {file_path} has {len(df)} rows, expected 189. Skipping.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "        if not self.observations:\n",
        "            raise ValueError(\"No valid observation files found.\")\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(189,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        # Get monthly last trading days\n",
        "        self.monthly_last_days = self.daily_prices.resample(\"ME\").last().index.tolist()\n",
        "        self.monthly_last_days = [d for d in self.monthly_last_days\n",
        "                                if d >= pd.Timestamp(\"2003-01-01\") and d <= pd.Timestamp(\"2024-12-31\")]\n",
        "\n",
        "        # Align observations with price data\n",
        "        self.total_steps = min(len(self.observations), len(self.monthly_last_days) - 1)\n",
        "        if self.total_steps < 1:\n",
        "            raise ValueError(\"Insufficient data for training.\")\n",
        "\n",
        "        # Initialize state\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state.\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take an action (weights) and compute the reward for the next month.\n",
        "        \"\"\"\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode is over\")\n",
        "\n",
        "        # Normalize action to sum to 1\n",
        "        action_sum = np.sum(action)\n",
        "        if action_sum > 0:\n",
        "            weights = action / action_sum\n",
        "        else:\n",
        "            weights = np.ones(14) / 14  # Equal weights if invalid\n",
        "\n",
        "        # Get start and end dates for the month\n",
        "        start_date = self.monthly_last_days[self.current_step]\n",
        "        end_date = self.monthly_last_days[self.current_step + 1]\n",
        "\n",
        "        # Get daily prices for the period\n",
        "        try:\n",
        "            daily_prices = self.daily_prices.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing price data for period {start_date} to {end_date}. Using zeros for reward.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute portfolio values\n",
        "        initial_prices = daily_prices.iloc[0].values\n",
        "        daily_portfolio_values = np.sum(daily_prices.values * weights, axis=1)\n",
        "\n",
        "        # Compute ROI\n",
        "        if daily_portfolio_values[0] > 0:\n",
        "            roi = (daily_portfolio_values[-1] / daily_portfolio_values[0]) - 1\n",
        "        else:\n",
        "            roi = 0\n",
        "\n",
        "        # Compute daily returns\n",
        "        daily_returns = daily_portfolio_values[1:] / daily_portfolio_values[:-1] - 1 if len(daily_portfolio_values) > 1 else np.array([0])\n",
        "\n",
        "        # Compute volatility\n",
        "        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Compute MDD\n",
        "        cummax = np.maximum.accumulate(daily_portfolio_values)\n",
        "        drawdown = (cummax - daily_portfolio_values) / cummax if cummax[0] > 0 else np.zeros_like(daily_portfolio_values)\n",
        "        mdd = np.max(drawdown) if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Move to next step\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check if done\n",
        "        done = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Get next observation\n",
        "        obs = self.observations[self.current_step] if not done else self.observations[-1]\n",
        "        info = {\"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_rl_agents(price_dir=\"metrics_used/price\", metrics_dir=\"organized/observation/metrics\",\n",
        "                   output_dir=\"results\", seeds=[1, 2, 3, 4, 5], total_timesteps=10000):\n",
        "    \"\"\"\n",
        "    Train PPO, SAC, DDPG, TD3 models with specified seeds and save results.\n",
        "\n",
        "    Parameters:\n",
        "    price_dir (str): Directory containing clean_data.csv\n",
        "    metrics_dir (str): Directory containing observation vectors\n",
        "    output_dir (str): Directory to save models and evaluation\n",
        "    seeds (list): List of random seeds\n",
        "    total_timesteps (int): Number of timesteps for training\n",
        "\n",
        "    Saves:\n",
        "    - Models: results/{model}_seed_{seed}.zip\n",
        "    - Evaluation: results/evaluation.csv\n",
        "    \"\"\"\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "    # Initialize evaluation results\n",
        "    evaluation_results = []\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        \"ppo\": PPO,\n",
        "        \"sac\": SAC,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    # Create vectorized environment\n",
        "    def make_env():\n",
        "        return CustomPortfolioEnv(price_dir=price_dir, metrics_dir=metrics_dir)\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\nTraining with seed {seed}\")\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        # Create environment\n",
        "        env = make_vec_env(make_env, n_envs=1, seed=seed)\n",
        "\n",
        "        for model_name, model_class in models.items():\n",
        "            print(f\"Training {model_name.upper()}...\")\n",
        "\n",
        "            try:\n",
        "                # Initialize model\n",
        "                model = model_class(\n",
        "                    policy=\"MlpPolicy\",\n",
        "                    env=env,\n",
        "                    verbose=0,\n",
        "                    seed=seed\n",
        "                )\n",
        "\n",
        "                # Train model\n",
        "                model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
        "\n",
        "                # Evaluate model\n",
        "                mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5)\n",
        "\n",
        "                # Save model\n",
        "                model_path = os.path.join(output_dir, f\"{model_name}_seed_{seed}.zip\")\n",
        "                model.save(model_path)\n",
        "                print(f\"Saved model: {model_path}\")\n",
        "\n",
        "                # Store evaluation results\n",
        "                evaluation_results.append({\n",
        "                    \"model\": model_name,\n",
        "                    \"seed\": seed,\n",
        "                    \"mean_reward\": mean_reward,\n",
        "                    \"std_reward\": std_reward\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error training {model_name} with seed {seed}: {e}\")\n",
        "\n",
        "        # Reset environment seed for next iteration\n",
        "        env.reset()\n",
        "\n",
        "    # Save evaluation results\n",
        "    eval_df = pd.DataFrame(evaluation_results)\n",
        "    eval_path = os.path.join(output_dir, \"evaluation.csv\")\n",
        "    eval_df.to_csv(eval_path, index=False)\n",
        "    print(f\"Saved evaluation results: {eval_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_rl_agents(\n",
        "        price_dir=\"./metrics_used/price\",\n",
        "        metrics_dir=\"./observation/metrics\",\n",
        "        output_dir=\"./results\",\n",
        "        seeds=[1, 2, 3, 4, 5],\n",
        "        total_timesteps=10000\n",
        "    )"
      ],
      "metadata": {
        "id": "2WRnV93UhcpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomPortfolioEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Gymnasium environment for portfolio management backtesting.\n",
        "\n",
        "    - Observation: 189-dimensional vector from observation/metrics/{yyyy}_{mm}_combined.csv\n",
        "    - Action: 14-dimensional weight allocations for 14 assets (sum to 1)\n",
        "    - Reward: 2 * ROI - 0.7 * volatility - 0.5 * MDD, computed over the next month\n",
        "    \"\"\"\n",
        "    def __init__(self, price_dir=\"metrics_used/price\", metrics_dir=\"observation/metrics\", start_month=None, end_month=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define tickers\n",
        "        self.tickers = [\n",
        "            'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "            '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "        ]\n",
        "\n",
        "        # Load daily price data\n",
        "        price_file = os.path.join(price_dir, \"clean_data.csv\")\n",
        "        if not os.path.exists(price_file):\n",
        "            raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
        "        self.daily_prices = pd.read_csv(price_file, index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "        # Load monthly observations into a dictionary\n",
        "        months = pd.date_range(start=\"2003-01-01\", end=\"2024-12-31\", freq=\"ME\")\n",
        "        self.observations = {}\n",
        "        for month in months:\n",
        "            file_path = os.path.join(metrics_dir, f\"{month.year}_{month.month:02d}_combined.csv\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, header=None)\n",
        "                    if len(df) == 189:\n",
        "                        self.observations[month.strftime(\"%Y-%m\")] = df.iloc[:, 0].values.astype(np.float32)\n",
        "                    else:\n",
        "                        print(f\"Warning: {file_path} has {len(df)} rows, expected 189. Skipping.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "        # Get monthly last trading days\n",
        "        self.monthly_last_days = self.daily_prices.resample(\"ME\").last().index.tolist()\n",
        "        self.month_list = [d.strftime(\"%Y-%m\") for d in self.monthly_last_days]\n",
        "\n",
        "        # Define backtest period\n",
        "        self.backtest_months = pd.date_range(start=\"2003-01-01\", end=\"2024-11-01\", freq=\"ME\").strftime(\"%Y-%m\").tolist()\n",
        "        for month in self.backtest_months:\n",
        "            if month not in self.observations:\n",
        "                raise ValueError(f\"Observation file for {month} not found.\")\n",
        "\n",
        "        self.total_steps = len(self.backtest_months) - 1  # 83 steps for 84 months of performance\n",
        "\n",
        "        # Define spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(189,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        # Initialize state\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state (December 2017 for January 2018 allocations).\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[self.backtest_months[0]]\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take an action (weights) and compute the reward for the next month.\n",
        "        \"\"\"\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode is over\")\n",
        "\n",
        "        # Normalize action\n",
        "        action_sum = np.sum(action)\n",
        "        if action_sum > 0:\n",
        "            weights = action / action_sum\n",
        "        else:\n",
        "            weights = np.ones(14) / 14\n",
        "\n",
        "        # Get allocation month (the month for which these weights apply)\n",
        "        allocation_month = self.backtest_months[self.current_step + 1]\n",
        "\n",
        "        # Get start and end dates for the allocation month\n",
        "        start_date = pd.to_datetime(allocation_month + \"-01\")\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Get daily prices for the period\n",
        "        try:\n",
        "            daily_prices = self.daily_prices.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError as e:\n",
        "            print(f\"Warning: Missing price data for period {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute portfolio values\n",
        "        initial_prices = daily_prices.iloc[0].values\n",
        "        daily_portfolio_values = np.sum(daily_prices.values * weights, axis=1)\n",
        "\n",
        "        # Compute ROI\n",
        "        if daily_portfolio_values[0] > 0:\n",
        "            roi = (daily_portfolio_values[-1] / daily_portfolio_values[0]) - 1\n",
        "        else:\n",
        "            roi = 0\n",
        "\n",
        "        # Compute daily returns\n",
        "        daily_returns = daily_portfolio_values[1:] / daily_portfolio_values[:-1] - 1 if len(daily_portfolio_values) > 1 else np.array([0])\n",
        "\n",
        "        # Compute volatility\n",
        "        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Compute MDD\n",
        "        cummax = np.maximum.accumulate(daily_portfolio_values)\n",
        "        drawdown = (cummax - daily_portfolio_values) / cummax if cummax[0] > 0 else np.zeros_like(daily_portfolio_values)\n",
        "        mdd = np.max(drawdown) if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check done\n",
        "        done = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Get next observation\n",
        "        if not done:\n",
        "            obs = self.observations[self.backtest_months[self.current_step]]\n",
        "        else:\n",
        "            obs = np.zeros(189)\n",
        "\n",
        "        # Info\n",
        "        info = {\n",
        "            \"allocation_month\": allocation_month,\n",
        "            \"roi\": roi,\n",
        "            \"volatility\": volatility,\n",
        "            \"mdd\": mdd\n",
        "        }\n",
        "\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def backtest_model(model, env, output_file):\n",
        "    \"\"\"\n",
        "    Backtest a trained RL model over the specified period, recording monthly allocations and performance.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained Stable Baselines 3 model\n",
        "    env: CustomPortfolioEnv instance\n",
        "    output_file (str): Path to save backtest results CSV\n",
        "\n",
        "    Saves:\n",
        "    - CSV with columns: month, weight_{ticker}, roi, volatility, mdd, reward\n",
        "    \"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "    while not done:\n",
        "        # Predict action\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action\n",
        "        if np.sum(action) > 0:\n",
        "            weights = action / np.sum(action)\n",
        "        else:\n",
        "            weights = np.ones(len(env.tickers)) / len(env.tickers)\n",
        "        # Record allocation\n",
        "        record = {\"month\": env.backtest_months[env.current_step + 1]}\n",
        "        for i, ticker in enumerate(env.tickers):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        # Take step\n",
        "        obs, reward, done, truncated, info = env.step(weights)\n",
        "        # Update record\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "    # Save results\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Saved backtest results to {output_file}\")\n",
        "\n",
        "def run_backtests(price_dir=\"metrics_used/price\", metrics_dir=\"observation/metrics\",\n",
        "                 results_dir=\"results\", output_dir=\"backtest_results\", seeds=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Run backtests for all models and seeds over 2018-2024, saving monthly allocations and performance.\n",
        "\n",
        "    Parameters:\n",
        "    price_dir (str): Directory containing clean_data.csv\n",
        "    metrics_dir (str): Directory containing observation vectors\n",
        "    results_dir (str): Directory containing trained models\n",
        "    output_dir (str): Directory to save backtest results\n",
        "    seeds (list): List of random seeds\n",
        "\n",
        "    Saves:\n",
        "    - Backtest results: backtest_results/{model}/seed_{seed}.csv\n",
        "    \"\"\"\n",
        "    # Define model classes\n",
        "    model_classes = {\n",
        "        \"ppo\": PPO,\n",
        "        \"sac\": SAC,\n",
        "        \"ddpg\": DDPG,\n",
        "        \"td3\": TD3\n",
        "    }\n",
        "\n",
        "    # Run backtests\n",
        "    for model_type in model_classes:\n",
        "        model_dir = os.path.join(output_dir, model_type)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        for seed in seeds:\n",
        "            model_path = os.path.join(results_dir, f\"{model_type}_seed_{seed}.zip\")\n",
        "            if not os.path.exists(model_path):\n",
        "                print(f\"Model {model_path} not found. Skipping. Please re-run training for this seed.\")\n",
        "                continue\n",
        "            try:\n",
        "                model = model_classes[model_type].load(model_path)\n",
        "                env = CustomPortfolioEnv(\n",
        "                    price_dir=price_dir,\n",
        "                    metrics_dir=metrics_dir\n",
        "                )\n",
        "                output_file = os.path.join(model_dir, f\"seed_{seed}.csv\")\n",
        "                backtest_model(model, env, output_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error backtesting {model_type} seed {seed}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_backtests()"
      ],
      "metadata": {
        "id": "d0Lng-vWhgpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define directories and parameters\n",
        "PRICE_DIR = \"metrics_used/price\"\n",
        "METRICS_DIR = \"metrics_used/sim_nlp\"\n",
        "RESULTS_DIR = \"results_NLP\"\n",
        "BACKTEST_DIR = \"backtest_results_NLP\"\n",
        "SEEDS = [1, 2, 3, 4, 5]\n",
        "TOTAL_TIMESTEPS = 20000\n",
        "TRAIN_START_MONTH = \"2003-01\"\n",
        "TRAIN_END_MONTH = \"2017-11\"\n",
        "BACKTEST_START_MONTH = \"2003-01\"\n",
        "BACKTEST_END_MONTH = \"2024-11\"\n",
        "\n",
        "# Define the list of tickers (14 assets)\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomPortfolioEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for portfolio optimization with NLP vectors.\"\"\"\n",
        "    def __init__(self, price_dir, metrics_dir, obs_months):\n",
        "        super().__init__()\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Load daily price data\n",
        "        price_file = os.path.join(price_dir, \"clean_data.csv\")\n",
        "        if not os.path.exists(price_file):\n",
        "            raise FileNotFoundError(f\"Price file {price_file} not found.\")\n",
        "        self.daily_prices = pd.read_csv(price_file, index_col=\"Date\", parse_dates=True)\n",
        "\n",
        "        # Load observation vectors\n",
        "        self.observations = {}\n",
        "        for month in obs_months:\n",
        "            file_path = os.path.join(metrics_dir, f\"{month[:4]}{month[5:]}.csv\")\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    if df.shape[1] == 28:\n",
        "                        self.observations[month] = df.iloc[0].values.astype(np.float32)\n",
        "                    else:\n",
        "                        print(f\"Warning: {file_path} has {df.shape[1]} columns, expected 28.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"Warning: Observation file for {month} not found.\")\n",
        "\n",
        "        self.obs_months = obs_months\n",
        "        self.total_steps = len(obs_months) - 1  # Steps = number of observations - 1\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(28,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[self.obs_months[0]]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Perform one step: allocate weights and compute reward.\"\"\"\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize weights to sum to 1\n",
        "        action_sum = np.sum(action)\n",
        "        weights = action / action_sum if action_sum > 0 else np.ones(14) / 14\n",
        "\n",
        "        # Determine the allocation month (next month)\n",
        "        allocation_month = self.obs_months[self.current_step + 1]\n",
        "        start_date = pd.to_datetime(allocation_month + \"-01\")\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices for the allocation month\n",
        "        try:\n",
        "            daily_prices = self.daily_prices.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute portfolio performance\n",
        "        initial_prices = daily_prices.iloc[0].values\n",
        "        daily_values = np.sum(daily_prices.values * weights, axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values[-1] / daily_values[0] - 1) if daily_values[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values[1:] / daily_values[:-1] - 1 if len(daily_values) > 1 else np.array([0])\n",
        "        volatility = np.std(daily_returns) if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = np.maximum.accumulate(daily_values)\n",
        "        drawdown = (cummax - daily_values) / cummax if cummax[0] > 0 else np.zeros_like(daily_values)\n",
        "        mdd = np.max(drawdown) if len(daily_values) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.obs_months[self.current_step]] if not done else np.zeros(28)\n",
        "\n",
        "        info = {\"allocation_month\": allocation_month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, done, truncated, info\n",
        "\n",
        "def get_obs_months(start_month, end_month):\n",
        "    \"\"\"Generate a list of month strings between start and end months.\"\"\"\n",
        "    start = pd.to_datetime(start_month + \"-01\")\n",
        "    end = pd.to_datetime(end_month + \"-01\")\n",
        "    return pd.date_range(start=start, end=end, freq=\"ME\").strftime(\"%Y-%m\").tolist()\n",
        "\n",
        "def train_models():\n",
        "    \"\"\"Train RL models for each seed and save them.\"\"\"\n",
        "    obs_months = get_obs_months(TRAIN_START_MONTH, TRAIN_END_MONTH)\n",
        "    env = make_vec_env(lambda: CustomPortfolioEnv(PRICE_DIR, METRICS_DIR, obs_months), n_envs=1)\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    for seed in SEEDS:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        for model_name in [\"ppo\", \"sac\", \"ddpg\", \"td3\"]:\n",
        "            model_class = globals()[model_name.upper()]\n",
        "            model = model_class(\"MlpPolicy\", env, verbose=0, seed=seed)\n",
        "            model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
        "            model_path = os.path.join(RESULTS_DIR, f\"{model_name}_seed_{seed}.zip\")\n",
        "            model.save(model_path)\n",
        "            print(f\"Trained and saved {model_name} with seed {seed} to {model_path}\")\n",
        "\n",
        "def backtest_model(model, env, output_file):\n",
        "    \"\"\"Run backtest for a single model and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        weights = action / np.sum(action) if np.sum(action) > 0 else np.ones(len(TICKERS)) / len(TICKERS)\n",
        "        record = {\"month\": env.obs_months[env.current_step + 1]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\"roi\": info[\"roi\"], \"volatility\": info[\"volatility\"], \"mdd\": info[\"mdd\"], \"reward\": reward})\n",
        "        records.append(record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file}\")\n",
        "\n",
        "def run_backtests():\n",
        "    \"\"\"Backtest all trained models and save results.\"\"\"\n",
        "    obs_months = get_obs_months(BACKTEST_START_MONTH, BACKTEST_END_MONTH)\n",
        "    env = CustomPortfolioEnv(PRICE_DIR, METRICS_DIR, obs_months)\n",
        "\n",
        "    for model_type in [\"ppo\", \"sac\", \"ddpg\", \"td3\"]:\n",
        "        model_dir = os.path.join(BACKTEST_DIR, model_type)\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        for seed in SEEDS:\n",
        "            model_path = os.path.join(RESULTS_DIR, f\"{model_type}_seed_{seed}.zip\")\n",
        "            if not os.path.exists(model_path):\n",
        "                print(f\"Model {model_path} not found. Skipping.\")\n",
        "                continue\n",
        "            try:\n",
        "                model = globals()[model_type.upper()].load(model_path)\n",
        "                output_file = os.path.join(model_dir, f\"seed_{seed}.csv\")\n",
        "                backtest_model(model, env, output_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error backtesting {model_type} seed {seed}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting training phase...\")\n",
        "    train_models()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    run_backtests()\n",
        "    print(\"Pipeline completed.\")"
      ],
      "metadata": {
        "id": "uHFYWR2ehkOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta"
      ],
      "metadata": {
        "id": "NarZIK73iXlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def create_meta_folders():\n",
        "    \"\"\"\n",
        "    Create a Meta folder with two subfolders: NLP_obs and Metrics_obs.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Creates empty folders\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the folder paths\n",
        "        meta_dir = \"Meta\"\n",
        "        nlp_obs_dir = os.path.join(meta_dir, \"NLP_obs\")\n",
        "        metrics_obs_dir = os.path.join(meta_dir, \"Metrics_obs\")\n",
        "\n",
        "        # Create the Meta folder\n",
        "        os.makedirs(meta_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {meta_dir}\")\n",
        "\n",
        "        # Create the NLP_obs subfolder\n",
        "        os.makedirs(nlp_obs_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {nlp_obs_dir}\")\n",
        "\n",
        "        # Create the Metrics_obs subfolder\n",
        "        os.makedirs(metrics_obs_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {metrics_obs_dir}\")\n",
        "\n",
        "        print(\"Folder structure created successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating folders: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_meta_folders()"
      ],
      "metadata": {
        "id": "x9ukDp2zhmbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_backtest_results(backtest_dir=\"backtest_results_NLP\", output_dir=\"Meta/NLP_obs\", seeds=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Merge backtest CSV files for each agent in backtest_results_NLP, combining all seeds side by side.\n",
        "\n",
        "    Parameters:\n",
        "    backtest_dir (str): Directory containing backtest results (backtest_results_NLP)\n",
        "    output_dir (str): Directory to save merged CSV files (Meta/NLP_obs)\n",
        "    seeds (list): List of seeds to process\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV files in output_dir as {agent}_merged.csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents (subfolders in backtest_results_NLP)\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Process each agent\n",
        "        for agent in agents:\n",
        "            agent_dir = os.path.join(backtest_dir, agent)\n",
        "            if not os.path.exists(agent_dir):\n",
        "                print(f\"Agent directory {agent_dir} not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize list to hold DataFrames for merging\n",
        "            merged_dfs = []\n",
        "\n",
        "            # Process each seed\n",
        "            for seed in seeds:\n",
        "                csv_file = os.path.join(agent_dir, f\"seed_{seed}.csv\")\n",
        "                if not os.path.exists(csv_file):\n",
        "                    print(f\"CSV file {csv_file} not found. Skipping seed {seed} for agent {agent}.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Read the CSV file\n",
        "                    df = pd.read_csv(csv_file)\n",
        "\n",
        "                    # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                    expected_rows = 261\n",
        "                    if len(df) != expected_rows:\n",
        "                        print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Rename columns to include seed identifier (except for 'month' in the first seed)\n",
        "                    if seed == seeds[0]:\n",
        "                        # For the first seed, keep the 'month' column as is\n",
        "                        renamed_columns = {'month': 'month'}\n",
        "                        for col in df.columns[1:]:  # Skip 'month'\n",
        "                            renamed_columns[col] = f\"{col}_seed_{seed}\"\n",
        "                    else:\n",
        "                        # For other seeds, exclude 'month' and rename all columns\n",
        "                        renamed_columns = {col: f\"{col}_seed_{seed}\" for col in df.columns if col != 'month'}\n",
        "                        df = df.drop(columns=['month'])\n",
        "\n",
        "                    df = df.rename(columns=renamed_columns)\n",
        "                    merged_dfs.append(df)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {csv_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Check if any DataFrames were loaded\n",
        "            if not merged_dfs:\n",
        "                print(f\"No valid CSV files found for agent {agent}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "            merged_df = merged_dfs[0]\n",
        "            for df in merged_dfs[1:]:\n",
        "                merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "            # Verify the number of columns (should be 20 * number of seeds)\n",
        "            expected_columns = 1 + (20 * len(merged_dfs) - (len(merged_dfs) - 1))  # 1 for 'month', 19 additional per seed\n",
        "            if len(merged_df.columns) != expected_columns:\n",
        "                print(f\"Warning: Merged DataFrame for {agent} has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "            # Save the merged DataFrame\n",
        "            output_file = os.path.join(output_dir, f\"{agent}_merged.csv\")\n",
        "            merged_df.to_csv(output_file, index=False)\n",
        "            print(f\"Merged CSV for agent {agent} saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_backtest_results()"
      ],
      "metadata": {
        "id": "gNNL90I0hnXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_nlp_obs_results(input_dir=\"Meta/NLP_obs\", output_dir=\"Meta/NLP_obs\"):\n",
        "    \"\"\"\n",
        "    Merge the four agent-specific merged CSV files in Meta/NLP_obs into a single CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the agent-specific merged CSV files (Meta/NLP_obs)\n",
        "    output_dir (str): Directory to save the final merged CSV (Meta/NLP_obs)\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV file as nlp_obs_unclean.csv in output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Initialize list to hold DataFrames for merging\n",
        "        merged_dfs = []\n",
        "\n",
        "        # Process each agent's merged CSV\n",
        "        for agent in agents:\n",
        "            csv_file = os.path.join(input_dir, f\"{agent}_merged.csv\")\n",
        "            if not os.path.exists(csv_file):\n",
        "                print(f\"CSV file {csv_file} not found. Skipping agent {agent}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read the CSV file\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Rename columns to include agent identifier (except for 'month' in the first agent)\n",
        "                if agent == agents[0]:\n",
        "                    # For the first agent, keep the 'month' column as is\n",
        "                    renamed_columns = {'month': 'month'}\n",
        "                    for col in df.columns[1:]:  # Skip 'month'\n",
        "                        renamed_columns[col] = f\"{col}_{agent}\"\n",
        "                else:\n",
        "                    # For other agents, exclude 'month' and rename all columns\n",
        "                    renamed_columns = {col: f\"{col}_{agent}\" for col in df.columns if col != 'month'}\n",
        "                    df = df.drop(columns=['month'])\n",
        "\n",
        "                df = df.rename(columns=renamed_columns)\n",
        "                merged_dfs.append(df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {csv_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Check if any DataFrames were loaded\n",
        "        if not merged_dfs:\n",
        "            print(\"No valid CSV files found to merge.\")\n",
        "            return\n",
        "\n",
        "        # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "        merged_df = merged_dfs[0]\n",
        "        for df in merged_dfs[1:]:\n",
        "            merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (100 columns per agent × 4 agents))\n",
        "        expected_columns = 1 + (100 * len(merged_dfs))  # 1 for 'month', 100 columns per agent\n",
        "        if len(merged_df.columns) != expected_columns:\n",
        "            print(f\"Warning: Merged DataFrame has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Save the final merged DataFrame\n",
        "        output_file = os.path.join(output_dir, \"nlp_obs_unclean.csv\")\n",
        "        merged_df.to_csv(output_file, index=False)\n",
        "        print(f\"Final merged CSV saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_nlp_obs_results()"
      ],
      "metadata": {
        "id": "LZ1lh5PjhoUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def clean_nlp_obs_results(input_file=\"Meta/NLP_obs/nlp_obs_unclean.csv\", output_file=\"Meta/NLP_obs/nlp_obs_clean.csv\"):\n",
        "    \"\"\"\n",
        "    Clean the merged NLP observation CSV by keeping only the 'month' and weight columns.\n",
        "\n",
        "    Parameters:\n",
        "    input_file (str): Path to the input merged CSV file (nlp_obs_unclean.csv)\n",
        "    output_file (str): Path to save the cleaned CSV file (nlp_obs_clean.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Cleaned CSV file with only 'month' and weight columns in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the input CSV\n",
        "        if not os.path.exists(input_file):\n",
        "            raise FileNotFoundError(f\"Input file {input_file} not found.\")\n",
        "\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "        expected_rows = 263\n",
        "        if len(df) != expected_rows:\n",
        "            print(f\"Warning: {input_file} has {len(df)} rows, expected {expected_rows}.\")\n",
        "\n",
        "        # Expected number of columns (401: 1 month + 100 per agent × 4 agents)\n",
        "        expected_columns = 401\n",
        "        if len(df.columns) != expected_columns:\n",
        "            print(f\"Warning: {input_file} has {len(df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Identify columns to keep: 'month' and all columns containing 'weight'\n",
        "        columns_to_keep = ['month']\n",
        "        for col in df.columns:\n",
        "            if 'weight' in col:\n",
        "                columns_to_keep.append(col)\n",
        "\n",
        "        # Create the cleaned DataFrame\n",
        "        cleaned_df = df[columns_to_keep]\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (14 weights × 5 seeds × 4 agents) = 281)\n",
        "        expected_cleaned_columns = 1 + (14 * 5 * 4)  # 1 month + 14 weights × 5 seeds × 4 agents\n",
        "        if len(cleaned_df.columns) != expected_cleaned_columns:\n",
        "            print(f\"Warning: Cleaned DataFrame has {len(cleaned_df.columns)} columns, expected {expected_cleaned_columns}.\")\n",
        "\n",
        "        # Save the cleaned DataFrame\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        cleaned_df.to_csv(output_file, index=False)\n",
        "        print(f\"Cleaned CSV saved to {output_file} with {len(cleaned_df)} rows and {len(cleaned_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Cleaning process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cleaning process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_nlp_obs_results()"
      ],
      "metadata": {
        "id": "vMW45idIhpMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_backtest_results(backtest_dir=\"backtest_results\", output_dir=\"Meta/Metrics_obs\", seeds=[1, 2, 3, 4, 5]):\n",
        "    \"\"\"\n",
        "    Merge backtest CSV files for each agent in backtest_results_NLP, combining all seeds side by side.\n",
        "\n",
        "    Parameters:\n",
        "    backtest_dir (str): Directory containing backtest results (backtest_results_NLP)\n",
        "    output_dir (str): Directory to save merged CSV files (Meta/NLP_obs)\n",
        "    seeds (list): List of seeds to process\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV files in output_dir as {agent}_merged.csv\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents (subfolders in backtest_results_NLP)\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Process each agent\n",
        "        for agent in agents:\n",
        "            agent_dir = os.path.join(backtest_dir, agent)\n",
        "            if not os.path.exists(agent_dir):\n",
        "                print(f\"Agent directory {agent_dir} not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize list to hold DataFrames for merging\n",
        "            merged_dfs = []\n",
        "\n",
        "            # Process each seed\n",
        "            for seed in seeds:\n",
        "                csv_file = os.path.join(agent_dir, f\"seed_{seed}.csv\")\n",
        "                if not os.path.exists(csv_file):\n",
        "                    print(f\"CSV file {csv_file} not found. Skipping seed {seed} for agent {agent}.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Read the CSV file\n",
        "                    df = pd.read_csv(csv_file)\n",
        "\n",
        "                    # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                    expected_rows = 261\n",
        "                    if len(df) != expected_rows:\n",
        "                        print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # Rename columns to include seed identifier (except for 'month' in the first seed)\n",
        "                    if seed == seeds[0]:\n",
        "                        # For the first seed, keep the 'month' column as is\n",
        "                        renamed_columns = {'month': 'month'}\n",
        "                        for col in df.columns[1:]:  # Skip 'month'\n",
        "                            renamed_columns[col] = f\"{col}_seed_{seed}\"\n",
        "                    else:\n",
        "                        # For other seeds, exclude 'month' and rename all columns\n",
        "                        renamed_columns = {col: f\"{col}_seed_{seed}\" for col in df.columns if col != 'month'}\n",
        "                        df = df.drop(columns=['month'])\n",
        "\n",
        "                    df = df.rename(columns=renamed_columns)\n",
        "                    merged_dfs.append(df)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {csv_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Check if any DataFrames were loaded\n",
        "            if not merged_dfs:\n",
        "                print(f\"No valid CSV files found for agent {agent}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "            merged_df = merged_dfs[0]\n",
        "            for df in merged_dfs[1:]:\n",
        "                merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "            # Verify the number of columns (should be 20 * number of seeds)\n",
        "            expected_columns = 1 + (20 * len(merged_dfs) - (len(merged_dfs) - 1))  # 1 for 'month', 19 additional per seed\n",
        "            if len(merged_df.columns) != expected_columns:\n",
        "                print(f\"Warning: Merged DataFrame for {agent} has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "            # Save the merged DataFrame\n",
        "            output_file = os.path.join(output_dir, f\"{agent}_merged.csv\")\n",
        "            merged_df.to_csv(output_file, index=False)\n",
        "            print(f\"Merged CSV for agent {agent} saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_backtest_results()"
      ],
      "metadata": {
        "id": "dzM76GmnhqB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def merge_nlp_obs_results(input_dir=\"Meta/Metrics_obs\", output_dir=\"Meta/Metrics_obs\"):\n",
        "    \"\"\"\n",
        "    Merge the four agent-specific merged CSV files in Meta/NLP_obs into a single CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    input_dir (str): Directory containing the agent-specific merged CSV files (Meta/NLP_obs)\n",
        "    output_dir (str): Directory to save the final merged CSV (Meta/NLP_obs)\n",
        "\n",
        "    Saves:\n",
        "    - Merged CSV file as nlp_obs_unclean.csv in output_dir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified output directory: {output_dir}\")\n",
        "\n",
        "        # Define agents\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "\n",
        "        # Initialize list to hold DataFrames for merging\n",
        "        merged_dfs = []\n",
        "\n",
        "        # Process each agent's merged CSV\n",
        "        for agent in agents:\n",
        "            csv_file = os.path.join(input_dir, f\"{agent}_merged.csv\")\n",
        "            if not os.path.exists(csv_file):\n",
        "                print(f\"CSV file {csv_file} not found. Skipping agent {agent}.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read the CSV file\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Rename columns to include agent identifier (except for 'month' in the first agent)\n",
        "                if agent == agents[0]:\n",
        "                    # For the first agent, keep the 'month' column as is\n",
        "                    renamed_columns = {'month': 'month'}\n",
        "                    for col in df.columns[1:]:  # Skip 'month'\n",
        "                        renamed_columns[col] = f\"{col}_{agent}\"\n",
        "                else:\n",
        "                    # For other agents, exclude 'month' and rename all columns\n",
        "                    renamed_columns = {col: f\"{col}_{agent}\" for col in df.columns if col != 'month'}\n",
        "                    df = df.drop(columns=['month'])\n",
        "\n",
        "                df = df.rename(columns=renamed_columns)\n",
        "                merged_dfs.append(df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {csv_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Check if any DataFrames were loaded\n",
        "        if not merged_dfs:\n",
        "            print(\"No valid CSV files found to merge.\")\n",
        "            return\n",
        "\n",
        "        # Merge DataFrames side by side on 'month' (from the first DataFrame)\n",
        "        merged_df = merged_dfs[0]\n",
        "        for df in merged_dfs[1:]:\n",
        "            merged_df = pd.concat([merged_df, df], axis=1)\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (100 columns per agent × 4 agents))\n",
        "        expected_columns = 1 + (100 * len(merged_dfs))  # 1 for 'month', 100 columns per agent\n",
        "        if len(merged_df.columns) != expected_columns:\n",
        "            print(f\"Warning: Merged DataFrame has {len(merged_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Save the final merged DataFrame\n",
        "        output_file = os.path.join(output_dir, \"nlp_obs_unclean.csv\")\n",
        "        merged_df.to_csv(output_file, index=False)\n",
        "        print(f\"Final merged CSV saved to {output_file} with {len(merged_df)} rows and {len(merged_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Merging process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merging process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merge_nlp_obs_results()"
      ],
      "metadata": {
        "id": "TK2SgBbmhq1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def clean_nlp_obs_results(input_file=\"Meta/Metrics_obs/nlp_obs_unclean.csv\", output_file=\"Meta/Metrics_obs/metrics_obs_clean.csv\"):\n",
        "    \"\"\"\n",
        "    Clean the merged NLP observation CSV by keeping only the 'month' and weight columns.\n",
        "\n",
        "    Parameters:\n",
        "    input_file (str): Path to the input merged CSV file (nlp_obs_unclean.csv)\n",
        "    output_file (str): Path to save the cleaned CSV file (nlp_obs_clean.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Cleaned CSV file with only 'month' and weight columns in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the input CSV\n",
        "        if not os.path.exists(input_file):\n",
        "            raise FileNotFoundError(f\"Input file {input_file} not found.\")\n",
        "\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "        expected_rows = 263\n",
        "        if len(df) != expected_rows:\n",
        "            print(f\"Warning: {input_file} has {len(df)} rows, expected {expected_rows}.\")\n",
        "\n",
        "        # Expected number of columns (401: 1 month + 100 per agent × 4 agents)\n",
        "        expected_columns = 401\n",
        "        if len(df.columns) != expected_columns:\n",
        "            print(f\"Warning: {input_file} has {len(df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Identify columns to keep: 'month' and all columns containing 'weight'\n",
        "        columns_to_keep = ['month']\n",
        "        for col in df.columns:\n",
        "            if 'weight' in col:\n",
        "                columns_to_keep.append(col)\n",
        "\n",
        "        # Create the cleaned DataFrame\n",
        "        cleaned_df = df[columns_to_keep]\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (14 weights × 5 seeds × 4 agents) = 281)\n",
        "        expected_cleaned_columns = 1 + (14 * 5 * 4)  # 1 month + 14 weights × 5 seeds × 4 agents\n",
        "        if len(cleaned_df.columns) != expected_cleaned_columns:\n",
        "            print(f\"Warning: Cleaned DataFrame has {len(cleaned_df.columns)} columns, expected {expected_cleaned_columns}.\")\n",
        "\n",
        "        # Save the cleaned DataFrame\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        cleaned_df.to_csv(output_file, index=False)\n",
        "        print(f\"Cleaned CSV saved to {output_file} with {len(cleaned_df)} rows and {len(cleaned_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Cleaning process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cleaning process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_nlp_obs_results()"
      ],
      "metadata": {
        "id": "XiCoBCGJhrkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import os\n",
        "\n",
        "# Define tickers\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomMetaEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for meta-agent portfolio optimization.\"\"\"\n",
        "    def __init__(self, csv_path=\"Meta/Metrics_obs/metrics_obs_clean.csv\", price_path=\"metrics_used/price/clean_data.csv\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load observation data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.months = df['month'].tolist()\n",
        "        self.observations = df.iloc[:, 1:].astype(np.float32).values  # Shape: (263, 280)\n",
        "\n",
        "        # Load price data\n",
        "        self.price_data = pd.read_csv(price_path, index_col=\"Date\", parse_dates=True)\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Validate tickers\n",
        "        for ticker in self.tickers:\n",
        "            if ticker not in self.price_data.columns:\n",
        "                raise ValueError(f\"Ticker {ticker} not in price data\")\n",
        "\n",
        "        self.total_steps = len(self.months)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(280,), dtype=np.float32)\n",
        "        # Define action space with finite bounds\n",
        "        self.action_space = spaces.Box(low=-10, high=10, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "\n",
        "        # Get current month\n",
        "        month = self.months[self.current_step]\n",
        "\n",
        "        # Determine start and end dates\n",
        "        start_date = pd.to_datetime(month + '-01')\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices\n",
        "        try:\n",
        "            daily_prices = self.price_data.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute daily portfolio values\n",
        "        daily_values = (daily_prices * weights).sum(axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values.iloc[-1] / daily_values.iloc[0] - 1) if daily_values.iloc[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values.pct_change().dropna()\n",
        "        volatility = daily_returns.std() if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = daily_values.cummax()\n",
        "        drawdown = (cummax - daily_values) / cummax\n",
        "        mdd = drawdown.max() if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.current_step] if not terminated else np.zeros(280)\n",
        "\n",
        "        info = {\"allocation_month\": month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_meta_agent():\n",
        "    \"\"\"Train the meta-agent and save the model.\"\"\"\n",
        "    env = make_vec_env(lambda: CustomMetaEnv(), n_envs=1, seed=1)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={'net_arch': [256, 256, 256]},\n",
        "        verbose=1,\n",
        "        seed=1\n",
        "    )\n",
        "    model.learn(total_timesteps=20000)\n",
        "    os.makedirs(\"meta_results\", exist_ok=True)\n",
        "    model_path = \"meta_results/meta_agent_seed1\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Meta-agent trained and saved to {model_path}.zip\")\n",
        "    return model\n",
        "\n",
        "def backtest_meta_agent(model, env, output_file=\"Meta/Metrics_obs/backtest_meta_seed_1.csv\"):\n",
        "    \"\"\"Backtest the trained meta-agent and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "        record = {\"month\": env.months[env.current_step]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "\n",
        "    # Save backtest results\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file} with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the full pipeline: train and backtest the meta-agent.\"\"\"\n",
        "    print(\"Starting training phase...\")\n",
        "    model = train_meta_agent()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    env = CustomMetaEnv()\n",
        "    backtest_meta_agent(model, env)\n",
        "    print(\"Pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "ETtSxHuihsZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import os\n",
        "\n",
        "# Define tickers\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomMetaEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for meta-agent portfolio optimization.\"\"\"\n",
        "    def __init__(self, csv_path=\"Meta/NLP_obs/nlp_obs_clean.csv\", price_path=\"metrics_used/price/clean_data.csv\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load observation data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.months = df['month'].tolist()\n",
        "        self.observations = df.iloc[:, 1:].astype(np.float32).values  # Shape: (263, 280)\n",
        "\n",
        "        # Load price data\n",
        "        self.price_data = pd.read_csv(price_path, index_col=\"Date\", parse_dates=True)\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Validate tickers\n",
        "        for ticker in self.tickers:\n",
        "            if ticker not in self.price_data.columns:\n",
        "                raise ValueError(f\"Ticker {ticker} not in price data\")\n",
        "\n",
        "        self.total_steps = len(self.months)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(280,), dtype=np.float32)\n",
        "        # Define action space with finite bounds\n",
        "        self.action_space = spaces.Box(low=-10, high=10, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "\n",
        "        # Get current month\n",
        "        month = self.months[self.current_step]\n",
        "\n",
        "        # Determine start and end dates\n",
        "        start_date = pd.to_datetime(month + '-01')\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices\n",
        "        try:\n",
        "            daily_prices = self.price_data.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute daily portfolio values\n",
        "        daily_values = (daily_prices * weights).sum(axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values.iloc[-1] / daily_values.iloc[0] - 1) if daily_values.iloc[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values.pct_change().dropna()\n",
        "        volatility = daily_returns.std() if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = daily_values.cummax()\n",
        "        drawdown = (cummax - daily_values) / cummax\n",
        "        mdd = drawdown.max() if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.current_step] if not terminated else np.zeros(280)\n",
        "\n",
        "        info = {\"allocation_month\": month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_meta_agent():\n",
        "    \"\"\"Train the meta-agent and save the model.\"\"\"\n",
        "    env = make_vec_env(lambda: CustomMetaEnv(), n_envs=1, seed=1)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={'net_arch': [256, 256, 256]},\n",
        "        verbose=1,\n",
        "        seed=1\n",
        "    )\n",
        "    model.learn(total_timesteps=20000)\n",
        "    os.makedirs(\"meta_results\", exist_ok=True)\n",
        "    model_path = \"meta_results/meta_agent_seed1_nlp\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Meta-agent trained and saved to {model_path}.zip\")\n",
        "    return model\n",
        "\n",
        "def backtest_meta_agent(model, env, output_file=\"Meta/NLP_obs/backtest_meta_seed_1.csv\"):\n",
        "    \"\"\"Backtest the trained meta-agent and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "        record = {\"month\": env.months[env.current_step]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "\n",
        "    # Save backtest results\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file} with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the full pipeline: train and backtest the meta-agent.\"\"\"\n",
        "    print(\"Starting training phase...\")\n",
        "    model = train_meta_agent()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    env = CustomMetaEnv()\n",
        "    backtest_meta_agent(model, env)\n",
        "    print(\"Pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "zFDw-ZMnhtOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUPER"
      ],
      "metadata": {
        "id": "1m48ArnUioBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def create_super_folder():\n",
        "    \"\"\"\n",
        "    Create an empty SUPER folder.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Creates an empty SUPER folder\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the folder path\n",
        "        super_dir = \"SUPER\"\n",
        "\n",
        "        # Create the SUPER folder\n",
        "        os.makedirs(super_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {super_dir}\")\n",
        "\n",
        "        print(\"SUPER folder created successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating SUPER folder: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_super_folder()"
      ],
      "metadata": {
        "id": "27pvHObthuML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def copy_backtests_to_super():\n",
        "    \"\"\"\n",
        "    Copy backtest files from Meta/NLP_obs and Meta/Metrics_obs to the SUPER folder.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Copies files to the SUPER folder\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define source and destination directories\n",
        "        nlp_source_dir = \"Meta/NLP_obs\"\n",
        "        metrics_source_dir = \"Meta/Metrics_obs\"\n",
        "        dest_dir = \"SUPER\"\n",
        "\n",
        "        # Validate source directories\n",
        "        if not os.path.exists(nlp_source_dir):\n",
        "            print(f\"Source directory {nlp_source_dir} does not exist.\")\n",
        "            return\n",
        "        if not os.path.exists(metrics_source_dir):\n",
        "            print(f\"Source directory {metrics_source_dir} does not exist.\")\n",
        "            return\n",
        "\n",
        "        # Validate destination directory\n",
        "        if not os.path.exists(dest_dir):\n",
        "            print(f\"Destination directory {dest_dir} does not exist. Please create it first.\")\n",
        "            return\n",
        "\n",
        "        # Copy files from NLP_obs\n",
        "        for filename in os.listdir(nlp_source_dir):\n",
        "            if filename.endswith(\".csv\"):\n",
        "                src_path = os.path.join(nlp_source_dir, filename)\n",
        "                dest_filename = f\"nlp_{filename}\"\n",
        "                dest_path = os.path.join(dest_dir, dest_filename)\n",
        "                try:\n",
        "                    shutil.copy2(src_path, dest_path)\n",
        "                    print(f\"Copied {src_path} to {dest_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "        # Copy files from Metrics_obs\n",
        "        for filename in os.listdir(metrics_source_dir):\n",
        "            if filename.endswith(\".csv\") and \"backtest_meta_seed_1\" in filename:\n",
        "                src_path = os.path.join(metrics_source_dir, filename)\n",
        "                dest_filename = f\"metrics_{filename}\"\n",
        "                dest_path = os.path.join(dest_dir, dest_filename)\n",
        "                try:\n",
        "                    shutil.copy2(src_path, dest_path)\n",
        "                    print(f\"Copied {src_path} to {dest_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error copying {src_path} to {dest_path}: {e}\")\n",
        "\n",
        "        print(\"Copy process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during copy process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    copy_backtests_to_super()"
      ],
      "metadata": {
        "id": "j6yZxW6bhvAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files_to_delete = [\"SUPER/nlp_ddpg_merged.csv\", \"SUPER/nlp_nlp_obs_clean.csv\", \"SUPER/nlp_nlp_obs_unclean.csv\", \"SUPER/nlp_ppo_merged.csv\", \"SUPER/nlp_td3_merged.csv\", \"SUPER/nlp_sac_merged.csv\"]\n",
        "\n",
        "for file in files_to_delete:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "        print(f\"Deleted: {file}\")\n",
        "    else:\n",
        "        print(f\"Not found: {file}\")\n"
      ],
      "metadata": {
        "id": "GREuYoELhv8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def concatenate_super_backtests(super_dir=\"SUPER\", output_file=\"SUPER/super_weights_clean.csv\"):\n",
        "    \"\"\"\n",
        "    Concatenate the backtest CSV files in the SUPER folder, keeping only the 'month' and weight columns.\n",
        "\n",
        "    Parameters:\n",
        "    super_dir (str): Directory containing the backtest CSV files (SUPER)\n",
        "    output_file (str): Path to save the concatenated and cleaned CSV file (SUPER/super_weights_clean.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Concatenated CSV file with only 'month' and weight columns in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the input files\n",
        "        nlp_file = os.path.join(super_dir, \"nlp_backtest_meta_seed_1.csv\")\n",
        "        metrics_file = os.path.join(super_dir, \"metrics_backtest_meta_seed_1.csv\")\n",
        "\n",
        "        # Validate input files\n",
        "        if not os.path.exists(nlp_file):\n",
        "            raise FileNotFoundError(f\"NLP backtest file {nlp_file} not found.\")\n",
        "        if not os.path.exists(metrics_file):\n",
        "            raise FileNotFoundError(f\"Metrics backtest file {metrics_file} not found.\")\n",
        "\n",
        "        # Read the CSV files\n",
        "        nlp_df = pd.read_csv(nlp_file)\n",
        "        metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "        # Expected number of rows (263 months: Feb 2003 to Dec 2024)\n",
        "        expected_rows = 263\n",
        "        if len(nlp_df) != expected_rows or len(metrics_df) != expected_rows:\n",
        "            print(f\"Warning: Expected {expected_rows} rows. NLP has {len(nlp_df)}, Metrics has {len(metrics_df)}.\")\n",
        "\n",
        "        # Keep only 'month' and weight columns\n",
        "        weight_cols = [col for col in nlp_df.columns if col.startswith(\"weight_\")]\n",
        "        nlp_df = nlp_df[['month'] + weight_cols]\n",
        "        metrics_df = metrics_df[['month'] + weight_cols]\n",
        "\n",
        "        # Rename weight columns to include source identifier\n",
        "        nlp_df = nlp_df.rename(columns={col: f\"{col}_nlp\" for col in weight_cols})\n",
        "        metrics_df = metrics_df.rename(columns={col: f\"{col}_metrics\" for col in weight_cols})\n",
        "\n",
        "        # Drop 'month' from metrics_df to avoid duplication\n",
        "        metrics_df = metrics_df.drop(columns=['month'])\n",
        "\n",
        "        # Concatenate side by side on 'month'\n",
        "        concatenated_df = pd.concat([nlp_df, metrics_df], axis=1)\n",
        "\n",
        "        # Verify the number of columns (should be 1 + (14 weights × 2 sources) = 29)\n",
        "        expected_columns = 1 + (14 * 2)\n",
        "        if len(concatenated_df.columns) != expected_columns:\n",
        "            print(f\"Warning: Concatenated DataFrame has {len(concatenated_df.columns)} columns, expected {expected_columns}.\")\n",
        "\n",
        "        # Save the concatenated DataFrame\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        concatenated_df.to_csv(output_file, index=False)\n",
        "        print(f\"Concatenated and cleaned CSV saved to {output_file} with {len(concatenated_df)} rows and {len(concatenated_df.columns)} columns.\")\n",
        "\n",
        "        print(\"Concatenation process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during concatenation process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    concatenate_super_backtests()"
      ],
      "metadata": {
        "id": "ba_yXJFZhwzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import os\n",
        "\n",
        "# Define tickers\n",
        "TICKERS = [\n",
        "    'GC=F', 'SI=F', '^DJI', '^IXIC', 'CL=F', '^GSPC', '^STOXX50E',\n",
        "    '^FCHI', '^FTSE', '^HSI', '000001.SS', '^KS11', '^BSESN', '^NSEI'\n",
        "]\n",
        "\n",
        "class CustomSuperMetaEnv(gym.Env):\n",
        "    \"\"\"Custom Gym environment for super meta-agent portfolio optimization.\"\"\"\n",
        "    def __init__(self, csv_path=\"SUPER/super_weights_clean.csv\", price_path=\"metrics_used/price/clean_data.csv\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load observation data\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.months = df['month'].tolist()\n",
        "        self.observations = df.iloc[:, 1:].astype(np.float32).values  # Shape: (263, 28)\n",
        "\n",
        "        # Load price data\n",
        "        self.price_data = pd.read_csv(price_path, index_col=\"Date\", parse_dates=True)\n",
        "        self.tickers = TICKERS\n",
        "\n",
        "        # Validate tickers\n",
        "        for ticker in self.tickers:\n",
        "            if ticker not in self.price_data.columns:\n",
        "                raise ValueError(f\"Ticker {ticker} not in price data\")\n",
        "\n",
        "        self.total_steps = len(self.months)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(28,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=-10, high=10, shape=(14,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        self.current_step = 0\n",
        "        obs = self.observations[0]\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.total_steps:\n",
        "            raise ValueError(\"Episode has ended.\")\n",
        "\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "\n",
        "        # Get current month\n",
        "        month = self.months[self.current_step]\n",
        "\n",
        "        # Determine start and end dates\n",
        "        start_date = pd.to_datetime(month + '-01')\n",
        "        end_date = start_date + pd.offsets.MonthEnd(0)\n",
        "\n",
        "        # Extract daily prices\n",
        "        try:\n",
        "            daily_prices = self.price_data.loc[start_date:end_date, self.tickers]\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Missing price data for {start_date} to {end_date}. Using zeros.\")\n",
        "            daily_prices = pd.DataFrame(0, index=pd.date_range(start_date, end_date), columns=self.tickers)\n",
        "\n",
        "        # Compute daily portfolio values\n",
        "        daily_values = (daily_prices * weights).sum(axis=1)\n",
        "\n",
        "        # Calculate ROI\n",
        "        roi = (daily_values.iloc[-1] / daily_values.iloc[0] - 1) if daily_values.iloc[0] > 0 else 0\n",
        "\n",
        "        # Calculate daily returns and volatility\n",
        "        daily_returns = daily_values.pct_change().dropna()\n",
        "        volatility = daily_returns.std() if len(daily_returns) > 0 else 0\n",
        "\n",
        "        # Calculate Maximum Drawdown (MDD)\n",
        "        cummax = daily_values.cummax()\n",
        "        drawdown = (cummax - daily_values) / cummax\n",
        "        mdd = drawdown.max() if len(drawdown) > 0 else 0\n",
        "\n",
        "        # Compute reward\n",
        "        reward = 2 * roi - 0.7 * volatility - 0.5 * mdd\n",
        "\n",
        "        # Advance step\n",
        "        self.current_step += 1\n",
        "        terminated = self.current_step >= self.total_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Next observation\n",
        "        next_obs = self.observations[self.current_step] if not terminated else np.zeros(28)\n",
        "\n",
        "        info = {\"allocation_month\": month, \"roi\": roi, \"volatility\": volatility, \"mdd\": mdd}\n",
        "        return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "def train_super_meta_agent():\n",
        "    \"\"\"Train the super meta-agent and save the model.\"\"\"\n",
        "    env = make_vec_env(lambda: CustomSuperMetaEnv(), n_envs=1, seed=1)\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={'net_arch': [256, 256, 256]},\n",
        "        verbose=1,\n",
        "        seed=1\n",
        "    )\n",
        "    model.learn(total_timesteps=1000)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    model_path = \"results/super_meta_agent_seed1\"\n",
        "    model.save(model_path)\n",
        "    print(f\"Super meta-agent trained and saved to {model_path}.zip\")\n",
        "    return model\n",
        "\n",
        "def backtest_super_meta_agent(model, env, output_file=\"SUPER/backtest_super_meta_seed_1.csv\"):\n",
        "    \"\"\"Backtest the trained super meta-agent and save results.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    records = []\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        # Normalize action to sum to 1 using softmax\n",
        "        weights = np.exp(action) / np.sum(np.exp(action))\n",
        "        record = {\"month\": env.months[env.current_step]}\n",
        "        for i, ticker in enumerate(TICKERS):\n",
        "            record[f\"weight_{ticker}\"] = weights[i]\n",
        "        obs, reward, done, _, info = env.step(action)\n",
        "        record.update({\n",
        "            \"roi\": info[\"roi\"],\n",
        "            \"volatility\": info[\"volatility\"],\n",
        "            \"mdd\": info[\"mdd\"],\n",
        "            \"reward\": reward\n",
        "        })\n",
        "        records.append(record)\n",
        "\n",
        "    # Save backtest results\n",
        "    df = pd.DataFrame(records)\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Backtest results saved to {output_file} with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"Run the full pipeline: train and backtest the super meta-agent.\"\"\"\n",
        "    print(\"Starting training phase...\")\n",
        "    model = train_super_meta_agent()\n",
        "    print(\"\\nStarting backtesting phase...\")\n",
        "    env = CustomSuperMetaEnv()\n",
        "    backtest_super_meta_agent(model, env)\n",
        "    print(\"Pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "VEJOA4fZhx4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def organize_all_backtests():\n",
        "    \"\"\"\n",
        "    Create the ALL_BACKTEST folder with subfolders Meta, Super, Metrics, NLP,\n",
        "    and copy all backtest CSV files into the appropriate subfolders with consistent structure.\n",
        "\n",
        "    Parameters:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    None: Creates the folder structure and copies files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define the main directory and subfolders\n",
        "        all_backtest_dir = \"ALL_BACKTEST\"\n",
        "        subfolders = [\"Meta\", \"Super\", \"Metrics\", \"NLP\"]\n",
        "\n",
        "        # Create the main directory and subfolders\n",
        "        os.makedirs(all_backtest_dir, exist_ok=True)\n",
        "        print(f\"Created/Verified folder: {all_backtest_dir}\")\n",
        "\n",
        "        for subfolder in subfolders:\n",
        "            subfolder_path = os.path.join(all_backtest_dir, subfolder)\n",
        "            os.makedirs(subfolder_path, exist_ok=True)\n",
        "            print(f\"Created/Verified folder: {subfolder_path}\")\n",
        "\n",
        "        # Define agents and seeds for Metrics and NLP pipelines\n",
        "        agents = [\"ppo\", \"sac\", \"ddpg\", \"td3\"]\n",
        "        seeds = [1, 2, 3, 4, 5]\n",
        "\n",
        "        # Copy Metrics pipeline files\n",
        "        metrics_source_dir = \"backtest_results\"\n",
        "        if not os.path.exists(metrics_source_dir):\n",
        "            print(f\"Metrics source directory {metrics_source_dir} does not exist.\")\n",
        "        else:\n",
        "            for agent in agents:\n",
        "                agent_source_dir = os.path.join(metrics_source_dir, agent)\n",
        "                agent_dest_dir = os.path.join(all_backtest_dir, \"Metrics\", agent)\n",
        "                os.makedirs(agent_dest_dir, exist_ok=True)\n",
        "                for seed in seeds:\n",
        "                    src_file = os.path.join(agent_source_dir, f\"seed_{seed}.csv\")\n",
        "                    if os.path.exists(src_file):\n",
        "                        dest_file = os.path.join(agent_dest_dir, f\"seed_{seed}.csv\")\n",
        "                        shutil.copy2(src_file, dest_file)\n",
        "                        print(f\"Copied {src_file} to {dest_file}\")\n",
        "                    else:\n",
        "                        print(f\"File {src_file} does not exist. Skipping.\")\n",
        "\n",
        "        # Copy NLP pipeline files\n",
        "        nlp_source_dir = \"backtest_results_NLP\"\n",
        "        if not os.path.exists(nlp_source_dir):\n",
        "            print(f\"NLP source directory {nlp_source_dir} does not exist.\")\n",
        "        else:\n",
        "            for agent in agents:\n",
        "                agent_source_dir = os.path.join(nlp_source_dir, agent)\n",
        "                agent_dest_dir = os.path.join(all_backtest_dir, \"NLP\", agent)\n",
        "                os.makedirs(agent_dest_dir, exist_ok=True)\n",
        "                for seed in seeds:\n",
        "                    src_file = os.path.join(agent_source_dir, f\"seed_{seed}.csv\")\n",
        "                    if os.path.exists(src_file):\n",
        "                        dest_file = os.path.join(agent_dest_dir, f\"seed_{seed}.csv\")\n",
        "                        shutil.copy2(src_file, dest_file)\n",
        "                        print(f\"Copied {src_file} to {dest_file}\")\n",
        "                    else:\n",
        "                        print(f\"File {src_file} does not exist. Skipping.\")\n",
        "\n",
        "        # Copy Meta pipeline file\n",
        "        meta_source_dir = \"Meta/Metrics_obs\"\n",
        "        meta_file = os.path.join(meta_source_dir, \"backtest_meta_seed_1.csv\")\n",
        "        meta_dest_dir = os.path.join(all_backtest_dir, \"Meta\", \"meta\")\n",
        "        os.makedirs(meta_dest_dir, exist_ok=True)\n",
        "        if os.path.exists(meta_file):\n",
        "            dest_file = os.path.join(meta_dest_dir, \"seed_1.csv\")\n",
        "            shutil.copy2(meta_file, dest_file)\n",
        "            print(f\"Copied {meta_file} to {dest_file}\")\n",
        "        else:\n",
        "            print(f\"Meta backtest file {meta_file} does not exist. Skipping.\")\n",
        "\n",
        "                # Copy Meta pipeline file\n",
        "        meta_source_dir = \"Meta/NLP_obs\"\n",
        "        meta_file = os.path.join(meta_source_dir, \"backtest_meta_seed_1.csv\")\n",
        "        meta_dest_dir = os.path.join(all_backtest_dir, \"Meta\", \"meta\")\n",
        "        os.makedirs(meta_dest_dir, exist_ok=True)\n",
        "        if os.path.exists(meta_file):\n",
        "            dest_file = os.path.join(meta_dest_dir, \"seed_1_nlp.csv\")\n",
        "            shutil.copy2(meta_file, dest_file)\n",
        "            print(f\"Copied {meta_file} to {dest_file}\")\n",
        "        else:\n",
        "            print(f\"Meta backtest file {meta_file} does not exist. Skipping.\")\n",
        "\n",
        "        # Copy Super pipeline file\n",
        "        super_source_dir = \"SUPER\"\n",
        "        super_file = os.path.join(super_source_dir, \"backtest_super_meta_seed_1.csv\")\n",
        "        super_dest_dir = os.path.join(all_backtest_dir, \"Super\", \"super_meta\")\n",
        "        os.makedirs(super_dest_dir, exist_ok=True)\n",
        "        if os.path.exists(super_file):\n",
        "            dest_file = os.path.join(super_dest_dir, \"seed_1.csv\")\n",
        "            shutil.copy2(super_file, dest_file)\n",
        "            print(f\"Copied {super_file} to {dest_file}\")\n",
        "        else:\n",
        "            print(f\"Super backtest file {super_file} does not exist. Skipping.\")\n",
        "\n",
        "        print(\"Backtest organization process completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during backtest organization process: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    organize_all_backtests()"
      ],
      "metadata": {
        "id": "2xQabtGThyzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def create_roi_matrix(all_backtest_dir=\"ALL_BACKTEST\", output_file=\"ALL_BACKTEST/ROI.csv\"):\n",
        "    \"\"\"\n",
        "    Create a matrix with rows as months and columns as models and seeds,\n",
        "    where each cell contains the ROI of the model at that month.\n",
        "\n",
        "    Parameters:\n",
        "    all_backtest_dir (str): Directory containing the backtest files (ALL_BACKTEST)\n",
        "    output_file (str): Path to save the ROI matrix (SUPER/ROI.csv)\n",
        "\n",
        "    Saves:\n",
        "    - ROI matrix as a CSV file in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define all files and their identifiers\n",
        "        all_files = [\n",
        "            # Metrics\n",
        "            (f\"{all_backtest_dir}/Metrics/{agent}/seed_{seed}.csv\", f\"Metrics_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1,2,3,4,5]\n",
        "        ] + [\n",
        "            # NLP\n",
        "            (f\"{all_backtest_dir}/NLP/{agent}/seed_{seed}.csv\", f\"NLP_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1,2,3,4,5]\n",
        "        ] + [\n",
        "            # Meta Metrics\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1.csv\", \"Meta_meta_metrics_1\"),\n",
        "            # Meta NLP\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1_nlp.csv\", \"Meta_meta_nlp_1\"),\n",
        "            # Super\n",
        "            (f\"{all_backtest_dir}/Super/super_meta/seed_1.csv\", \"Super_super_meta_1\")\n",
        "        ]\n",
        "\n",
        "        # Initialize list for DataFrames\n",
        "        dfs = []\n",
        "\n",
        "        # Read each file and add model_seed column\n",
        "        for file_path, identifier in all_files:\n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                # Validate the number of rows\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {file_path} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "                # Validate the presence of required columns\n",
        "                if 'month' not in df.columns or 'roi' not in df.columns:\n",
        "                    print(f\"Warning: 'month' or 'roi' column not found in {file_path}. Skipping.\")\n",
        "                    continue\n",
        "                df['model_seed'] = identifier\n",
        "                df = df[['month', 'roi', 'model_seed']]\n",
        "                dfs.append(df)\n",
        "            else:\n",
        "                print(f\"File {file_path} does not exist. Skipping.\")\n",
        "\n",
        "        # Concatenate all DataFrames\n",
        "        if dfs:\n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        else:\n",
        "            print(\"No dataframes to concatenate. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Pivot the combined DataFrame\n",
        "        roi_matrix = combined_df.pivot(index='month', columns='model_seed', values='roi')\n",
        "\n",
        "        # Sort the index (months)\n",
        "        roi_matrix = roi_matrix.sort_index()\n",
        "\n",
        "        # Save to CSV\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        roi_matrix.to_csv(output_file)\n",
        "        print(f\"ROI matrix saved to {output_file} with {len(roi_matrix)} rows and {len(roi_matrix.columns)} columns.\")\n",
        "\n",
        "        print(\"ROI matrix creation completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ROI matrix creation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_roi_matrix()"
      ],
      "metadata": {
        "id": "r3DFzPtxhzxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_portfolio_value_matrix(all_backtest_dir=\"ALL_BACKTEST\", output_file=\"ALL_BACKTEST/portfolio_value.csv\"):\n",
        "    \"\"\"\n",
        "    Create a matrix with rows as months and columns as models and seeds,\n",
        "    where each cell contains the cumulative portfolio value of the model at that month.\n",
        "\n",
        "    Parameters:\n",
        "    all_backtest_dir (str): Directory containing the backtest files (ALL_BACKTEST)\n",
        "    output_file (str): Path to save the portfolio value matrix (SUPER/portfolio_value.csv)\n",
        "\n",
        "    Saves:\n",
        "    - Portfolio value matrix as a CSV file in output_file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define all files and their identifiers\n",
        "        all_files = [\n",
        "            # Metrics\n",
        "            (f\"{all_backtest_dir}/Metrics/{agent}/seed_{seed}.csv\", f\"Metrics_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1, 2, 3, 4, 5]\n",
        "        ] + [\n",
        "            # NLP\n",
        "            (f\"{all_backtest_dir}/NLP/{agent}/seed_{seed}.csv\", f\"NLP_{agent}_{seed}\") for agent in [\"ppo\", \"sac\", \"ddpg\", \"td3\"] for seed in [1, 2, 3, 4, 5]\n",
        "        ] + [\n",
        "            # Meta Metrics\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1.csv\", \"Meta_meta_metrics_1\"),\n",
        "            # Meta NLP\n",
        "            (f\"{all_backtest_dir}/Meta/meta/seed_1_nlp.csv\", \"Meta_meta_nlp_1\"),\n",
        "            # Super\n",
        "            (f\"{all_backtest_dir}/Super/super_meta/seed_1.csv\", \"Super_super_meta_1\")\n",
        "        ]\n",
        "\n",
        "        # Initialize list for DataFrames\n",
        "        dfs = []\n",
        "\n",
        "        # Read each file and compute cumulative ROI\n",
        "        for file_path, identifier in all_files:\n",
        "            if os.path.exists(file_path):\n",
        "                df = pd.read_csv(file_path)\n",
        "                # Validate the number of rows\n",
        "                expected_rows = 261\n",
        "                if len(df) != expected_rows:\n",
        "                    print(f\"Warning: {file_path} has {len(df)} rows, expected {expected_rows}. Skipping.\")\n",
        "                    continue\n",
        "                # Validate the presence of required columns\n",
        "                if 'month' not in df.columns or 'roi' not in df.columns:\n",
        "                    print(f\"Warning: 'month' or 'roi' column not found in {file_path}. Skipping.\")\n",
        "                    continue\n",
        "                # Compute cumulative ROI\n",
        "                df['cumulative_roi'] = (1 + df['roi']).cumprod()\n",
        "                df['model_seed'] = identifier\n",
        "                dfs.append(df[['month', 'cumulative_roi', 'model_seed']])\n",
        "            else:\n",
        "                print(f\"File {file_path} does not exist. Skipping.\")\n",
        "\n",
        "        # Concatenate all DataFrames\n",
        "        if dfs:\n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        else:\n",
        "            print(\"No dataframes to concatenate. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Pivot the combined DataFrame\n",
        "        portfolio_value_matrix = combined_df.pivot(index='month', columns='model_seed', values='cumulative_roi')\n",
        "\n",
        "        # Sort the index (months)\n",
        "        portfolio_value_matrix = portfolio_value_matrix.sort_index()\n",
        "\n",
        "        # Save to CSV\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        portfolio_value_matrix.to_csv(output_file)\n",
        "        print(f\"Portfolio value matrix saved to {output_file} with {len(portfolio_value_matrix)} rows and {len(portfolio_value_matrix.columns)} columns.\")\n",
        "\n",
        "        print(\"Portfolio value matrix creation completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during portfolio value matrix creation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_portfolio_value_matrix()"
      ],
      "metadata": {
        "id": "SiFFLODZh0jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_portfolio_value_evolution(csv_file=\"ALL_BACKTEST/portfolio_value.csv\", output_file=\"ALL_BACKTEST/portfolio_value_evolution.png\"):\n",
        "    \"\"\"\n",
        "    Plot the portfolio value evolution for all model-seed combinations in a single graph.\n",
        "\n",
        "    Parameters:\n",
        "    csv_file (str): Path to the portfolio value CSV file\n",
        "    output_file (str): Path to save the plot image\n",
        "\n",
        "    Saves:\n",
        "    - Plot image as a PNG file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the CSV file exists\n",
        "        if not os.path.exists(csv_file):\n",
        "            raise FileNotFoundError(f\"The file {csv_file} does not exist. Please generate it using create_portfolio_value_matrix.py.\")\n",
        "\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(csv_file, index_col=\"month\")\n",
        "\n",
        "        # Validate the number of rows\n",
        "        expected_rows = 263\n",
        "        if len(df) != expected_rows:\n",
        "            print(f\"Warning: {csv_file} has {len(df)} rows, expected {expected_rows}.\")\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        for column in df.columns:\n",
        "            plt.plot(df.index, df[column], label=column)\n",
        "\n",
        "        # Customize the plot\n",
        "        plt.xlabel(\"Month\")\n",
        "        plt.ylabel(\"Portfolio Value\")\n",
        "        plt.title(\"Portfolio Value Evolution for All Models and Seeds\")\n",
        "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        plt.savefig(output_file)\n",
        "        print(f\"Plot saved to {output_file}\")\n",
        "\n",
        "        # Show the plot (optional, for interactive environments)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Plotting completed successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during plotting: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    plot_portfolio_value_evolution()"
      ],
      "metadata": {
        "id": "u3ZbUL66h1ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_model_seeds(df, models, title, output_file):\n",
        "    \"\"\"\n",
        "    Plot portfolio value evolution for specified models and seeds.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for model in models:\n",
        "        model_columns = [col for col in df.columns if col.startswith(model)]\n",
        "        for column in model_columns:\n",
        "            plt.plot(df.index, df[column], label=column)\n",
        "\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Portfolio Value\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {output_file}\")\n",
        "\n",
        "def plot_meta_super(df, title, output_file):\n",
        "    \"\"\"\n",
        "    Plot portfolio value evolution for meta and super agents.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    meta_super_columns = [col for col in df.columns if col.startswith('Meta_') or col.startswith('Super_')]\n",
        "    for column in meta_super_columns:\n",
        "        plt.plot(df.index, df[column], label=column)\n",
        "\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Portfolio Value\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=8)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {output_file}\")\n",
        "\n",
        "def create_plots():\n",
        "    \"\"\"\n",
        "    Create and save plots for portfolio value evolution in specified time periods,\n",
        "    with 2018-2024 plots normalized to start at 1 in January 2018.\n",
        "    \"\"\"\n",
        "    # Define directories\n",
        "    plot_dir = \"ALL_BACKTEST/plot\"\n",
        "    full_period_dir = os.path.join(plot_dir, \"2003-2024\")\n",
        "    recent_period_dir = os.path.join(plot_dir, \"2018-2024\")\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(full_period_dir, exist_ok=True)\n",
        "    os.makedirs(recent_period_dir, exist_ok=True)\n",
        "\n",
        "    # Load the portfolio value data\n",
        "    csv_file = \"SUPER/portfolio_value.csv\"\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise FileNotFoundError(f\"File {csv_file} not found.\")\n",
        "\n",
        "    df = pd.read_csv(csv_file, index_col=\"month\")\n",
        "\n",
        "    # Verify that '2018-01' exists in the index\n",
        "    if '2018-01' not in df.index:\n",
        "        raise ValueError(\"The date '2018-01' is not found in the data.\")\n",
        "\n",
        "    # Filter data for the 2018-2024 period\n",
        "    df_recent = df.loc['2018-01':].copy()\n",
        "\n",
        "    # Normalize the 2018-2024 data to start at 1 in January 2018\n",
        "    start_values = df_recent.iloc[0]  # Values at '2018-01'\n",
        "    normalized_df = df_recent / start_values  # Divide all values by January 2018 values\n",
        "\n",
        "    # Define models to plot\n",
        "    models = [\"ppo\", \"sac\", \"td3\", \"ddpg\"]\n",
        "\n",
        "    # Generate plots for the full period (2003-2024)\n",
        "    for model in models:\n",
        "        plot_model_seeds(\n",
        "            df,\n",
        "            [f\"Metrics_{model}\", f\"NLP_{model}\"],\n",
        "            f\"Portfolio Value Evolution - {model.upper()} Seeds (2003-2024)\",\n",
        "            os.path.join(full_period_dir, f\"{model}_seeds_2003-2024.png\")\n",
        "        )\n",
        "\n",
        "    plot_meta_super(\n",
        "        df,\n",
        "        \"Portfolio Value Evolution - Meta and Super Agents (2003-2024)\",\n",
        "        os.path.join(full_period_dir, \"meta_super_2003-2024.png\")\n",
        "    )\n",
        "\n",
        "    # Generate normalized plots for the recent period (2018-2024)\n",
        "    for model in models:\n",
        "        plot_model_seeds(\n",
        "            normalized_df,\n",
        "            [f\"Metrics_{model}\", f\"NLP_{model}\"],\n",
        "            f\"Portfolio Value Evolution - {model.upper()} Seeds (2018-2024, Normalized to Start at 1 in Jan 2018)\",\n",
        "            os.path.join(recent_period_dir, f\"{model}_seeds_2018-2024.png\")\n",
        "        )\n",
        "\n",
        "    plot_meta_super(\n",
        "        normalized_df,\n",
        "        \"Portfolio Value Evolution - Meta and Super Agents (2018-2024, Normalized to Start at 1 in Jan 2018)\",\n",
        "        os.path.join(recent_period_dir, \"meta_super_2018-2024.png\")\n",
        "    )\n",
        "\n",
        "    print(\"All plots generated successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        create_plots()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "lm1bbUYfh2Qp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}