{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YvPVg336VXD_"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import logging\n",
        "import os\n",
        "# Utility Function: Delete all files and directories in a specified directory except for one file\n",
        "def delete_all_except(directory: str, file_to_keep: str):\n",
        "    \"\"\"\n",
        "    Deletes all files and directories in the specified directory except for the specified file.\n",
        "\n",
        "    :param directory: Path to the directory.\n",
        "    :param file_to_keep: Filename to preserve.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        logging.warning(f\"Directory '{directory}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename == file_to_keep:\n",
        "            logging.info(f\"Preserving file: {file_path}\")\n",
        "            continue\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.remove(file_path)\n",
        "                logging.info(f\"Deleted file: {file_path}\")\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "                logging.info(f\"Deleted directory and its contents: {file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to delete {file_path}. Reason: {e}\")\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "current_directory\n",
        "file_to_preserve = \"google_news_results\"\n",
        "delete_all_except(current_directory, file_to_preserve)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance pandas numpy matplotlib seaborn gym stable-baselines3 shimmy torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gurVaZSKVhNU",
        "outputId": "c80d37b0-86a1-479b-a6dc-31287e8a2d2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.61)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.4)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 shimmy-2.0.0 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib.parse import urlencode\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "import shutil\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_google_news_link(query, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Generate a Google News search URL with custom date range.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The search query.\n",
        "        start_date (str): Start date in MM/DD/YYYY format.\n",
        "        end_date (str): End date in MM/DD/YYYY format.\n",
        "\n",
        "    Returns:\n",
        "        str: Google News search URL.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.google.com/search\"\n",
        "\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"tbs\": f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\",\n",
        "        \"tbm\": \"nws\"\n",
        "    }\n",
        "\n",
        "    return f\"{base_url}?{urlencode(params)}\"\n",
        "\n",
        "# List of tickers and their homologous terms\n",
        "search_queries = [\n",
        "    [\"SP 500\", \"S&P 500\", \"Standard and Poor's 500\", \"^GSPC\", \"S&P 500 Index\", \"SPX\", \"S&P 500 ETF\", \"S&P500\", \"S&P Index\", \"Standard & Poor's 500 Index\", \"S&P 500 Stock Market\", \"US Stock Market\", \"American Stocks\", \"USA Economy\", \"U.S. Markets\", \"U.S. Economy\", \"Wall Street Index\", \"US Equity Market\", \"U.S. Stock Exchange\", \"S&P 500 Companies\"],\n",
        "    [\"NASDAQ\", \"NASDAQ Composite\", \"NASDAQ Index\", \"^IXIC\", \"Nasdaq Composite Index\", \"NASDAQ-100\", \"Nasdaq 100\", \"NASDAQ-100 Index\", \"NASDAQ stocks\", \"NASDAQ Index ETF\", \"American Technology Stocks\", \"U.S. Tech Stocks\", \"Tech-heavy Index\", \"USA Stock Market\", \"Nasdaq Tech Index\", \"NASDAQ Growth\", \"Silicon Valley Stocks\", \"NASDAQ 100 Tech\", \"USA Technology\", \"Tech Stocks Index\"],\n",
        "    [\"Dow Jones\", \"DJIA\", \"Dow Jones Industrial Average\", \"^DJI\", \"Dow Jones Index\", \"Dow Jones Average\", \"DJIA Index\", \"Dow Jones Industrial\", \"Dow Jones Industrial Stocks\", \"DJIA ETF\", \"US Blue-Chip Stocks\", \"USA Industrial Stocks\", \"U.S. Market Leaders\", \"Wall Street Benchmark\", \"Dow Jones Companies\", \"US Industrials\", \"American Economy\", \"US Industrial Market\", \"US Stock Index\", \"Wall Street Giants\"],\n",
        "    [\"CAC 40\", \"Paris Stock Exchange\", \"Euronext Paris\", \"^FCHI\", \"French Stock Market\", \"Paris Index\", \"French Economy\", \"France Stock Exchange\", \"CAC 40 Companies\", \"Paris Bourse\", \"French Markets\", \"Paris Exchange\", \"Euronext Index\", \"French Blue Chip Stocks\", \"French Equity Market\", \"France Stock Index\", \"Paris Market\", \"Eurozone Stocks\", \"France Economy\", \"Eurozone Market\"],\n",
        "    [\"FTSE 100\", \"London Stock Exchange\", \"UK 100 Index\", \"^FTSE\", \"FTSE 100 Index\", \"London Index\", \"UK Stock Market\", \"British Stock Exchange\", \"UK Economy\", \"FTSE 100 Companies\", \"London Market\", \"UK Economy Stocks\", \"FTSE Index ETF\", \"British Blue Chips\", \"London Exchange\", \"UK Markets\", \"British Economy\", \"UK Financial Markets\", \"London Stock Index\", \"UK Stock Exchange\", \"FTSE 100 Stocks\"],\n",
        "    [\"^STOXX50E\", \"EuroStoxx 50\", \"EuroStoxx 50 Index\", \"European Stock Market\", \"Eurozone Stocks\", \"European Economy\", \"Stoxx Europe 50\", \"Eurozone 50 Index\", \"EuroStoxx Index\", \"Europe Market Leaders\", \"Eurozone Leaders\", \"Top European Stocks\", \"Eurozone Top Companies\", \"European Blue Chips\", \"European Equity Market\", \"Eurozone Financials\", \"Eurozone Benchmark\", \"European Blue Chip Stocks\", \"Eurozone Economic Index\", \"European Market Index\"],\n",
        "    [\"^N225\", \"Nikkei 225\", \"Nikkei Index\", \"Japanese Stock Market\", \"Japan Economy\", \"Nikkei Average\", \"Tokyo Stock Exchange\", \"Japan Top 225 Stocks\", \"Japan Stock Index\", \"Japanese Equity Market\", \"Nikkei 225 Companies\", \"Japan's Leading Stocks\", \"Japanese Financial Market\", \"Tokyo Exchange\", \"Nikkei Market\", \"Japan Economic Index\", \"Japanese Economy\", \"Japan's Stock Exchange\", \"Top Japanese Companies\", \"Nikkei 225 ETF\"],\n",
        "    [\"^HSI\", \"Hang Seng\", \"Hang Seng Index\", \"Hong Kong Stock Market\", \"Hong Kong Economy\", \"Hong Kong Exchange\", \"HSI Index\", \"Hang Seng Index ETF\", \"Hong Kong Financial Market\", \"Chinese Stock Market\", \"HSI Stocks\", \"Asia-Pacific Stocks\", \"Hong Kong Blue Chips\", \"Hong Kong's Leading Stocks\", \"HSI Index Stocks\", \"Asian Financial Market\", \"Hong Kong Leading Companies\", \"Asian Market Leaders\", \"Hang Seng Companies\", \"Hong Kong Stock Index\"],\n",
        "    [\"000001.SS\", \"Shanghai Composite\", \"Shanghai Stock Exchange\", \"Chinese Stock Market\", \"China Economy\", \"Shanghai Index\", \"China Financial Market\", \"Shanghai Composite Index\", \"Shanghai Exchange\", \"Chinese Stock Index\", \"China's Leading Stocks\", \"Shanghai Exchange Companies\", \"China Blue Chip Stocks\", \"Chinese Equity Market\", \"Shanghai Financial Index\", \"China Benchmark\", \"Shanghai Composite ETF\", \"China's Leading Companies\", \"Chinese Market Leaders\", \"China's Economic Stocks\"],\n",
        "    [\"^BSESN\", \"Bombay Sensex\", \"S&P BSE Sensex\", \"Indian Stock Market\", \"India Economy\", \"BSE Sensex\", \"Mumbai Stock Exchange\", \"Sensex Companies\", \"Indian Financial Market\", \"Sensex 30\", \"BSE 30 Index\", \"India's Leading Stocks\", \"Indian Market Leaders\", \"Indian Equity Market\", \"Bombay Exchange\", \"Indian Stock Index\", \"Sensex Index ETF\", \"BSE India\", \"Indian Economy Stocks\", \"Bombay Financial Index\"],\n",
        "    [\"^NSEI\", \"Nifty 50\", \"National Stock Exchange of India\", \"Indian Stock Index\", \"India Stock Market\", \"Nifty Index\", \"India Economy\", \"Indian Market Leaders\", \"Nifty 50 Stocks\", \"NSE India\", \"Indian Blue Chips\", \"Indian Financial Market\", \"Indian Stock Market\", \"India 50 Index\", \"Nifty Index ETF\", \"India's Top 50\", \"India's Leading Stocks\", \"Nifty 50 Companies\", \"India Economic Stocks\", \"Indian Market Index\"],\n",
        "    [\"^KS11\", \"KOSPI\", \"Korea Composite Stock Price Index\", \"Korean Stock Market\", \"Korea Economy\", \"KOSPI Index\", \"South Korean Stock Market\", \"Korean Market Leaders\", \"KOSPI 200\", \"South Korea Financial Market\", \"Korean Exchange\", \"KOSPI ETF\", \"Korean Leading Stocks\", \"South Korean Economy\", \"Korean Market Index\", \"South Korea Stock Index\", \"Korean Economy Stocks\", \"South Korea Exchange\", \"Korean Equity Market\", \"Korea Stock Index\"],\n",
        "    [\"Gold\", \"XAU\", \"Gold Price\", \"Gold Market\", \"Precious Metals\", \"Gold Spot\", \"Gold Bullion\", \"Gold ETF\", \"Gold Investment\", \"Gold Mining\", \"Gold Futures\", \"Gold Stocks\", \"Gold Index\", \"Gold Commodity\", \"Gold Trading\", \"Gold Bullion ETF\", \"Gold Commodity Index\", \"Gold Market Trends\", \"Gold Investment Funds\", \"Gold Prices Today\"],\n",
        "    [\"Silver\", \"XAG\", \"Silver Price\", \"Silver Market\", \"Precious Metals\", \"Silver Spot\", \"Silver Bullion\", \"Silver ETF\", \"Silver Investment\", \"Silver Mining\", \"Silver Futures\", \"Silver Stocks\", \"Silver Index\", \"Silver Commodity\", \"Silver Trading\", \"Silver Bullion ETF\", \"Silver Commodity Index\", \"Silver Market Trends\", \"Silver Investment Funds\", \"Silver Prices Today\"],\n",
        "    [\"Oil\", \"Crude Oil\", \"WTI\", \"Brent Crude\", \"Oil Price\", \"Crude Oil Price\", \"OPEC\", \"Oil Futures\", \"Oil Market\", \"Oil Stocks\", \"Oil ETF\", \"Global Oil Supply\", \"Oil Trading\", \"Oil Production\", \"Brent Oil Futures\", \"Oil Price Index\", \"Oil Investment\", \"Oil Exploration\", \"Oil Trading Market\", \"Oil Price Trends\"]\n",
        "]\n",
        "\n",
        "\n",
        "# Define the year range\n",
        "start_year = 2003\n",
        "end_year = 2024\n",
        "\n",
        "# Store all generated links\n",
        "links = []\n",
        "\n",
        "for query_group in search_queries:  # Loop through each group of related terms\n",
        "    for query in query_group:  # Loop through each term in the group\n",
        "        for year in range(start_year, end_year + 1):  # Loop through years\n",
        "            for month in range(1, 13):  # Loop through months\n",
        "                start_date = f\"{month}/1/{year}\"\n",
        "                # Calculate the last day of the month\n",
        "                next_month = month % 12 + 1\n",
        "                next_year = year if month < 12 else year + 1\n",
        "                end_date = (datetime(next_year, next_month, 1) - timedelta(days=1)).strftime(\"%m/%d/%Y\")\n",
        "\n",
        "                # Generate the search link\n",
        "                link = generate_google_news_link(query, start_date, end_date)\n",
        "                links.append([query, start_date, end_date, link])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(links, columns=[\"Query\", \"Start Date\", \"End Date\", \"Google News Link\"])\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"links.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Create the NLP_data folder if it doesn't exist\n",
        "os.makedirs(\"NLP_data\", exist_ok=True)\n",
        "\n",
        "# Move the links.csv file into the NLP_data folder\n",
        "shutil.move(\"links.csv\", os.path.join(\"NLP_data\", \"links.csv\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7v6Ira8tVh4b",
        "outputId": "3e72ae40-91b9-4f27-93b3-6d0eaf02d4e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NLP_data/links.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "from urllib.parse import urlencode\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_google_news_link(query, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Generate a Google News search URL with custom date range.\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.google.com/search\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"tbs\": f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\",\n",
        "        \"tbm\": \"nws\"\n",
        "    }\n",
        "    return f\"{base_url}?{urlencode(params)}\"\n",
        "\n",
        "def get_news_links(search_url):\n",
        "    \"\"\"\n",
        "    Fetch news article links from a Google News search results page.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(search_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract all links from the search results\n",
        "        links = []\n",
        "        for a_tag in soup.find_all(\"a\", href=True):\n",
        "            href = a_tag[\"href\"]\n",
        "            if \"https://\" in href and \"google.com\" not in href:\n",
        "                links.append(href)\n",
        "        return set(links)  # Remove duplicates\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve {search_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load CSV file with search queries from the NLP_data folder\n",
        "csv_file_path = os.path.join(\"NLP_data\", \"links.csv\")\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Base folder to save results\n",
        "output_base_folder = \"google_news_results\"\n",
        "os.makedirs(output_base_folder, exist_ok=True)\n",
        "\n",
        "# Process each row in the CSV\n",
        "for index, row in df.iterrows():\n",
        "    query = row[\"Query\"].replace(\" \", \"_\")  # Sanitize folder name\n",
        "    search_url = row[\"Google News Link\"]\n",
        "\n",
        "    # Create directory for the query\n",
        "    query_folder = os.path.join(output_base_folder, query)\n",
        "    os.makedirs(query_folder, exist_ok=True)\n",
        "\n",
        "    # Get news article links\n",
        "    news_links = get_news_links(search_url)\n",
        "\n",
        "    # Sanitize file name by replacing slashes with underscores\n",
        "    start_date_sanitized = row['Start Date'].replace(\"/\", \"_\")\n",
        "    end_date_sanitized = row['End Date'].replace(\"/\", \"_\")\n",
        "    file_path = os.path.join(query_folder, f\"{start_date_sanitized}_to_{end_date_sanitized}.txt\")\n",
        "\n",
        "    # Save links to a text file\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for link in news_links:\n",
        "            f.write(link + \"\\n\")\n",
        "\n",
        "    print(f\"Saved {len(news_links)} links for {query} ({row['Start Date']} to {row['End Date']})\")\n",
        "\n",
        "    # Respect Google's policies, wait between requests\n",
        "    time.sleep(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xu5ycH4pVkKE",
        "outputId": "e6f97059-914b-433c-c7e8-9bbb49de41bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 0 links for SP_500 (1/1/2003 to 01/31/2003)\n",
            "Saved 2 links for SP_500 (2/1/2003 to 02/28/2003)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c0f90b15d559>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Respect Google's policies, wait between requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "# Base folder where raw results are stored\n",
        "output_base_folder = \"google_news_results\"\n",
        "\n",
        "# Move non-empty files and delete empty ones\n",
        "for query in os.listdir(output_base_folder):\n",
        "    query_path = os.path.join(output_base_folder, query)\n",
        "    if os.path.isdir(query_path):\n",
        "        for file_name in os.listdir(query_path):\n",
        "            file_path = os.path.join(query_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                # Delete file if it is empty\n",
        "                if os.path.getsize(file_path) == 0:\n",
        "                    os.remove(file_path)\n",
        "                    print(f\"Deleted empty file: {file_path}\")\n",
        "                else:\n",
        "                    # Extract the date part from the filename (assumes format: MM_DD_YYYY_to_...)\n",
        "                    try:\n",
        "                        date_part = file_name.split(\"_to_\")[0]  # Get the start date part\n",
        "                        date_obj = datetime.strptime(date_part, \"%m_%d_%Y\")  # Convert to datetime\n",
        "                        year = date_obj.year\n",
        "                        month = date_obj.month\n",
        "                        month_name = date_obj.strftime(\"%B\")  # Full month name\n",
        "\n",
        "                        # New folder structure: output_base_folder/year/MonthName/query\n",
        "                        new_location = os.path.join(output_base_folder, str(year), month_name, query)\n",
        "                        os.makedirs(new_location, exist_ok=True)\n",
        "\n",
        "                        # Move the file to the new location\n",
        "                        os.rename(file_path, os.path.join(new_location, file_name))\n",
        "                        print(f\"Moved file to {new_location}\")\n",
        "                    except ValueError:\n",
        "                        print(f\"Skipping file due to incorrect date format: {file_name}\")\n",
        "\n",
        "# Delete any now-empty query folders at the top level of output_base_folder\n",
        "for query in os.listdir(output_base_folder):\n",
        "    query_path = os.path.join(output_base_folder, query)\n",
        "    if os.path.isdir(query_path) and not os.listdir(query_path):\n",
        "        os.rmdir(query_path)\n",
        "        print(f\"Deleted empty group folder: {query_path}\")\n",
        "\n",
        "print(\"Cleanup and organization complete!\")\n"
      ],
      "metadata": {
        "id": "7YrAA2z9Vlii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Dictionary mapping standardized query groups to their variants\n",
        "query_groups = {\n",
        "    \"SP_500\": [\"SP_500\", \"S&P_500\", \"Standard_and_Poor's_500\", \"^GSPC\", \"S&P_500_Index\", \"SPX\", \"S&P_500_ETF\", \"S&P500\", \"S&P_Index\", \"Standard_&_Poor's_500_Index\", \"S&P_500_Stock_Market\", \"US_Stock_Market\", \"American_Stocks\", \"USA_Economy\", \"U.S._Markets\", \"U.S._Economy\", \"Wall_Street_Index\", \"US_Equity_Market\", \"U.S._Stock_Exchange\", \"S&P_500_Companies\"],\n",
        "    \"NASDAQ\": [\"NASDAQ\", \"NASDAQ_Composite\", \"NASDAQ_Index\", \"^IXIC\", \"Nasdaq_Composite_Index\", \"NASDAQ-100\", \"Nasdaq_100\", \"NASDAQ-100_Index\", \"NASDAQ_stocks\", \"NASDAQ_Index_ETF\", \"American_Technology_Stocks\", \"U.S._Tech_Stocks\", \"Tech-heavy_Index\", \"USA_Stock_Market\", \"Nasdaq_Tech_Index\", \"NASDAQ_Growth\", \"Silicon_Valley_Stocks\", \"NASDAQ_100_Tech\", \"USA_Technology\", \"Tech_Stocks_Index\"],\n",
        "    \"Dow_Jones\": [\"Dow_Jones\", \"DJIA\", \"Dow_Jones_Industrial_Average\", \"^DJI\", \"Dow_Jones_Index\", \"Dow_Jones_Average\", \"DJIA_Index\", \"Dow_Jones_Industrial\", \"Dow_Jones_Industrial_Stocks\", \"DJIA_ETF\", \"US_Blue-Chip_Stocks\", \"USA_Industrial_Stocks\", \"U.S._Market_Leaders\", \"Wall_Street_Benchmark\", \"Dow_Jones_Companies\", \"US_Industrials\", \"American_Economy\", \"US_Industrial_Market\", \"US_Stock_Index\", \"Wall_Street_Giants\"],\n",
        "    \"CAC_40\": [\"CAC_40\", \"Paris_Stock_Exchange\", \"Euronext_Paris\", \"^FCHI\", \"French_Stock_Market\", \"Paris_Index\", \"French_Economy\", \"France_Stock_Exchange\", \"CAC_40_Companies\", \"Paris_Bourse\", \"French_Markets\", \"Paris_Exchange\", \"Euronext_Index\", \"French_Blue_Chip_Stocks\", \"French_Equity_Market\", \"France_Stock_Index\", \"Paris_Market\", \"Eurozone_Stocks\", \"France_Economy\", \"Eurozone_Market\"],\n",
        "    \"FTSE_100\": [\"FTSE_100\", \"London_Stock_Exchange\", \"UK_100_Index\", \"^FTSE\", \"FTSE_100_Index\", \"London_Index\", \"UK_Stock_Market\", \"British_Stock_Exchange\", \"UK_Economy\", \"FTSE_100_Companies\", \"London_Market\", \"UK_Economy_Stocks\", \"FTSE_Index_ETF\", \"British_Blue_Chips\", \"London_Exchange\", \"UK_Markets\", \"British_Economy\", \"UK_Financial_Markets\", \"London_Stock_Index\", \"UK_Stock_Exchange\", \"FTSE_100_Stocks\"],\n",
        "    \"EuroStoxx_50\": [\"^STOXX50E\", \"EuroStoxx_50\", \"EuroStoxx_50_Index\", \"European_Stock_Market\", \"Eurozone_Stocks\", \"European_Economy\", \"Stoxx_Europe_50\", \"Eurozone_50_Index\", \"EuroStoxx_Index\", \"Europe_Market_Leaders\", \"Eurozone_Leaders\", \"Top_European_Stocks\", \"Eurozone_Top_Companies\", \"European_Blue_Chips\", \"European_Equity_Market\", \"Eurozone_Financials\", \"Eurozone_Benchmark\", \"European_Blue_Chip_Stocks\", \"Eurozone_Economic_Index\", \"European_Market_Index\"],\n",
        "    \"Nikkei_225\": [\"^N225\", \"Nikkei_225\", \"Nikkei_Index\", \"Japanese_Stock_Market\", \"Japan_Economy\", \"Nikkei_Average\", \"Tokyo_Stock_Exchange\", \"Japan_Top_225_Stocks\", \"Japan_Stock_Index\", \"Japanese_Equity_Market\", \"Nikkei_225_Companies\", \"Japan's_Leading_Stocks\", \"Japanese_Financial_Market\", \"Tokyo_Exchange\", \"Nikkei_Market\", \"Japan_Economic_Index\", \"Japanese_Economy\", \"Japan's_Stock_Exchange\", \"Top_Japanese_Companies\", \"Nikkei_225_ETF\"],\n",
        "    \"Hang_Seng\": [\"^HSI\", \"Hang_Seng\", \"Hang_Seng_Index\", \"Hong_Kong_Stock_Market\", \"Hong_Kong_Economy\", \"Hong_Kong_Exchange\", \"HSI_Index\", \"Hang_Seng_Index_ETF\", \"Hong_Kong_Financial_Market\", \"Chinese_Stock_Market\", \"HSI_Stocks\", \"Asia-Pacific_Stocks\", \"Hong_Kong_Blue_Chips\", \"Hong_Kong's_Leading_Stocks\", \"HSI_Index_Stocks\", \"Asian_Financial_Market\", \"Hong_Kong_Leading_Companies\", \"Asian_Market_Leaders\", \"Hang_Seng_Companies\", \"Hong_Kong_Stock_Index\"],\n",
        "    \"Shanghai_Composite\": [\"000001.SS\", \"Shanghai_Composite\", \"Shanghai_Stock_Exchange\", \"Chinese_Stock_Market\", \"China_Economy\", \"Shanghai_Index\", \"China_Financial_Market\", \"Shanghai_Composite_Index\", \"Shanghai_Exchange\", \"Chinese_Stock_Index\", \"China's_Leading_Stocks\", \"Shanghai_Exchange_Companies\", \"China_Blue_Chip_Stocks\", \"Chinese_Equity_Market\", \"Shanghai_Financial_Index\", \"China_Benchmark\", \"Shanghai_Composite_ETF\", \"China's_Leading_Companies\", \"Chinese_Market_Leaders\", \"China's_Economic_Stocks\"],\n",
        "    \"Bombay_Sensex\": [\"^BSESN\", \"Bombay_Sensex\", \"S&P_BSE_Sensex\", \"Indian_Stock_Market\", \"India_Economy\", \"BSE_Sensex\", \"Mumbai_Stock_Exchange\", \"Sensex_Companies\", \"Indian_Financial_Market\", \"Sensex_30\", \"BSE_30_Index\", \"India's_Leading_Stocks\", \"Indian_Market_Leaders\", \"Indian_Equity_Market\", \"Bombay_Exchange\", \"Indian_Stock_Index\", \"Sensex_Index_ETF\", \"BSE_India\", \"Indian_Economy_Stocks\", \"Bombay_Financial_Index\"],\n",
        "    \"Nifty_50\": [\"^NSEI\", \"Nifty_50\", \"National_Stock_Exchange_of_India\", \"Indian_Stock_Index\", \"India_Stock_Market\", \"Nifty_Index\", \"India_Economy\", \"Indian_Market_Leaders\", \"Nifty_50_Stocks\", \"NSE_India\", \"Indian_Blue_Chips\", \"Indian_Financial_Market\", \"Indian_Stock_Market\", \"India_50_Index\", \"Nifty_Index_ETF\", \"India's_Top_50\", \"India's_Leading_Stocks\", \"Nifty_50_Companies\", \"India_Economic_Stocks\", \"Indian_Market_Index\"],\n",
        "    \"KOSPI\": [\"^KS11\", \"KOSPI\", \"Korea_Composite_Stock_Price_Index\", \"Korean_Stock_Market\", \"Korea_Economy\", \"KOSPI_Index\", \"South_Korean_Stock_Market\", \"Korean_Market_Leaders\", \"KOSPI_200\", \"South_Korea_Financial_Market\", \"Korean_Exchange\", \"KOSPI_ETF\", \"Korean_Leading_Stocks\", \"South_Korean_Economy\", \"Korean_Market_Index\", \"South_Korea_Stock_Index\", \"Korean_Economy_Stocks\", \"South_Korea_Exchange\", \"Korean_Equity_Market\", \"Korea_Stock_Index\"],\n",
        "    \"Gold\": [\"Gold\", \"XAU\", \"Gold_Price\", \"Gold_Market\", \"Precious_Metals\", \"Gold_Spot\", \"Gold_Bullion\", \"Gold_ETF\", \"Gold_Investment\", \"Gold_Mining\", \"Gold_Futures\", \"Gold_Stocks\", \"Gold_Index\", \"Gold_Commodity\", \"Gold_Trading\", \"Gold_Bullion_ETF\", \"Gold_Commodity_Index\", \"Gold_Market_Trends\", \"Gold_Investment_Funds\", \"Gold_Prices_Today\"],\n",
        "    \"Silver\": [\"Silver\", \"XAG\", \"Silver_Price\", \"Silver_Market\", \"Precious_Metals\", \"Silver_Spot\", \"Silver_Bullion\", \"Silver_ETF\", \"Silver_Investment\", \"Silver_Mining\", \"Silver_Futures\", \"Silver_Stocks\", \"Silver_Index\", \"Silver_Commodity\", \"Silver_Trading\", \"Silver_Bullion_ETF\", \"Silver_Commodity_Index\", \"Silver_Market_Trends\", \"Silver_Investment_Funds\", \"Silver_Prices_Today\"],\n",
        "    \"Oil\": [\"Oil\", \"Crude_Oil\", \"WTI\", \"Brent_Crude\", \"Oil_Price\", \"Crude_Oil_Price\", \"OPEC\", \"Oil_Futures\", \"Oil_Market\", \"Oil_Stocks\", \"Oil_ETF\", \"Global_Oil_Supply\", \"Oil_Trading\", \"Oil_Production\", \"Brent_Oil_Futures\", \"Oil_Price_Index\", \"Oil_Investment\", \"Oil_Exploration\", \"Oil_Trading_Market\", \"Oil_Price_Trends\"]\n",
        "}\n",
        "\n",
        "# Loop through each year/month folder and unify text files for each query group\n",
        "for year_folder in os.listdir(output_base_folder):\n",
        "    year_path = os.path.join(output_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for query_group, query_list in query_groups.items():\n",
        "                    unified_file_path = os.path.join(month_path, f\"{query_group}_united.txt\")\n",
        "                    with open(unified_file_path, \"w\", encoding=\"utf-8\") as unified_file:\n",
        "                        for query in query_list:\n",
        "                            query_folder_path = os.path.join(month_path, query)\n",
        "                            if os.path.isdir(query_folder_path):\n",
        "                                for file_name in os.listdir(query_folder_path):\n",
        "                                    file_path = os.path.join(query_folder_path, file_name)\n",
        "                                    if file_path.endswith(\".txt\") and os.path.isfile(file_path):\n",
        "                                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                                            content = f.read()\n",
        "                                            unified_file.write(content + \"\\n\")\n",
        "                                        os.remove(file_path)\n",
        "                                        print(f\"Deleted {file_path}\")\n",
        "                                # Remove the now-empty query folder\n",
        "                                os.rmdir(query_folder_path)\n",
        "                                print(f\"Deleted empty folder: {query_folder_path}\")\n",
        "                    print(f\"Created unified file: {unified_file_path}\")\n"
      ],
      "metadata": {
        "id": "fIdcPaC1Vmme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def remove_empty_lines(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(non_empty_lines)\n",
        "    print(f\"Removed empty lines from: {file_path}\")\n",
        "\n",
        "# Loop through each year/month folder and clean text files\n",
        "for year_folder in os.listdir(output_base_folder):\n",
        "    year_path = os.path.join(output_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for query_folder in os.listdir(month_path):\n",
        "                    query_folder_path = os.path.join(month_path, query_folder)\n",
        "                    if os.path.isdir(query_folder_path):\n",
        "                        for file_name in os.listdir(query_folder_path):\n",
        "                            file_path = os.path.join(query_folder_path, file_name)\n",
        "                            if file_path.endswith(\".txt\") and os.path.isfile(file_path):\n",
        "                                remove_empty_lines(file_path)\n",
        "print(\"Empty lines removal completed.\")\n"
      ],
      "metadata": {
        "id": "bbRvPbAeVnmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_article_details(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        title = soup.title.string if soup.title else \"No title found\"\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = \" \".join([p.get_text() for p in paragraphs])\n",
        "        first_500_words = \" \".join(article_text.split()[:500])\n",
        "        return title, first_500_words\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve {url}: {e}\")\n",
        "        return \"Error\", \"Error\"\n",
        "\n",
        "# Process unified text files to extract article details and save as CSV\n",
        "for year_folder in os.listdir(output_base_folder):\n",
        "    year_path = os.path.join(output_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\"_united.txt\"):\n",
        "                        file_path = os.path.join(month_path, file_name)\n",
        "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                            urls = f.readlines()\n",
        "                        article_data = []\n",
        "                        for url in urls:\n",
        "                            url = url.strip()\n",
        "                            title, first_500_words = extract_article_details(url)\n",
        "                            article_data.append([url, title, first_500_words])\n",
        "                        csv_file_path = os.path.splitext(file_path)[0] + \".csv\"\n",
        "                        df = pd.DataFrame(article_data, columns=[\"URL\", \"Title\", \"First_500_Words\"])\n",
        "                        df.to_csv(csv_file_path, index=False, encoding=\"utf-8\")\n",
        "                        print(f\"Created CSV for {file_name}: {csv_file_path}\")\n",
        "print(\"Data extraction and CSV creation completed.\")\n"
      ],
      "metadata": {
        "id": "tXi7V-g5Von8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define source and target base folders\n",
        "source_base_folder = \"google_news_results\"\n",
        "target_base_folder = \"sentiment_analysis\"\n",
        "os.makedirs(target_base_folder, exist_ok=True)\n",
        "\n",
        "# Move CSV files from source to target structure, preserving year/month hierarchy\n",
        "for year_folder in os.listdir(source_base_folder):\n",
        "    year_path = os.path.join(source_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        target_year_path = os.path.join(target_base_folder, year_folder)\n",
        "        os.makedirs(target_year_path, exist_ok=True)\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                target_month_path = os.path.join(target_year_path, month_folder)\n",
        "                os.makedirs(target_month_path, exist_ok=True)\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\".csv\"):\n",
        "                        file_path = os.path.join(month_path, file_name)\n",
        "                        target_file_path = os.path.join(target_month_path, file_name)\n",
        "                        shutil.move(file_path, target_file_path)\n",
        "                        print(f\"Moved {file_name} to {target_file_path}\")\n",
        "print(\"CSV files have been moved to the sentiment_analysis folder.\")\n"
      ],
      "metadata": {
        "id": "3F2Mcs8WVpoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "# Load FinBERT sentiment analysis model\n",
        "sentiment_pipeline = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
        "def add_sentiment_labels(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if 'First_500_Words' not in df.columns:\n",
        "        print(f\"Skipping {csv_path} (no 'First_500_Words' column found)\")\n",
        "        return\n",
        "    sentiment_labels = []\n",
        "    sentiment_scores = []\n",
        "    for text in df['First_500_Words']:\n",
        "        if isinstance(text, str):\n",
        "            truncated_text = text[:500]  # Truncate text if needed\n",
        "            sentiment = sentiment_pipeline(truncated_text)[0]\n",
        "            sentiment_labels.append(sentiment['label'])\n",
        "            sentiment_scores.append(sentiment['score'])\n",
        "        else:\n",
        "            sentiment_labels.append(\"Error\")\n",
        "            sentiment_scores.append(0.0)\n",
        "    df['Sentiment_Label'] = sentiment_labels\n",
        "    df['Sentiment_Score'] = sentiment_scores\n",
        "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Updated sentiment for {csv_path}\")\n",
        "\n",
        "# Apply sentiment analysis to each CSV in sentiment_analysis folder\n",
        "sentiment_base_folder = \"sentiment_analysis\"\n",
        "for year_folder in os.listdir(sentiment_base_folder):\n",
        "    year_path = os.path.join(sentiment_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\".csv\"):\n",
        "                        csv_path = os.path.join(month_path, file_name)\n",
        "                        add_sentiment_labels(csv_path)\n",
        "print(\"Sentiment analysis labels and scores have been added.\")\n"
      ],
      "metadata": {
        "id": "-mh6UU9iVqkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "def calculate_average_score(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if 'Sentiment_Label' not in df.columns or 'Sentiment_Score' not in df.columns:\n",
        "        print(f\"Skipping {csv_path} (required columns missing)\")\n",
        "        return None\n",
        "    positive_scores = df[df['Sentiment_Label'] == 'positive']['Sentiment_Score']\n",
        "    negative_scores = df[df['Sentiment_Label'] == 'negative']['Sentiment_Score']\n",
        "    sum_positive = positive_scores.sum()\n",
        "    sum_negative = negative_scores.sum()\n",
        "    num_positive = len(positive_scores)\n",
        "    num_negative = len(negative_scores)\n",
        "    if num_positive + num_negative == 0:\n",
        "        return None\n",
        "    average_score = (sum_positive - sum_negative) / (num_positive + num_negative)\n",
        "    return average_score\n",
        "# Process each month folder to create a summary CSV\n",
        "sentiment_base_folder = \"sentiment_analysis\"\n",
        "for year_folder in os.listdir(sentiment_base_folder):\n",
        "    year_path = os.path.join(sentiment_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                summary_data = []\n",
        "                print(f\"Processing {month_folder}...\")\n",
        "                for file_name in os.listdir(month_path):\n",
        "                    if file_name.endswith(\".csv\"):\n",
        "                        csv_path = os.path.join(month_path, file_name)\n",
        "                        # Extract the query group name from filename (remove _united.csv)\n",
        "                        query_group_name = file_name.replace(\"_united.csv\", \"\")\n",
        "                        avg_score = calculate_average_score(csv_path)\n",
        "                        if avg_score is not None:\n",
        "                            print(f\"Adding {query_group_name} with score: {avg_score}\")\n",
        "                            summary_data.append([query_group_name, avg_score])\n",
        "                if summary_data:\n",
        "                    summary_df = pd.DataFrame(summary_data, columns=[\"Query Group\", \"Average Score\"])\n",
        "                    summary_csv_path = os.path.join(month_path, \"monthly_summary.csv\")\n",
        "                    summary_df.to_csv(summary_csv_path, index=False, encoding=\"utf-8\")\n",
        "                    print(f\"Created monthly summary for {month_folder}: {summary_csv_path}\")\n",
        "                else:\n",
        "                    print(f\"No data for {month_folder}, skipping.\")\n",
        "print(\"Monthly summaries have been created.\")\n"
      ],
      "metadata": {
        "id": "czhH9JCwVrmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define base folders for final results\n",
        "sentiment_base_folder = \"sentiment_analysis\"\n",
        "final_results_folder = \"final_results\"\n",
        "os.makedirs(final_results_folder, exist_ok=True)\n",
        "\n",
        "# Process monthly summaries and copy them to final_results, while aggregating data for unified matrix\n",
        "all_months_data = []\n",
        "\n",
        "for year_folder in os.listdir(sentiment_base_folder):\n",
        "    year_path = os.path.join(sentiment_base_folder, year_folder)\n",
        "    if os.path.isdir(year_path):\n",
        "        target_year_path = os.path.join(final_results_folder, year_folder)\n",
        "        os.makedirs(target_year_path, exist_ok=True)\n",
        "        for month_folder in os.listdir(year_path):\n",
        "            month_path = os.path.join(year_path, month_folder)\n",
        "            if os.path.isdir(month_path):\n",
        "                summary_csv_path = os.path.join(month_path, \"monthly_summary.csv\")\n",
        "                if os.path.exists(summary_csv_path):\n",
        "                    target_month_path = os.path.join(target_year_path, month_folder)\n",
        "                    os.makedirs(target_month_path, exist_ok=True)\n",
        "                    shutil.copy(summary_csv_path, target_month_path)\n",
        "                    month_data = pd.read_csv(summary_csv_path)\n",
        "                    month_data['Month'] = month_folder\n",
        "                    month_data['Year'] = year_folder\n",
        "                    all_months_data.append(month_data)\n",
        "\n",
        "# Create unified matrix if data is available\n",
        "if all_months_data:\n",
        "    unified_df = pd.concat(all_months_data)\n",
        "    unified_matrix = unified_df.pivot_table(index=['Year', 'Month'], columns='Query Group', values='Average Score')\n",
        "    unified_matrix_path = os.path.join(final_results_folder, \"unified_matrix.csv\")\n",
        "    unified_matrix.to_csv(unified_matrix_path, encoding=\"utf-8\")\n",
        "    print(f\"Unified matrix has been created at: {unified_matrix_path}\")\n",
        "else:\n",
        "    print(\"No data available to create the unified matrix.\")\n",
        "\n",
        "# (Optional) Save a simulated unified matrix as a separate file\n",
        "unified_matrix.to_csv(\"simulated_unified_matrix.csv\", encoding=\"utf-8\")\n",
        "print(\"Simulated unified matrix saved to simulated_unified_matrix.csv\")\n"
      ],
      "metadata": {
        "id": "0egHlpqNVsxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}